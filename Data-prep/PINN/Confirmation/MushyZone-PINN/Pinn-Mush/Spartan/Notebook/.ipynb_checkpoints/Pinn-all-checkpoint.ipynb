{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Three Phase Simulation of Alloys and PINN model development \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the simulation of 1D Phase change of aluminium alloy. There will be three phases (solid,liquid and mushy).   \n",
    "\n",
    "The approach used is finite difference method and the physics involved in heat conduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import csv\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "from pinn_loss import loss_fn_data, l1_regularization, pde_loss, boundary_loss, ic_loss, accuracy\n",
    "from Input_vec_gen import input_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the constants and inital geometric domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_l = 3.394878564540885e-05, alpha_s = 3.686205086349929e-05, m_eff = 6.296953764744878e-06\n",
      "dx is 0.0003061224489795918\n",
      "dt is  0.0012711033647622566\n",
      "num_steps is 31469\n",
      "cfl is 0.0012711033647622566\n",
      "stability criteria satisfied\n"
     ]
    }
   ],
   "source": [
    "# Geometry\n",
    "length = 15.0e-3             # Length of the rod\n",
    "\n",
    "# Material properties\n",
    "rho = 2300.0                     # Density of AL380 (kg/m^3)\n",
    "rho_l = 2460.0                   # Density of AL380 (kg/m^3)\n",
    "rho_s = 2710.0                    # Density of AL380 (kg/m^3)\n",
    "rho_m = (rho_l + rho_s )/2       # Desnity in mushy zone is taken as average of liquid and solid density\n",
    "\n",
    "k = 104.0                       # W/m-K\n",
    "k_l = k                       # W/m-K\n",
    "k_s = 96.2                    # W/m-K\n",
    "k_m =  (k_l+k_s)/2                     # W/m-K\n",
    "k_mo = 41.5\n",
    "\n",
    "\n",
    "cp = 1245.3                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_l = cp                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_s = 963.0                 # Specific heat of aluminum (J/kg-K)\n",
    "cp_m =  (cp_l+cp_s)/2                 # Specific heat of mushy zone is taken as average of liquid and solid specific heat\n",
    "# cp_m = cp\n",
    "           # Thermal diffusivity\n",
    "alpha_l = k_l / (rho_l * cp_l) \n",
    "alpha_s = k_s / (rho_s*cp_s)\n",
    "alpha_m = k_m / (rho_m * cp_m)          #`Thermal diffusivity in mushy zone is taken as average of liquid and solid thermal diffusivity`\n",
    "\n",
    "\n",
    "#L_fusion = 3.9e3                 # J/kg\n",
    "L_fusion = 389.0e3               # J/kg  # Latent heat of fusion of aluminum\n",
    "         # Thermal diffusivity\n",
    "\n",
    "\n",
    "T_L = 574.4 +273.0                       #  K -Liquidus Temperature (615 c) AL 380\n",
    "T_S = 497.3 +273.0                     # K- Solidus Temperature (550 C)\n",
    "m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "print (f\"alpha_l = {alpha_l}, alpha_s = {alpha_s}, m_eff = {m_eff}\")\n",
    "\n",
    "# htc = 10.0                   # W/m^2-K\n",
    "# q = htc*(919.0-723.0)\n",
    "# q = 10000.0\n",
    "\n",
    "\n",
    "num_points = 50                        # Number of spatial points\n",
    "dx = length / (num_points - 1)         # Distance between two spatial points\n",
    "print('dx is',dx)\n",
    "\n",
    "                                                              \n",
    "# Time Discretization  \n",
    "# \n",
    "time_end = 40        # seconds                         \n",
    "\n",
    "maxi = max(alpha_s,alpha_l,alpha_m)\n",
    "dt = abs(0.5*((dx**2) /maxi)) \n",
    "\n",
    "print('dt is ',dt)\n",
    "num_steps = round(time_end/dt)\n",
    "print('num_steps is',num_steps)\n",
    "cfl = 0.5 *(dx**2/max(alpha_l,alpha_s,alpha_m))\n",
    "print('cfl is',cfl)\n",
    "\n",
    "time_steps = np.linspace(0, time_end, num_steps + 1)\n",
    "step_coeff = dt / (dx ** 2)\n",
    "\n",
    "if dt <= cfl:\n",
    "    print('stability criteria satisfied')\n",
    "else:\n",
    "    print('stability criteria not satisfied')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial and Boundary Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_init = 919.0\n",
    "# Initial temperature and phase fields\n",
    "temperature = np.full(num_points+2, 919.0)            # Initial temperature of the rod with ghost points at both ends\n",
    "phase = np.zeros(num_points+2)*0.0                    # Initial phase of the rod with ghost points at both ends\n",
    "\n",
    "# Set boundary conditions\n",
    "# temperature[-1] = 919.0 \n",
    "phase[-1] = 1.0\n",
    "\n",
    "# temperature[0] = 919.0 #(40 C)\n",
    "phase[0] = 1.0\n",
    "\n",
    "# Store initial state in history\n",
    "temperature_history = [temperature.copy()]    # List to store temperature at each time step\n",
    "phi_history = [phase.copy()]                    # List to store phase at each time step\n",
    "temp_init = temperature.copy()                 # Initial temperature of the rod\n",
    "# print(temperature_history,phi_history)\n",
    "# Array to store temperature at midpoint over time\n",
    "midpoint_index = num_points // 2                          # Index of the midpoint\n",
    "\n",
    "midpoint_temperature_history = [temperature[midpoint_index]]            # List to store temperature at midpoint over time\n",
    "dm = 60.0e-3                                                            # die thickness in m\n",
    "\n",
    "# r_m =  (k_mo / dm) + (1/htc)\n",
    "\n",
    "t_surr = 500.0                                        # Surrounding temperature in K\n",
    "# t_surr = h()\n",
    "\n",
    "def kramp(temp,v1,v2,T_L,T_s):                                      # Function to calculate thermal conductivity in Mushy Zone\n",
    "        slope = (v1-v2)/(T_L-T_S)\n",
    "        if temp > T_L:\n",
    "            k_m = k_l\n",
    "        elif temp < T_S:\n",
    "            k_m = k_s\n",
    "        else:\n",
    "            k_m = k_s + slope*(temp-T_S)\n",
    "        return k_m\n",
    "\n",
    "def cp_ramp(temp,v1,v2,T_L,T_s):                                    # Function to calculate specific heat capacity in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        cp_m = cp_l\n",
    "    elif temp < T_S:\n",
    "        cp_m = cp_s\n",
    "    else:\n",
    "        cp_m = cp_s + slope*(temp-T_S)\n",
    "    return cp_m\n",
    "\n",
    "def rho_ramp(temp,v1,v2,T_L,T_s):                                       # Function to calculate density in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        rho_m = rho_l\n",
    "    elif temp < T_S:\n",
    "        rho_m = rho_s\n",
    "    else:\n",
    "        rho_m = rho_s + slope*(temp-T_S)\n",
    "    return rho_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the HT equation and phase change numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for m in range(1, num_steps+1):                                                                            # time loop\n",
    "    htc = 10.0                   # htc of Still air in W/m^2-K\n",
    "    q1 = htc*(temp_init[0]-t_surr)   # Heat flux at the left boundary\n",
    "    \n",
    "    # print(f\"q1 is {q1}\")\n",
    "    temperature[0] = temp_init[0] + alpha_l * step_coeff * ((2.0*temp_init[1]) - (2.0 * temp_init[0])-(2.0*dx*(q1)))  # Update boundary condition temperature\n",
    "    \n",
    "    q2 = htc*(temp_init[-1]-t_surr)                   # Heat flux at the right boundary\n",
    "    temperature[-1] = temp_init[-1] + alpha_l * step_coeff * ((2.0*temp_init[-2]) - (2.0 * temp_init[-1])-(2.0*dx*(q2)))  # Update boundary condition temperature\n",
    "    \n",
    "    for n in range(1,num_points+1):              # space loop, adjusted range\n",
    "       \n",
    "        if temperature[n] >= T_L:\n",
    "            temperature[n] += ((alpha_l * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            phase[n] = 0\n",
    "            \n",
    "            # print(f\" Time-Step{m},Spatial point{n},Temperature{temperature[n]}\")\n",
    "        elif T_S < temperature[n] < T_L:\n",
    "            \n",
    "            k_m = kramp(temperature[n],k_l,k_s,T_L,T_S)\n",
    "            cp_m = cp_ramp(temperature[n],cp_l,cp_s,T_L,T_S)\n",
    "            rho_m = rho_ramp(temperature[n],rho_l,rho_s,T_L,T_S)\n",
    "            m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "            \n",
    "            temperature[n] += ((m_eff * step_coeff)* (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            \n",
    "            phase[n] = (T_L - temperature[n]) / (T_L - T_S)\n",
    "            # print(m,n,temperature[n],phase[n])\n",
    "         \n",
    "        elif temperature[n]<T_S:\n",
    "            temperature[n] += ((alpha_s * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n])+ temp_init[n-1]))\n",
    "            phase[n] = 1\n",
    "                     \n",
    "        else:\n",
    "            print(\"ERROR: should not be here\")\n",
    "\n",
    "     \n",
    "          \n",
    "    temperature = temperature.copy()                                                                # Update temperature\n",
    "    phase = phase.copy()                                                                            # Update phase\n",
    "    temp_init = temperature.copy()                                                                  # Update last time step temperature\n",
    "    temperature_history.append(temperature.copy())                                                  # Append the temperature history to add ghost points\n",
    "    phi_history.append(phase.copy())                                                                # Append the phase history to add ghost points\n",
    "    midpoint_temperature_history.append(temperature[midpoint_index])                                # Store midpoint temperature\n",
    "    \n",
    "    \n",
    "    # print(f\"Step {m}, Temperature: {temperature}\")\n",
    "    \n",
    "\n",
    "\n",
    "# print(midpoint_temperature_history)\n",
    "#print(phi_history)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot temperature history for debugging\n",
    "# temperature_history_1 = np.array(temperature_history)\n",
    "# print(temperature_history_1.shape)\n",
    "# time_ss= np.linspace(0, time_end, num_steps+1)\n",
    "# # print(time_ss.shape)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(time_ss, midpoint_temperature_history, label='Midpoint Temperature')\n",
    "# plt.axhline(y=T_L, color='r', linestyle='--', label='Liquidus Temperature')\n",
    "# plt.axhline(y=T_S, color='g', linestyle='--', label='Solidus Temperature')\n",
    "# plt.xlabel('Time(s)')\n",
    "# plt.ylabel('Temperature (K)')\n",
    "# plt.title('Temperature Distribution Over Time at x = 7.5mm') \n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data into Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_history = np.array(temperature_history)\n",
    "\n",
    "phi_history = np.array(phi_history)\n",
    "\n",
    "t_hist = np.array(temperature_history[:,1:-1])\n",
    "p_hist = np.array(phi_history[:,1:-1])\n",
    "\n",
    "t_hist_init = t_hist[0,:]\n",
    "t_hist_bc_l = t_hist[:,0]\n",
    "t_hist_bc_r = t_hist[:,-1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have temperature_history and phi_history as lists of arrays\n",
    "\n",
    "\n",
    "# # Check the new shape after transposing\n",
    "# print(\"Transposed Temperature History Shape:\", temperature_history.shape)\n",
    "# print(\"Transposed Phi History Shape:\", phi_history.shape)\n",
    "\n",
    "# # Create a meshgrid for space and time coordinates\n",
    "# space_coord, time_coord = np.meshgrid(np.arange(temperature_history.shape[1]), np.arange(temperature_history.shape[0]))\n",
    "\n",
    "# time_coord = time_coord * dt \n",
    "# # Create a figure with two subplots\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# # Plot the temperature history on the left subplot\n",
    "# im1 = ax1.pcolormesh(space_coord, time_coord, temperature_history, cmap='viridis')\n",
    "# ax1.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_title('Temperature Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im1, ax=ax1, label='Temperature')\n",
    "\n",
    "# # Plot the phase history on the right subplot\n",
    "# im2 = ax2.pcolormesh(space_coord, time_coord, phi_history, cmap='viridis')\n",
    "# ax2.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=18)\n",
    "# ax2.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax2.set_title('Phase Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im2, ax=ax2, label='Phase')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #plot the main\n",
    "# fig, ax = plt.subplots(figsize=(14, 6))\n",
    "# im = ax.pcolormesh(space_coord, time_coord, Dim_ny, cmap='viridis')\n",
    "# ax.set_xlabel('Space Coordinate')\n",
    "# ax.set_ylabel('Time')\n",
    "# ax.set_title('Niyama Variation Over Time')\n",
    "# fig.colorbar(im, ax=ax, label='Main')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU/CPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# check for gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,) (31470,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "space = np.linspace(0, length, num_points) # Spatial points\n",
    "time = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "print(space.shape,time.shape)\n",
    "\n",
    "sp_i = np.linspace(0, length, num_points) # Spatial points\n",
    "time_i = np.zeros(num_points) # Time points\n",
    "\n",
    "sp_b_l = np.zeros(num_steps+1) # Spatial points\n",
    "time_b_l = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "\n",
    "sp_b_r = np.ones(num_steps+1)*length # Spatial points\n",
    "time_b_r = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = input_gen(space,time,'mgrid')\n",
    "inputs_i = input_gen(sp_i,time_i,'scr')\n",
    "inputs_b_l = input_gen(sp_b_l,time_b_l,'scr')\n",
    "inputs_b_r = input_gen(sp_b_r,time_b_r,'scr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1573500, 2) (50, 2) (31470, 2) (31470, 2)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape,inputs_i.shape,inputs_b_l.shape,inputs_b_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2])\n",
      "torch.Size([1573500, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs = torch.tensor(inputs).float().to(device) # Convert the inputs to a tensor\n",
    "\n",
    "\n",
    "inputs_init = torch.tensor(inputs_i).float().to(device) # Convert the inputs to a tensor\n",
    "inputs_b_l = torch.tensor(inputs_b_l).float().to(device) # Convert the inputs to a tensor\n",
    "inputs_b_r = torch.tensor(inputs_b_r).float().to(device) # Convert the inputs to a tensor\n",
    "\n",
    "print(inputs_init.shape)\n",
    "# label/temp data\n",
    "temp_tr = torch.tensor(t_hist).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp = temp_tr.reshape(-1,1) # Reshape the temperature tensor to a column vector\n",
    "temp_inp_init = torch.tensor(t_hist_init).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp_bc_l = torch.tensor(t_hist_bc_l).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp_bc_r = torch.tensor(t_hist_bc_r).float().to(device) # Convert the temperature history to a tensor\n",
    "print(temp_inp.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Data Splitting\n",
    "\n",
    "# train_inputs, val_test_inputs, train_temp_inp, val_test_temp_inp = train_test_split(inputs, temp_inp, test_size=0.2, random_state=42)\n",
    "# val_inputs, test_inputs, val_temp_inp, test_temp_inp = train_test_split(val_test_inputs, val_test_temp_inp, test_size=0.8, random_state=42)\n",
    "\n",
    "train_inputs, test_inputs, train_temp_inp, test_temp_inp = train_test_split(inputs, temp_inp, test_size=0.2, random_state=42)\n",
    "train_inputs_init, test_inputs_init, train_temp_inp_init, test_temp_inp_init = train_test_split(inputs_init, temp_inp_init, test_size=0.2, random_state=42)\n",
    "train_inputs_bc_l, test_inputs_bc_l, train_temp_inp_bc_l, test_temp_inp_bc_l = train_test_split(inputs_b_l, temp_inp_bc_l, test_size=0.2, random_state=42)\n",
    "train_inputs_bc_r, test_inputs_bc_r, train_temp_inp_bc_r, test_temp_inp_bc_r = train_test_split(inputs_b_r, temp_inp_bc_r, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, temp_inp,transform=None, target_transform =None):\n",
    "        self.inputs = inputs\n",
    "        self.temp_inp = temp_inp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.temp_inp[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "  \n",
    "train_dataset = TensorDataset(train_inputs, train_temp_inp) # Create the training dataset\n",
    "# val_dataset = TensorDataset(val_inputs, val_temp_inp) # Create the validation dataset\n",
    "test_dataset = TensorDataset(test_inputs, test_temp_inp) # Create the test dataset\n",
    "\n",
    "train_dataset_init = TensorDataset(train_inputs_init, train_temp_inp_init) # Create the training dataset\n",
    "test_dataset_init = TensorDataset(test_inputs_init, test_temp_inp_init) # Create the test dataset\n",
    "train_dataset_bc_l = TensorDataset(train_inputs_bc_l, train_temp_inp_bc_l) # Create the training dataset\n",
    "test_dataset_bc_l = TensorDataset(test_inputs_bc_l, test_temp_inp_bc_l) # Create the test dataset\n",
    "train_dataset_bc_r = TensorDataset(train_inputs_bc_r, train_temp_inp_bc_r) # Create the training dataset\n",
    "test_dataset_bc_r = TensorDataset(test_inputs_bc_r, test_temp_inp_bc_r) # Create the test dataset\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "random_sampler_train = RandomSampler(train_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "# random_sampler_val = RandomSampler(val_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the validation dataset\n",
    "random_sampler_test = RandomSampler(test_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "\n",
    "random_sampler_train_init = RandomSampler(train_dataset_init, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "random_sampler_test_init = RandomSampler(test_dataset_init, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "random_sampler_train_bc_l = RandomSampler(train_dataset_bc_l, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "random_sampler_test_bc_l = RandomSampler(test_dataset_bc_l, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "random_sampler_train_bc_r = RandomSampler(train_dataset_bc_r, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "random_sampler_test_bc_r = RandomSampler(test_dataset_bc_r, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=random_sampler_train) # Create the training dataloader\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=random_sampler_val) # Create the validation dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=random_sampler_test) # Create the test dataloader\n",
    "\n",
    "train_loader_init = DataLoader(train_dataset_init, batch_size=batch_size, sampler=random_sampler_train_init) # Create the training dataloader\n",
    "test_loader_init = DataLoader(test_dataset_init, batch_size=batch_size, sampler=random_sampler_test_init) # Create the test dataloader\n",
    "train_loader_bc_l = DataLoader(train_dataset_bc_l, batch_size=batch_size, sampler=random_sampler_train_bc_l) # Create the training dataloader\n",
    "test_loader_bc_l = DataLoader(test_dataset_bc_l, batch_size=batch_size, sampler=random_sampler_test_bc_l) # Create the test dataloader\n",
    "train_loader_bc_r = DataLoader(train_dataset_bc_r, batch_size=batch_size, sampler=random_sampler_train_bc_r) # Create the training dataloader\n",
    "test_loader_bc_r = DataLoader(test_dataset_bc_r, batch_size=batch_size, sampler=random_sampler_test_bc_r) # Create the test dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural network architecture\n",
    "class Mushydata(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size): # This is the constructor\n",
    "        super(Mushydata, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):                               # This is the forward pass\n",
    "        input_features = torch.cat([x, t], dim=1)          # Concatenate the input features\n",
    "        m = self.base(input_features)                                 # Pass through the third layer\n",
    "        return m                    # Return the output of the network\n",
    "\n",
    "\n",
    "# features = torch.rand(1, 2)\n",
    "# model = HeatPINN(2, 20, 1)\n",
    "# output = model(features[:, 0:1], features[:, 1:2])\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# Loss function for data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 60\n",
    "learning_rate = 0.002\n",
    "epochs = 30000\n",
    "# alpha = 0.01  # Adjust this value based on your problem\n",
    "# boundary_value = 313.0\n",
    "# initial_value = init_temp\n",
    "# Initialize the model\n",
    "model = Mushydata(input_size=2, hidden_size=hidden_size,output_size=1).to(device)\n",
    "lambd = 0.1\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss List Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of train_loader is <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(f\"Datatype of train_loader is {type(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn_data(u_pred, u_true):\n",
    "#     return nn.MSELoss()(u_pred, u_true)\n",
    "\n",
    "# def l1_regularization(model, lambd):\n",
    "#     l1_reg = sum(param.abs().sum() for param in model.parameters())\n",
    "#     return l1_reg * lambd\n",
    "\n",
    "# def pde_loss(u_pred,x,t):\n",
    "#     # u_pred.requires_grad = True\n",
    "#     x.requires_grad = True\n",
    "#     t.requires_grad = True\n",
    "    \n",
    "#     u_pred = model(x,t).requires_grad_()\n",
    "#     u_t = torch.autograd.grad(u_pred, t, \n",
    "#                                 torch.ones_like(u_pred).to(device),\n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused=True,\n",
    "#                                 )[0] # Calculate the first time derivative\n",
    "#     if u_t is None:\n",
    "#         raise RuntimeError(\"u_t is None\")\n",
    "\n",
    "#     u_x = torch.autograd.grad(u_pred, \n",
    "#                                 x, \n",
    "#                                 torch.ones_like(u_pred).to(device), \n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused =True)[0] # Calculate the first space derivative\n",
    "            \n",
    "#     u_xx = torch.autograd.grad(u_x, \n",
    "#                                 x, \n",
    "#                                 torch.ones_like(u_x).to(device), \n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused=True)[0] \n",
    "    \n",
    "#     T_S_tensor = torch.tensor(T_S, device=device)\n",
    "#     T_L_tensor = torch.tensor(T_L, device=device)\n",
    "    \n",
    "#     k_m = torch.where((u_pred >= T_S_tensor) * (u_pred <= T_L_tensor),\\\n",
    "#                        kramp(u_pred, k_l,k_s,T_L,T_S),torch.tensor(0.0,device=device))\n",
    "#     cp_m = torch.where(u_pred >= T_S_tensor * u_pred <= T_L_tensor, cp_ramp((u_pred), cp_l,cp_s,T_L,T_S))\n",
    "#     rho_m = torch.where(u_pred >= T_S_tensor * u_pred <= T_L_tensor, rho_ramp((u_pred), rho_l,rho_s,T_L,T_S))\n",
    "#     m_eff = (k_m / (rho_m * (cp_m + (L_fusion / (T_L - T_S)))))\n",
    "\n",
    "#     alpha_T = torch.where(u_pred >= T_L_tensor, alpha_l, torch.where(u_pred<=T_S_tensor,alpha_s ,m_eff))\n",
    "#     alpha_T = 1\n",
    "#     residual = u_t - alpha_T * u_xx\n",
    "\n",
    "#     return nn.MSELoss()(residual,torch.zeros_like(residual))\n",
    "\n",
    "# def boundary_loss(u_pred,x,t,t_surr):\n",
    "    \n",
    "#     u_x = torch.autograd.grad(u_pred,x, \n",
    "#                                 torch.ones_like(u_pred).to(device), \n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused =True)[0] # Calculate the first space derivative\n",
    "#     t_surr_t = torch.tensor(t_surr, device=device)\n",
    "#     res_l = u_x -(htc* (u_pred-t_surr_t))\n",
    "   \n",
    "\n",
    "#     return nn.MSELoss()(res_l,torch.zeros_like(res_l))\n",
    "\n",
    "# def ic_loss(u_pred):\n",
    "#     temp_init_tsr = torch.tensor(temp_init[1:-1],device=device)\n",
    "#     ic = u_pred -temp_init_tsr\n",
    "#     return nn.MSELoss()(ic,torch.zeros_like(ic))\n",
    "\n",
    "def accuracy(u_pred, u_true):\n",
    "    return torch.mean(torch.abs(u_pred - u_true) / u_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Testing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, model, loss_fn_data, optimizer, train_dataloader,):\n",
    "    train_losses = []  # Initialize the list to store the training losses\n",
    "    val_losses = []    # Initialize the list to store the validation losses\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                                                           # Set the model to training mode\n",
    "        train_loss = 0                                                                              # Initialize the training loss\n",
    "        train_accuracy = 0\n",
    "        for (batch,batch_init,batch_left,batch_right) in \\\n",
    "             zip (train_dataloader,train_loader_init,train_loader_bc_l,train_inputs_bc_r):                                                          # Loop through the training dataloader\n",
    "            inputs, temp_inp= batch                                                             # Get the inputs and the true values\n",
    "            inputs_init, temp_inp_init= batch_init                                                             # Get the inputs and the true values \n",
    "            inputs_left, temp_inp_left= batch_left                                                             # Get the inputs and the true values\n",
    "            inputs_right, temp_inp_right= batch_right                                                             # Get the inputs and the true values\n",
    "\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device)                             # Move the inputs and true values to the GPU\n",
    "            inputs_init, temp_inp_init= inputs_init.to(device), temp_inp_init.to(device)                             # Move the initial condition inputs and temperature to the GPU\n",
    "            inputs_left, temp_inp_left= inputs_left.to(device), temp_inp_left.to(device)                             # Move the left boundary condition inputs and temperature values to the GPU\n",
    "            inputs_right, temp_inp_right= inputs_right.to(device), temp_inp_right.to(device)                             # Move the right boundary condition inputs and temperature values to the GPU\n",
    "\n",
    "            optimizer.zero_grad()                                                                    # Zero the gradients\n",
    "            \n",
    "            # Forward pass\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).to(device)                       # Get the predictions\n",
    "            u_initl = model(inputs_init[:,0].unsqueeze(1), inputs_init[:,1].unsqueeze(1)).to(device)                       # Get the predictions\n",
    "            \n",
    "            u_left = model(inputs_b_l[:,0].unsqueeze(1), inputs_b_l[:,1].unsqueeze(1)).to(device)               # Left boundary of the temperature\n",
    "            u_right = model(inputs_b_r[:,0].unsqueeze(1), inputs_b_r[:,1].unsqueeze(1)).to(device)             # Right boundary of the temperature\n",
    "\n",
    "            # Loss calculation\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp)                                              # Calculate the data loss\n",
    "            \n",
    "            pd_loss = pde_loss(model,inputs[:,0].unsqueeze(1),inputs[:,1].unsqueeze(1))             # Calculate the PDE loss\n",
    "            # pd_loss = 0\n",
    "            \n",
    "            initc_loss = ic_loss(u_initl) \n",
    "            # initc_loss =0                                                      # Calculate initial condition loss\n",
    "            \n",
    "            bc_loss_left = boundary_loss(model,inputs_b_l[:,0].unsqueeze(1),inputs_b_l[:,1].unsqueeze(1),t_surr) # Calculate the left boundary condition loss\n",
    "            bc_loss_right = boundary_loss(model,inputs_b_r[:,0].unsqueeze(1),inputs_b_r[:,1].unsqueeze(1),t_surr) # Calculate the right boundary condition loss\n",
    "            bc_loss = bc_loss_left + bc_loss_right\n",
    "            # l1_regularization_loss = l1_regularization(model, lambda_l1)                      # Calculate the L1 regularization loss\n",
    "            # loss = data_loss  + pd_loss + initc_loss + bc_loss                                              # Calculate the total loss\n",
    "            w1 = 0.01\n",
    "            w2 = 0.01\n",
    "            w3 = 0.0001\n",
    "            loss = data_loss + w1* pd_loss + w2 *initc_loss + w3* bc_loss\n",
    "            train_accuracy += accuracy(u_pred, temp_inp)                                                              # Calculate the total loss\n",
    "            # Backpropagation\n",
    "            loss.backward(retain_graph=True)                                                        # Backpropagate the gradients\n",
    "            \n",
    "            optimizer.step()                                                                           # Update the weights\n",
    "            \n",
    "            train_loss += loss.item()                                                           # Add the loss to the training set loss                 \n",
    "\n",
    "        \n",
    "\n",
    "        # model.eval()\n",
    "        # test_loss = 0\n",
    "        # test_accuracy = 0\n",
    "        # with torch.no_grad():   \n",
    "        #     for batch in test_dataloader:\n",
    "        #         inputs, temp_inp= batch\n",
    "        #         inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "        #         u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "        #         data_loss = loss_fn_data(u_pred, temp_inp)\n",
    "        #         # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "        #         # loss = data_loss  + l1_regularization_loss\n",
    "        #         loss = data_loss\n",
    "        #         test_accuracy = accuracy(u_pred, temp_inp)\n",
    "        #         test_loss += loss.item()\n",
    "        #     test_losses.append(test_loss)\n",
    "\n",
    "        train_losses.append(train_loss)                                                   # Append the training loss to the list of training losses\n",
    "        \n",
    "        # if epoch % 10 == 0:\n",
    "        #     print(f\"Epoch {epoch}, Training-Loss {train_loss:.4e}\")\n",
    "        \n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training-Loss {train_loss:.4e}, Data-loss {data_loss:.4e}\\\n",
    "                  , pde-loss {pd_loss:.4e}, initc-loss {initc_loss:.4e}\\\n",
    "                    bc_loss {bc_loss:.4e}\") \n",
    "\n",
    "    return train_losses, val_losses                                                             # Return the training and validation losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(epochs, model, loss_fn_data, optimizer, train_dataloader, test_dataloader):\n",
    "      \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    with torch.no_grad():   \n",
    "        for batch in test_dataloader:\n",
    "            inputs, temp_inp= batch\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp)\n",
    "            # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "            # loss = data_loss  + l1_regularization_loss\n",
    "            loss = data_loss\n",
    "            test_accuracy = accuracy(u_pred, temp_inp)\n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss)\n",
    "    if epochs % 10 == 0:\n",
    "        print(f\"Epoch {epochs}, Test-Loss {test_loss:.4e}, Test-Accuracy {test_accuracy:.4e}\")      \n",
    "    return test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Button "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training-Loss 6.7749e+05, Data-loss 6.6403e+05                  , pde-loss 2.9204e-07, initc-loss 8.4527e+05                    bc_loss 5.0077e+07\n",
      "Epoch 10, Training-Loss 6.6737e+05, Data-loss 6.5396e+05                  , pde-loss 1.2938e-06, initc-loss 8.4306e+05                    bc_loss 4.9837e+07\n",
      "Epoch 20, Training-Loss 6.6395e+05, Data-loss 6.5058e+05                  , pde-loss 2.0861e-06, initc-loss 8.4090e+05                    bc_loss 4.9602e+07\n",
      "Epoch 30, Training-Loss 6.6102e+05, Data-loss 6.4769e+05                  , pde-loss 8.2554e-07, initc-loss 8.3895e+05                    bc_loss 4.9390e+07\n",
      "Epoch 40, Training-Loss 6.5773e+05, Data-loss 6.4444e+05                  , pde-loss 1.6566e-07, initc-loss 8.3715e+05                    bc_loss 4.9195e+07\n",
      "Epoch 50, Training-Loss 6.6645e+05, Data-loss 6.5319e+05                  , pde-loss 6.2682e-08, initc-loss 8.3538e+05                    bc_loss 4.9004e+07\n",
      "Epoch 60, Training-Loss 6.5039e+05, Data-loss 6.3717e+05                  , pde-loss 2.9900e-08, initc-loss 8.3367e+05                    bc_loss 4.8818e+07\n",
      "Epoch 70, Training-Loss 6.5571e+05, Data-loss 6.4252e+05                  , pde-loss 2.8467e-08, initc-loss 8.3198e+05                    bc_loss 4.8635e+07\n",
      "Epoch 80, Training-Loss 6.6267e+05, Data-loss 6.4953e+05                  , pde-loss 2.5312e-08, initc-loss 8.3031e+05                    bc_loss 4.8454e+07\n",
      "Epoch 90, Training-Loss 6.4578e+05, Data-loss 6.3267e+05                  , pde-loss 2.1525e-08, initc-loss 8.2860e+05                    bc_loss 4.8270e+07\n",
      "Epoch 100, Training-Loss 6.4641e+05, Data-loss 6.3334e+05                  , pde-loss 2.0881e-08, initc-loss 8.2682e+05                    bc_loss 4.8079e+07\n",
      "Epoch 110, Training-Loss 6.5888e+05, Data-loss 6.4584e+05                  , pde-loss 2.2221e-08, initc-loss 8.2490e+05                    bc_loss 4.7871e+07\n",
      "Epoch 120, Training-Loss 6.5311e+05, Data-loss 6.4012e+05                  , pde-loss 1.4171e-08, initc-loss 8.2290e+05                    bc_loss 4.7656e+07\n",
      "Epoch 130, Training-Loss 6.4425e+05, Data-loss 6.3130e+05                  , pde-loss 1.7166e-08, initc-loss 8.2087e+05                    bc_loss 4.7438e+07\n",
      "Epoch 140, Training-Loss 6.4148e+05, Data-loss 6.2857e+05                  , pde-loss 1.5048e-08, initc-loss 8.1886e+05                    bc_loss 4.7221e+07\n",
      "Epoch 150, Training-Loss 6.4406e+05, Data-loss 6.3120e+05                  , pde-loss 5.0854e-09, initc-loss 8.1686e+05                    bc_loss 4.7007e+07\n",
      "Epoch 160, Training-Loss 6.4106e+05, Data-loss 6.2823e+05                  , pde-loss 4.8897e-09, initc-loss 8.1482e+05                    bc_loss 4.6788e+07\n",
      "Epoch 170, Training-Loss 6.4112e+05, Data-loss 6.2834e+05                  , pde-loss 1.4660e-08, initc-loss 8.1264e+05                    bc_loss 4.6555e+07\n",
      "Epoch 180, Training-Loss 6.3994e+05, Data-loss 6.2720e+05                  , pde-loss 1.6023e-08, initc-loss 8.1040e+05                    bc_loss 4.6315e+07\n",
      "Epoch 190, Training-Loss 6.2941e+05, Data-loss 6.1672e+05                  , pde-loss 1.1534e-08, initc-loss 8.0808e+05                    bc_loss 4.6067e+07\n",
      "Epoch 200, Training-Loss 6.3897e+05, Data-loss 6.2633e+05                  , pde-loss 4.5072e-09, initc-loss 8.0580e+05                    bc_loss 4.5824e+07\n",
      "Epoch 210, Training-Loss 6.2727e+05, Data-loss 6.1467e+05                  , pde-loss 6.1819e-09, initc-loss 8.0343e+05                    bc_loss 4.5571e+07\n",
      "Epoch 220, Training-Loss 6.3576e+05, Data-loss 6.2321e+05                  , pde-loss 5.8016e-09, initc-loss 8.0105e+05                    bc_loss 4.5318e+07\n",
      "Epoch 230, Training-Loss 6.3347e+05, Data-loss 6.2098e+05                  , pde-loss 4.1399e-09, initc-loss 7.9873e+05                    bc_loss 4.5072e+07\n",
      "Epoch 240, Training-Loss 6.2427e+05, Data-loss 6.1183e+05                  , pde-loss 6.3767e-09, initc-loss 7.9637e+05                    bc_loss 4.4821e+07\n",
      "Epoch 250, Training-Loss 6.2374e+05, Data-loss 6.1134e+05                  , pde-loss 4.9296e-09, initc-loss 7.9405e+05                    bc_loss 4.4575e+07\n",
      "Epoch 260, Training-Loss 6.1595e+05, Data-loss 6.0359e+05                  , pde-loss 3.9209e-09, initc-loss 7.9179e+05                    bc_loss 4.4336e+07\n",
      "Epoch 270, Training-Loss 6.1673e+05, Data-loss 6.0443e+05                  , pde-loss 3.1369e-09, initc-loss 7.8958e+05                    bc_loss 4.4102e+07\n",
      "Epoch 280, Training-Loss 6.1994e+05, Data-loss 6.0768e+05                  , pde-loss 2.7258e-09, initc-loss 7.8740e+05                    bc_loss 4.3871e+07\n",
      "Epoch 290, Training-Loss 6.1281e+05, Data-loss 6.0059e+05                  , pde-loss 2.3345e-09, initc-loss 7.8523e+05                    bc_loss 4.3642e+07\n",
      "Epoch 300, Training-Loss 6.0748e+05, Data-loss 5.9531e+05                  , pde-loss 4.3191e-09, initc-loss 7.8296e+05                    bc_loss 4.3404e+07\n",
      "Epoch 310, Training-Loss 5.9896e+05, Data-loss 5.8684e+05                  , pde-loss 2.2793e-09, initc-loss 7.8073e+05                    bc_loss 4.3169e+07\n",
      "Epoch 320, Training-Loss 6.0930e+05, Data-loss 5.9722e+05                  , pde-loss 1.9695e-09, initc-loss 7.7854e+05                    bc_loss 4.2939e+07\n",
      "Epoch 330, Training-Loss 6.1679e+05, Data-loss 6.0476e+05                  , pde-loss 1.8312e-09, initc-loss 7.7639e+05                    bc_loss 4.2712e+07\n",
      "Epoch 340, Training-Loss 6.1045e+05, Data-loss 5.9845e+05                  , pde-loss 1.5201e-09, initc-loss 7.7425e+05                    bc_loss 4.2489e+07\n",
      "Epoch 350, Training-Loss 6.0141e+05, Data-loss 5.8946e+05                  , pde-loss 1.2665e-09, initc-loss 7.7214e+05                    bc_loss 4.2267e+07\n",
      "Epoch 360, Training-Loss 5.9056e+05, Data-loss 5.7865e+05                  , pde-loss 9.8115e-10, initc-loss 7.7003e+05                    bc_loss 4.2046e+07\n",
      "Epoch 370, Training-Loss 6.0314e+05, Data-loss 5.9128e+05                  , pde-loss 2.2274e-09, initc-loss 7.6781e+05                    bc_loss 4.1815e+07\n",
      "Epoch 380, Training-Loss 6.0590e+05, Data-loss 5.9408e+05                  , pde-loss 1.3067e-09, initc-loss 7.6562e+05                    bc_loss 4.1587e+07\n",
      "Epoch 390, Training-Loss 6.0298e+05, Data-loss 5.9121e+05                  , pde-loss 1.0463e-09, initc-loss 7.6346e+05                    bc_loss 4.1362e+07\n",
      "Epoch 400, Training-Loss 5.9608e+05, Data-loss 5.8436e+05                  , pde-loss 8.9794e-10, initc-loss 7.6132e+05                    bc_loss 4.1139e+07\n",
      "Epoch 410, Training-Loss 5.9846e+05, Data-loss 5.8678e+05                  , pde-loss 8.2829e-10, initc-loss 7.5921e+05                    bc_loss 4.0920e+07\n",
      "Epoch 420, Training-Loss 5.9172e+05, Data-loss 5.8008e+05                  , pde-loss 8.0177e-10, initc-loss 7.5711e+05                    bc_loss 4.0702e+07\n",
      "Epoch 430, Training-Loss 5.8690e+05, Data-loss 5.7531e+05                  , pde-loss 7.0341e-10, initc-loss 7.5504e+05                    bc_loss 4.0487e+07\n",
      "Epoch 440, Training-Loss 5.8307e+05, Data-loss 5.7151e+05                  , pde-loss 5.1499e-10, initc-loss 7.5298e+05                    bc_loss 4.0274e+07\n",
      "Epoch 450, Training-Loss 5.7889e+05, Data-loss 5.6737e+05                  , pde-loss 3.5991e-10, initc-loss 7.5091e+05                    bc_loss 4.0060e+07\n",
      "Epoch 460, Training-Loss 5.7527e+05, Data-loss 5.6380e+05                  , pde-loss 1.2202e-09, initc-loss 7.4872e+05                    bc_loss 3.9834e+07\n",
      "Epoch 470, Training-Loss 5.7863e+05, Data-loss 5.6721e+05                  , pde-loss 7.2989e-10, initc-loss 7.4655e+05                    bc_loss 3.9611e+07\n",
      "Epoch 480, Training-Loss 5.7476e+05, Data-loss 5.6338e+05                  , pde-loss 5.7885e-10, initc-loss 7.4442e+05                    bc_loss 3.9391e+07\n",
      "Epoch 490, Training-Loss 5.7948e+05, Data-loss 5.6814e+05                  , pde-loss 5.3325e-10, initc-loss 7.4231e+05                    bc_loss 3.9174e+07\n",
      "Epoch 500, Training-Loss 5.7085e+05, Data-loss 5.5955e+05                  , pde-loss 4.5174e-10, initc-loss 7.4022e+05                    bc_loss 3.8960e+07\n",
      "Epoch 510, Training-Loss 5.7370e+05, Data-loss 5.6245e+05                  , pde-loss 4.4455e-10, initc-loss 7.3815e+05                    bc_loss 3.8748e+07\n",
      "Epoch 520, Training-Loss 5.6942e+05, Data-loss 5.5820e+05                  , pde-loss 4.2449e-10, initc-loss 7.3610e+05                    bc_loss 3.8538e+07\n",
      "Epoch 530, Training-Loss 5.6898e+05, Data-loss 5.5781e+05                  , pde-loss 3.6813e-10, initc-loss 7.3406e+05                    bc_loss 3.8329e+07\n",
      "Epoch 540, Training-Loss 5.6501e+05, Data-loss 5.5388e+05                  , pde-loss 3.3437e-10, initc-loss 7.3204e+05                    bc_loss 3.8122e+07\n",
      "Epoch 550, Training-Loss 5.7294e+05, Data-loss 5.6185e+05                  , pde-loss 3.3624e-10, initc-loss 7.3002e+05                    bc_loss 3.7917e+07\n",
      "Epoch 560, Training-Loss 5.6641e+05, Data-loss 5.5536e+05                  , pde-loss 3.2156e-10, initc-loss 7.2802e+05                    bc_loss 3.7713e+07\n",
      "Epoch 570, Training-Loss 5.6196e+05, Data-loss 5.5095e+05                  , pde-loss 2.7181e-10, initc-loss 7.2602e+05                    bc_loss 3.7510e+07\n",
      "Epoch 580, Training-Loss 5.5481e+05, Data-loss 5.4384e+05                  , pde-loss 2.5179e-10, initc-loss 7.2404e+05                    bc_loss 3.7308e+07\n",
      "Epoch 590, Training-Loss 5.4792e+05, Data-loss 5.3699e+05                  , pde-loss 2.0708e-10, initc-loss 7.2206e+05                    bc_loss 3.7108e+07\n",
      "Epoch 600, Training-Loss 5.6012e+05, Data-loss 5.4923e+05                  , pde-loss 1.8121e-10, initc-loss 7.2010e+05                    bc_loss 3.6909e+07\n",
      "Epoch 610, Training-Loss 5.5827e+05, Data-loss 5.4742e+05                  , pde-loss 1.0956e-10, initc-loss 7.1814e+05                    bc_loss 3.6711e+07\n",
      "Epoch 620, Training-Loss 5.5389e+05, Data-loss 5.4308e+05                  , pde-loss 2.4879e-10, initc-loss 7.1612e+05                    bc_loss 3.6506e+07\n",
      "Epoch 630, Training-Loss 5.5374e+05, Data-loss 5.4297e+05                  , pde-loss 3.3088e-10, initc-loss 7.1398e+05                    bc_loss 3.6290e+07\n",
      "Epoch 640, Training-Loss 5.4917e+05, Data-loss 5.3844e+05                  , pde-loss 2.0456e-10, initc-loss 7.1189e+05                    bc_loss 3.6080e+07\n",
      "Epoch 650, Training-Loss 5.4512e+05, Data-loss 5.3444e+05                  , pde-loss 1.5182e-10, initc-loss 7.0983e+05                    bc_loss 3.5873e+07\n",
      "Epoch 660, Training-Loss 5.4034e+05, Data-loss 5.2969e+05                  , pde-loss 1.2007e-10, initc-loss 7.0779e+05                    bc_loss 3.5667e+07\n",
      "Epoch 670, Training-Loss 5.4863e+05, Data-loss 5.3803e+05                  , pde-loss 1.7398e-10, initc-loss 7.0571e+05                    bc_loss 3.5460e+07\n",
      "Epoch 680, Training-Loss 5.3691e+05, Data-loss 5.2635e+05                  , pde-loss 3.2304e-10, initc-loss 7.0349e+05                    bc_loss 3.5237e+07\n",
      "Epoch 690, Training-Loss 5.3871e+05, Data-loss 5.2820e+05                  , pde-loss 2.0043e-10, initc-loss 7.0135e+05                    bc_loss 3.5023e+07\n",
      "Epoch 700, Training-Loss 5.4724e+05, Data-loss 5.3676e+05                  , pde-loss 2.0930e-10, initc-loss 6.9923e+05                    bc_loss 3.4811e+07\n",
      "Epoch 710, Training-Loss 5.3697e+05, Data-loss 5.2654e+05                  , pde-loss 1.7735e-10, initc-loss 6.9714e+05                    bc_loss 3.4603e+07\n",
      "Epoch 720, Training-Loss 5.3615e+05, Data-loss 5.2576e+05                  , pde-loss 1.6300e-10, initc-loss 6.9507e+05                    bc_loss 3.4397e+07\n",
      "Epoch 730, Training-Loss 5.3029e+05, Data-loss 5.1994e+05                  , pde-loss 1.5431e-10, initc-loss 6.9303e+05                    bc_loss 3.4194e+07\n",
      "Epoch 740, Training-Loss 5.4089e+05, Data-loss 5.3058e+05                  , pde-loss 1.5993e-10, initc-loss 6.9100e+05                    bc_loss 3.3992e+07\n",
      "Epoch 750, Training-Loss 5.3056e+05, Data-loss 5.2029e+05                  , pde-loss 1.3212e-10, initc-loss 6.8898e+05                    bc_loss 3.3792e+07\n",
      "Epoch 760, Training-Loss 5.2345e+05, Data-loss 5.1322e+05                  , pde-loss 1.3239e-10, initc-loss 6.8698e+05                    bc_loss 3.3594e+07\n",
      "Epoch 770, Training-Loss 5.2568e+05, Data-loss 5.1549e+05                  , pde-loss 1.2074e-10, initc-loss 6.8499e+05                    bc_loss 3.3398e+07\n",
      "Epoch 780, Training-Loss 5.2724e+05, Data-loss 5.1709e+05                  , pde-loss 1.1990e-10, initc-loss 6.8302e+05                    bc_loss 3.3203e+07\n",
      "Epoch 790, Training-Loss 5.2032e+05, Data-loss 5.1021e+05                  , pde-loss 1.1102e-10, initc-loss 6.8105e+05                    bc_loss 3.3009e+07\n",
      "Epoch 800, Training-Loss 5.2069e+05, Data-loss 5.1062e+05                  , pde-loss 9.8153e-11, initc-loss 6.7909e+05                    bc_loss 3.2816e+07\n",
      "Epoch 810, Training-Loss 5.2513e+05, Data-loss 5.1510e+05                  , pde-loss 1.0652e-10, initc-loss 6.7714e+05                    bc_loss 3.2625e+07\n",
      "Epoch 820, Training-Loss 5.2557e+05, Data-loss 5.1558e+05                  , pde-loss 1.0633e-10, initc-loss 6.7521e+05                    bc_loss 3.2435e+07\n",
      "Epoch 830, Training-Loss 5.2095e+05, Data-loss 5.1099e+05                  , pde-loss 9.3597e-11, initc-loss 6.7328e+05                    bc_loss 3.2246e+07\n",
      "Epoch 840, Training-Loss 5.1552e+05, Data-loss 5.0560e+05                  , pde-loss 8.8437e-11, initc-loss 6.7135e+05                    bc_loss 3.2058e+07\n",
      "Epoch 850, Training-Loss 5.1527e+05, Data-loss 5.0539e+05                  , pde-loss 8.2923e-11, initc-loss 6.6944e+05                    bc_loss 3.1871e+07\n",
      "Epoch 860, Training-Loss 5.1589e+05, Data-loss 5.0604e+05                  , pde-loss 8.2218e-11, initc-loss 6.6753e+05                    bc_loss 3.1685e+07\n",
      "Epoch 870, Training-Loss 5.0965e+05, Data-loss 4.9985e+05                  , pde-loss 7.6731e-11, initc-loss 6.6563e+05                    bc_loss 3.1499e+07\n",
      "Epoch 880, Training-Loss 5.0864e+05, Data-loss 4.9887e+05                  , pde-loss 7.2726e-11, initc-loss 6.6373e+05                    bc_loss 3.1315e+07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses, val_losses = training_loop(epochs, model, loss_fn_data, optimizer,train_loader)  # Train the model\n",
    " \n",
    "test_losses = test_loop(epochs, model, loss_fn_data, optimizer, train_loader, test_loader)  # Test the model\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_nn = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).cpu().detach().numpy() # Get the predictions from the model\n",
    "\n",
    "temp_nn = temp_nn.reshape(num_steps+1, num_points) # Reshape the predictions to a 2D array\n",
    "time_ss= np.linspace(0, time_end, num_steps+1)\n",
    "plt.figure\n",
    "plt.plot(time_ss, temp_nn[:,num_points//2], label='Predicted Temperature')\n",
    "plt.plot(time_ss, temperature_history[:,num_points//2], label='Actual Temperature')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.title('Predicted vs Actual Temperature at x = 7.5mm')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
