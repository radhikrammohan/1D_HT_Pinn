{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import csv\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SubsetRandomSampler, random_split\n",
    "\n",
    "from pinn_loss import loss_fn_data, l1_regularization, pde_loss, boundary_loss, ic_loss, accuracy\n",
    "from Input_vec_gen import input_gen, temp_data_gen, st_gen, meshgen, input_vgen\n",
    "from Datagen import sim1d\n",
    "from sampler import g_sampler, strat_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stable\n",
      "Stable\n",
      "Stable\n"
     ]
    }
   ],
   "source": [
    "L1 = sim1d(rho_l=2460.0, rho_s=2710.0, k_l=104.0, k_s= 96.2, cp_l=1245.3, cp_s=963.0, \\\n",
    "            t_surr=298.0, L_fusion=400670, temp_init=913.0, htc_l=10.0,htc_r= 12.0, length =15.0e-3)\n",
    "                \n",
    "L2 = sim1d(rho_l=2460.0, rho_s=2710.0, k_l=104.0, k_s= 96.2, cp_l=1245.3, cp_s=963.0, \\\n",
    "            t_surr=298.0, L_fusion=389000, temp_init=913.0, htc_l=12.0,htc_r= 12.0, length =15.0e-3)\n",
    "L3 = sim1d(rho_l=2460.0, rho_s=2710.0, k_l=104.0, k_s= 96.2, cp_l=1245.3, cp_s=963.0, \n",
    "               t_surr=298.0, L_fusion=377330, temp_init=913.0, htc_l=15.0,htc_r= 12.0, length =15.0e-3)          \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the input and output vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 15.0e-3 # length of the rod\n",
    "time_end =40 # end time of the simulation\n",
    "htc_l_1,htc_l_2,htc_l_3 = 10.0,12.0,15.0 # htc values for the 3 simulations\n",
    "L_f1,L_f2,L_f3 = 400670.0,389000.0,377330.0 # Latent heat of fusion values for the 3 simulations\n",
    "\n",
    "# Extract the temperature, space and time vectors from the simulation data\n",
    "T1,space_1,time_1 = L1[0],L1[1],L1[2] # 1st element is the temperature matrix, 2nd is the space vector, 3rd is the time vector\n",
    "T2,space_2,time_2 = L2[0],L2[1],L2[2]\n",
    "T3,space_3,time_3 = L3[0],L3[1],L3[2]\n",
    "\n",
    "Temp_1,space_a1, time_a1 = np.array(T1),np.array(space_1),np.array(time_1)\n",
    "Temp_2,space_a2, time_a2 = np.array(T2),np.array(space_2),np.array(time_2)\n",
    "Temp_3,space_a3, time_a3 = np.array(T3),np.array(space_3),np.array(time_3)\n",
    "\n",
    "# generate htc_l values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Normalize the data\n",
    "\n",
    "# Temp_1n,Temp_2n, Temp_3n = scaler.fit_transform(Temp_1),scaler.fit_transform(Temp_2),scaler.fit_transform(Temp_3)\n",
    "# Space_1n,Space_2n, Space_3n = scaler.fit_transform(space_a1.reshape(-1,1)),scaler.fit_transform(space_a2.reshape(-1,1)),scaler.fit_transform(space_a3.reshape(-1,1))\n",
    "# Time_1n,Time_2n, Time_3n = scaler.fit_transform(time_a1.reshape(-1,1)),scaler.fit_transform(time_a2.reshape(-1,1)),scaler.fit_transform(time_a3.reshape(-1,1))\n",
    "\n",
    "# print(Temp_1n.shape,Space_1n.shape,Time_1n.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## standardize outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'space_tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a1 \u001b[38;5;241m=\u001b[39m \u001b[43minput_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspace_a1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtime_a1\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmgrid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m a2 \u001b[38;5;241m=\u001b[39m input_gen(space_a2,time_a2,\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmgrid\u001b[39m\u001b[38;5;124m'\u001b[39m,scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m a3 \u001b[38;5;241m=\u001b[39m input_gen(space_a3,time_a3,\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmgrid\u001b[39m\u001b[38;5;124m'\u001b[39m,scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/github/gen3/1D_HT_Pinn/Data-prep/PINN/MushyZone-PINN/Pinn-Variables/Non-spartan/Input_vec_gen.py:32\u001b[0m, in \u001b[0;36minput_gen\u001b[0;34m(space, time, type, scale)\u001b[0m\n\u001b[1;32m     30\u001b[0m time \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(time\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     31\u001b[0m space, time \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(space, time)\n\u001b[0;32m---> 32\u001b[0m space \u001b[38;5;241m=\u001b[39m \u001b[43mspace_tr\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     33\u001b[0m time \u001b[38;5;241m=\u001b[39m time_tr\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m input_vec \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((space,time))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'space_tr' is not defined"
     ]
    }
   ],
   "source": [
    "a1 = input_gen(space_a1,time_a1,type='mgrid',scale=True)\n",
    "a2 = input_gen(space_a2,time_a2,type='mgrid',scale=True)\n",
    "a3 = input_gen(space_a3,time_a3,type='mgrid',scale=True)\n",
    "\n",
    "sp1,t_1 = a1[1],a1[2]\n",
    "sp2,t_2 = a2[1],a2[2]\n",
    "sp3,t_3 = a3[1],a3[2]\n",
    "\n",
    "# print(sp1.shape,t_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def htc_gen(htc_l_1,space_a1):\n",
    "    htc = np.ones_like(space_a1)*htc_l_1\n",
    "    \n",
    "    return htc\n",
    "\n",
    "htcl_1 = htc_gen(htc_l_1,sp1)\n",
    "htcl_2 = htc_gen(htc_l_2,sp2)\n",
    "htcl_3 = htc_gen(htc_l_3,sp3)\n",
    "\n",
    "print(htcl_1.shape)\n",
    "\n",
    "Lf_1 = htc_gen(L_f1,sp1)\n",
    "Lf_2= htc_gen(L_f2,sp2)\n",
    "Lf_3 = htc_gen(L_f3,sp3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the input vectors for the 3 simulations\n",
    "\n",
    "input_1 = input_vgen(sp1,t_1,htcl_1,Lf_1)\n",
    "# input_1_pde = input_vgen(sp_pde_1,t_pde_1,htcl_pde_1,Lf_pde_1)\n",
    "# input_1_ic = input_vgen(sp_ic_1,t_ic_1,htcl_ic_1,Lf_ic_1)\n",
    "# input_1_bc_l = input_vgen(sp_bc_l_1,t_bc_l_1,htcl_bc_l_1,Lf_bc_l_1)\n",
    "# input_1_bc_r = input_vgen(sp_bc_r_1,t_bc_r_1,htcl_bc_r_1,Lf_bc_r_1)\n",
    "\n",
    "\n",
    "input_2 = input_vgen(sp2,t_2,htcl_2,Lf_2)\n",
    "# input_2_pde = input_vgen(sp_pde_2,t_pde_2,htcl_pde_2,Lf_pde_2)\n",
    "# input_2_ic = input_vgen(sp_ic_2,t_ic_2,htcl_ic_2,Lf_ic_2)\n",
    "# input_2_bc_l = input_vgen(sp_bc_l_2,t_bc_l_2,htcl_bc_l_2,Lf_bc_l_2)\n",
    "# input_2_bc_r = input_vgen(sp_bc_r_2,t_bc_r_2,htcl_bc_r_2,Lf_bc_r_2)\n",
    "\n",
    "input_3 = input_vgen(sp3,t_3,htcl_3,Lf_3)\n",
    "# input_3_pde = input_vgen(sp_pde_3,t_pde_3,htcl_pde_3,Lf_pde_3)\n",
    "# input_3_ic = input_vgen(sp_ic_3,t_ic_3,htcl_ic_3,Lf_ic_3)\n",
    "# input_3_bc_l = input_vgen(sp_bc_l_3,t_bc_l_3,htcl_bc_l_3,Lf_bc_l_3)\n",
    "# input_3_bc_r = input_vgen(sp_bc_r_3,t_bc_r_3,htcl_bc_r_3,Lf_bc_r_3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = temp_data_gen(Temp_1) \n",
    "m2 = temp_data_gen(Temp_2)\n",
    "m3 = temp_data_gen(Temp_3)\n",
    "\n",
    "t1,t1_pde,t1_ic,t1_bc_l,t1_bc_r = m1[0],m1[1],m1[2],m1[3],m1[4]\n",
    "t2,t2_pde,t2_ic,t2_bc_l,t2_bc_r = m2[0],m2[1],m2[2],m2[3],m2[4]\n",
    "t3,t3_pde,t3_ic,t3_bc_l,t3_bc_r = m3[0],m3[1],m3[2],m3[3],m3[4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_all = np.concatenate((input_1,input_2,input_3),axis=0)\n",
    "# inp_pde_all = np.concatenate((input_1_pde,input_2_pde,input_3_pde),axis=0)\n",
    "# inp_ic_all = np.concatenate((input_1_ic,input_2_ic,input_3_ic),axis=0)\n",
    "# inp_bc_l_all = np.concatenate((input_1_bc_l,input_2_bc_l,input_3_bc_l),axis=0)\n",
    "# inp_bc_r_all = np.concatenate((input_1_bc_r,input_2_bc_r,input_3_bc_r),axis=0)\n",
    "\n",
    "temp_all = np.concatenate((t1,t2,t3),axis=0)\n",
    "# temp_pde_all = np.concatenate((t1_pde,t2_pde,t3_pde),axis=0)\n",
    "# temp_ic_all = np.concatenate((t1_ic,t2_ic,t3_ic),axis=0)\n",
    "# temp_bc_l_all = np.concatenate((t1_bc_l,t2_bc_l,t3_bc_l),axis=0)\n",
    "# temp_bc_r_all = np.concatenate((t1_bc_r,t2_bc_r,t3_bc_r),axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the Analytical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt a subplot of the three plots\n",
    "space_cord, time_cord = np.meshgrid(np.arange(T1.shape[1]), np.arange(T1.shape[0]))\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n",
    "axs[0].pcolormesh(space_cord, time_cord, T1, cmap='coolwarm', shading='auto')\n",
    "axs[0].set_title('htc_l = 10.0, L_fusion = 400670')\n",
    "axs[0].set_xlabel('Space')\n",
    "axs[0].set_ylabel('Time')\n",
    "axs[1].pcolormesh(space_cord, time_cord, T2, cmap='coolwarm', shading='auto')\n",
    "axs[1].set_title('htc_l = 12.0, L_fusion = 389000')\n",
    "axs[1].set_xlabel('Space')\n",
    "axs[1].set_ylabel('Time')\n",
    "axs[2].pcolormesh(space_cord, time_cord, T3, cmap='coolwarm', shading='auto')\n",
    "axs[2].set_title('htc_l = 15.0, L_fusion = 377330')\n",
    "axs[2].set_xlabel('Space')\n",
    "axs[2].set_ylabel('Time')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device2 = torch.device('cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler Preparation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_surr = 298.0 # Surrounding temperature\n",
    "t_init = 913.0 # Initial temperature\n",
    "inputs_sam, T_sam = strat_sampler(inp_all,temp_all,0.01) # Sample the input and output data\n",
    "# inputs_pde_sam, T_pde_sam = strat_sampler(inp_pde_all,temp_pde_all,0.1) # Sample the input and output data for pde \n",
    "# inputs_i_sam,T_ic_sam = strat_sampler(inp_ic_all,temp_ic_all,1) # Sample the input and output data for initial condition\n",
    "# inputs_b_l_sam,T_bcl_sam = strat_sampler(inp_bc_l_all,temp_bc_l_all,0.2) # Sample the input and output data for boundary condition left\n",
    "# inputs_b_r_sam, T_bcr_sam = strat_sampler(inp_bc_r_all, temp_bc_r_all, 0.2) # Sample the input and output data for boundary condition right\n",
    "\n",
    "print(inputs_sam.shape,T_sam.shape)\n",
    "# print(inputs_pde_sam.shape,T_pde_sam.shape)\n",
    "# print(inputs_i_sam.shape,T_ic_sam.shape)\n",
    "# print(inputs_b_l_sam.shape,T_bcl_sam.shape)\n",
    "# print(inputs_b_r_sam.shape,T_bcr_sam.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor the Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_sam, T_sam = torch.tensor(inputs_sam).float(), torch.tensor(T_sam).float()\n",
    "# inp_pde_sam, T_pde_sam = torch.tensor(inputs_pde_sam).float(), torch.tensor(T_pde_sam).float()\n",
    "# inp_i_sam, T_ic_sam = torch.tensor(inputs_i_sam).float(), torch.tensor(T_ic_sam).float()\n",
    "# inp_b_l_sam, T_bcl_sam = torch.tensor(inputs_b_l_sam).float(), torch.tensor(T_bcl_sam).float()\n",
    "# inp_b_r_sam, T_bcr_sam = torch.tensor(inputs_b_r_sam).float(), torch.tensor(T_bcr_sam).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inp_sam,test_inp_sam,train_T_sam,test_T_sam = train_test_split(inp_sam,T_sam,\\\n",
    "                                                                     test_size=0.2,random_state=42)\n",
    "# train_inp_pde_sam,test_inp_pde_sam,train_T_pde_sam,test_T_pde_sam = train_test_split(inp_pde_sam,T_pde_sam,\\\n",
    "#                                                                                      test_size=0.2,random_state=42)\n",
    "# train_inp_i_sam,test_inp_i_sam,train_T_i_sam,test_T_i_sam = train_test_split(inp_i_sam,T_ic_sam,\\\n",
    "#                                                                              test_size=0.2,random_state=42)\n",
    "# train_inp_b_l_sam,test_inp_b_l_sam,train_T_b_l_sam,test_T_b_l_sam = train_test_split(inp_b_l_sam,T_bcl_sam, \\\n",
    "#                                                                                      test_size=0.2,random_state=42)\n",
    "# train_inp_b_r_sam,test_inp_b_r_sam,train_T_b_r_sam,test_T_b_r_sam = train_test_split(inp_b_r_sam,T_bcr_sam,\\\n",
    "#                                                                                      test_size=0.2,random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PinnDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PinnDataset(train_inp_sam, train_T_sam)\n",
    "test_dataset = PinnDataset(test_inp_sam, test_T_sam)\n",
    "\n",
    "# train_pde_dataset = PinnDataset(train_inp_pde_sam, train_T_pde_sam)\n",
    "# test_pde_dataset = PinnDataset(test_inp_pde_sam, test_T_pde_sam)\n",
    "\n",
    "# train_ic_dataset = PinnDataset(train_inp_i_sam, train_T_i_sam)\n",
    "# test_ic_dataset = PinnDataset(test_inp_i_sam, test_T_i_sam)\n",
    "\n",
    "# train_b_l_dataset = PinnDataset(train_inp_b_l_sam, train_T_b_l_sam)\n",
    "# test_b_l_dataset = PinnDataset(test_inp_b_l_sam, test_T_b_l_sam)\n",
    "\n",
    "# train_b_r_dataset = PinnDataset(train_inp_b_r_sam, train_T_b_r_sam)\n",
    "# test_b_r_dataset = PinnDataset(test_inp_b_r_sam, test_T_b_r_sam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try random sampler if it doesnt work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# train_pde_loader = DataLoader(train_pde_dataset, batch_size=64, shuffle=True)\n",
    "# test_pde_loader = DataLoader(test_pde_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# train_ic_loader = DataLoader(train_ic_dataset, batch_size=64, shuffle=True)\n",
    "# test_ic_loader = DataLoader(test_ic_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# train_b_l_loader = DataLoader(train_b_l_dataset, batch_size=64, shuffle=True)\n",
    "# test_b_l_loader = DataLoader(test_b_l_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# train_b_r_loader = DataLoader(train_b_r_dataset, batch_size=64, shuffle=True)\n",
    "# test_b_r_loader = DataLoader(test_b_r_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data into interior , boundary and initial condition\n",
    "class Pinn_Var(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size): # This is the constructor\n",
    "        super(Pinn_Var, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t, h,Lf):                               # This is the forward pass\n",
    "        input_features = torch.cat([x, t, h,Lf], dim=1)          # Concatenate the input features\n",
    "        m = self.base(input_features)                                 # Pass through the third layer\n",
    "        return m                    # Return the output of the network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 100\n",
    "learning_rate = 0.009\n",
    "epochs = 60000\n",
    "# alpha = 0.01  # Adjust this value based on your problem\n",
    "# boundary_value = 313.0\n",
    "# initial_value = init_temp\n",
    "# Initialize the model\n",
    "model = Pinn_Var(input_size=4, hidden_size=hidden_size,output_size=1).to(device)\n",
    "lambd = 0.1\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training & Testing Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, model, loss_fn_data, optimizer, train_loader,test_dataloader):\n",
    "    train_losses = []  # Initialize the list to store the training losses\n",
    "    # val_losses = []    # Initialize the list to store the validation losses\n",
    "    test_losses = []   # Initialize the list to store the test losses\n",
    "    data_losses = []   # Initialize the list to store the data losses\n",
    "    \n",
    "    #Initialize the list to store the boundary condition losses\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                                                           # Set the model to training mode\n",
    "        train_loss = 0                                                                              # Initialize the training loss\n",
    "        train_accuracy = 0\n",
    "        \n",
    "        for batch in (train_loader):                                                          # Loop through the training dataloader\n",
    "            \n",
    "            train_inputs_sample, train_T_sam= batch                                                             # Get the inputs and the true values\n",
    "            \n",
    "                                                                         # Get the inputs and the true values\n",
    "\n",
    "            train_inputs_sample, train_T_sam = train_inputs_sample.to(device), train_T_sam.to(device)                                                             # Get the inputs and the true values\n",
    "\n",
    "            # print(train_inputs_sample[:1,:],train_T_sam[:1,:])\n",
    "\n",
    "            # print(train_inputs_sample.sh)\n",
    "            \n",
    "            # print(inputs.shape)\n",
    "            # print(inputs_init.shape)\n",
    "            # print(inputs_left.shape)\n",
    "            # print(inputs_right.shape)\n",
    "\n",
    "            optimizer.zero_grad()                                                                    # Zero the gradients\n",
    "            \n",
    "            # Forward pass\n",
    "            u_pred = model(train_inputs_sample[:,0].unsqueeze(1), train_inputs_sample[:,1].unsqueeze(1),\\\n",
    "                           train_inputs_sample[:,2].unsqueeze(1),train_inputs_sample[:,3].unsqueeze(1))                       # Get the predictions\n",
    "                         \n",
    "\n",
    "            # Loss calculation\n",
    "            data_loss = loss_fn_data(u_pred, train_T_sam)                                              # Calculate the data loss\n",
    "            \n",
    "            \n",
    "            loss = data_loss \n",
    "            train_accuracy += accuracy(u_pred, train_T_sam)                                                              # Calculate the total loss\n",
    "            # Backpropagation\n",
    "            loss.backward(retain_graph=True)                                                        # Backpropagate the gradients\n",
    "            \n",
    "            optimizer.step()                                                                           # Update the weights\n",
    "            \n",
    "            train_loss += loss.item()                                                           # Add the loss to the training set loss  \n",
    "            \n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0\n",
    "        # with torch.no_grad(): \n",
    "        for batch in test_dataloader:\n",
    "            test_inputs_sample, test_temp_inp_sample= batch\n",
    "            test_inputs_sample, test_temp_inp_sample= test_inputs_sample.to(device), test_temp_inp_sample.to(device)\n",
    "            u_pred = model(test_inputs_sample[:,0].unsqueeze(1), test_inputs_sample[:,1].unsqueeze(1),\\\n",
    "                           test_inputs_sample[:,2].unsqueeze(1),test_inputs_sample[:,3].unsqueeze(1))\n",
    "            data_loss_t = loss_fn_data(u_pred, test_temp_inp_sample)\n",
    "            # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "            # loss = data_loss  + l1_regularization_loss\n",
    "            \n",
    "            loss = data_loss_t\n",
    "            test_accuracy = accuracy(u_pred, test_temp_inp_sample)\n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "                                                           # Append the training loss to the list of training losses\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch},| Training-Loss {train_loss:.4e},| test-loss {test_loss:.4e}\") \n",
    "\n",
    "    return train_losses, test_losses,data_losses                                                      # Return the training and validation losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(epochs, model, loss_fn_data, optimizer, train_dataloader, test_dataloader):\n",
    "    for epoch in range(epochs):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0\n",
    "        with torch.no_grad():   \n",
    "            for batch in test_dataloader:\n",
    "                test_inputs_sample, test_temp_inp_sample= batch\n",
    "                test_inputs_sample, test_temp_inp_sample= test_inputs_sample.to(device), test_temp_inp_sample.to(device)\n",
    "                u_pred = model(test_inputs_sample[:,0].unsqueeze(1), test_inputs_sample[:,1].unsqueeze(1))\n",
    "                data_loss = loss_fn_data(u_pred, test_temp_inp_sample)\n",
    "                # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "                # loss = data_loss  + l1_regularization_loss\n",
    "                \n",
    "                loss = data_loss\n",
    "                test_accuracy = accuracy(u_pred, test_temp_inp_sample)\n",
    "                test_loss += loss.item()\n",
    "        test_losses.append(test_loss)\n",
    "        if epochs % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Test-Loss {test_loss:.4e}, Test-Accuracy {test_accuracy:.4e}\")      \n",
    "    return test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_losses, test_losses, data_losses = training_loop(epochs, model, \\\n",
    "                                        loss_fn_data, optimizer,train_loader,test_loader,\\\n",
    "                                            )  # Train the model\n",
    " \n",
    "# test_losses = test_loop(epochs, model, loss_fn_data, optimizer, train_loader, test_loader)  # Test the model\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor(inp_tr_1).float().to(device) # Convert the inputs to a tensor\n",
    "temp_nn = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1),inputs[:,2].unsqueeze(1)).cpu().detach().numpy() # Get the predictions from the model\n",
    "num_points = space_a1.shape[0] # Number of points in the space vector\n",
    "print(temp_nn.shape)\n",
    "temp_nn = temp_nn.reshape(time_a1.shape, space_a1.shape) # Reshape the predictions to a 2D array\n",
    "print(temp_nn.shape)\n",
    "time_ss= np.linspace(0, time_end, time_a1.shape)\n",
    "plt.figure\n",
    "plt.plot(time_ss, temp_nn[:, num_points//2], label='Predicted Temperature')\n",
    "plt.plot(time_ss, T1[:,num_points//2], label='Actual Temperature')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.title('Predicted vs Actual Temperature at x = 7.5mm')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='test Loss')\n",
    "# plt.xticks(np.arange(0, epochs, 10000))\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pde_losses_n = [tensor.cpu().detach().numpy() for tensor in pde_losses]\n",
    "data_losses_n = [tensor.cpu().detach().numpy() for tensor in data_losses]\n",
    "ic_losses_n = [tensor.cpu().detach().numpy() for tensor in ic_losses]\n",
    "bc_losses_n = [tensor.cpu().detach().numpy() for tensor in bc_losses]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data_losses_n, label='Data Loss')\n",
    "plt.plot(pde_losses_n, label='Pde Loss')\n",
    "plt.plot(ic_losses_n, label='IC Loss')\n",
    "plt.plot(bc_losses_n, label='BC Loss')\n",
    "plt.yscale('log')\n",
    "# plt.axhline(y=1e-6, color='red', linestyle='--', label='Near-Zero Line')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_coord, time_coord = np.meshgrid(np.arange(T1.shape[1]), np.arange(T1.shape[0]))\n",
    "\n",
    "# time_coord = time_coord * dt \n",
    "# Create a figure with two subplots\n",
    "print(space_coord.shape,time_coord.shape,T1.shape,temp_nn.shape)\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "\n",
    "# Plot the temperature history on the left subplot\n",
    "im1 = ax1.pcolormesh(space_coord, time_coord, T1, cmap='viridis', shading='auto')\n",
    "ax1.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "ax1.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "ax1.set_title('Temperature Variation Over Time(Analytical Model)',fontname='Times New Roman', fontsize=20)\n",
    "ax1.contour(space_coord, time_coord, T1, colors='red', linewidths=1.0, alpha=0.9)\n",
    "\n",
    "ax1.grid(True)\n",
    "cbar = fig.colorbar(im1, ax=ax1)\n",
    "cbar.ax.invert_yaxis()\n",
    "cbar.set_label('Temperature (K)', rotation=270, labelpad=20, fontname='Times New Roman', fontsize=16)\n",
    "\n",
    "im2 = ax2.pcolormesh(space_coord, time_coord, temp_nn, cmap='viridis', shading='auto')\n",
    "ax2.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "ax2.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "ax2.set_title('Temperature Variation Over Time(PINN Approach)',fontname='Times New Roman', fontsize=20)\n",
    "ax2.contour(space_coord, time_coord, temp_nn, colors='red', linewidths=1.0, alpha=0.9)\n",
    "\n",
    "ax2.grid(True)\n",
    "cbar = fig.colorbar(im2, ax=ax2)\n",
    "cbar.ax.invert_yaxis()\n",
    "cbar.set_label('Temperature (K)', rotation=270, labelpad=20, fontname='Times New Roman', fontsize=16)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
