{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Three Phase Simulation of Alloys and PINN model development \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the simulation of 1D Phase change of aluminium alloy. There will be three phases (solid,liquid and mushy).   \n",
    "\n",
    "The approach used is finite difference method and the physics involved in heat conduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import csv\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "from pinn_loss import loss_fn_data, l1_regularization, pde_loss, boundary_loss, ic_loss, accuracy\n",
    "from Input_vec_gen import input_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the constants and inital geometric domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_l = 3.394878564540885e-05, alpha_s = 3.686205086349929e-05, m_eff = 6.296953764744878e-06\n",
      "dx is 0.0003061224489795918\n",
      "dt is  0.0012711033647622566\n",
      "num_steps is 31469\n",
      "cfl is 0.0012711033647622566\n",
      "stability criteria satisfied\n"
     ]
    }
   ],
   "source": [
    "# Geometry\n",
    "length = 15.0e-3             # Length of the rod\n",
    "\n",
    "# Material properties\n",
    "rho = 2300.0                     # Density of AL380 (kg/m^3)\n",
    "rho_l = 2460.0                   # Density of AL380 (kg/m^3)\n",
    "rho_s = 2710.0                    # Density of AL380 (kg/m^3)\n",
    "rho_m = (rho_l + rho_s )/2       # Desnity in mushy zone is taken as average of liquid and solid density\n",
    "\n",
    "k = 104.0                       # W/m-K\n",
    "k_l = k                       # W/m-K\n",
    "k_s = 96.2                    # W/m-K\n",
    "k_m =  (k_l+k_s)/2                     # W/m-K\n",
    "k_mo = 41.5\n",
    "\n",
    "\n",
    "cp = 1245.3                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_l = cp                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_s = 963.0                 # Specific heat of aluminum (J/kg-K)\n",
    "cp_m =  (cp_l+cp_s)/2                 # Specific heat of mushy zone is taken as average of liquid and solid specific heat\n",
    "# cp_m = cp\n",
    "           # Thermal diffusivity\n",
    "alpha_l = k_l / (rho_l * cp_l) \n",
    "alpha_s = k_s / (rho_s*cp_s)\n",
    "alpha_m = k_m / (rho_m * cp_m)          #`Thermal diffusivity in mushy zone is taken as average of liquid and solid thermal diffusivity`\n",
    "\n",
    "\n",
    "#L_fusion = 3.9e3                 # J/kg\n",
    "L_fusion = 389.0e3               # J/kg  # Latent heat of fusion of aluminum\n",
    "         # Thermal diffusivity\n",
    "\n",
    "\n",
    "T_L = 574.4 +273.0                       #  K -Liquidus Temperature (615 c) AL 380\n",
    "T_S = 497.3 +273.0                     # K- Solidus Temperature (550 C)\n",
    "m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "print (f\"alpha_l = {alpha_l}, alpha_s = {alpha_s}, m_eff = {m_eff}\")\n",
    "\n",
    "# htc = 10.0                   # W/m^2-K\n",
    "# q = htc*(919.0-723.0)\n",
    "# q = 10000.0\n",
    "\n",
    "\n",
    "num_points = 50                        # Number of spatial points\n",
    "dx = length / (num_points - 1)         # Distance between two spatial points\n",
    "print('dx is',dx)\n",
    "\n",
    "                                                              \n",
    "# Time Discretization  \n",
    "# \n",
    "time_end = 40        # seconds                         \n",
    "\n",
    "maxi = max(alpha_s,alpha_l,alpha_m)\n",
    "dt = abs(0.5*((dx**2) /maxi)) \n",
    "\n",
    "print('dt is ',dt)\n",
    "num_steps = round(time_end/dt)\n",
    "print('num_steps is',num_steps)\n",
    "cfl = 0.5 *(dx**2/max(alpha_l,alpha_s,alpha_m))\n",
    "print('cfl is',cfl)\n",
    "\n",
    "time_steps = np.linspace(0, time_end, num_steps + 1)\n",
    "step_coeff = dt / (dx ** 2)\n",
    "\n",
    "if dt <= cfl:\n",
    "    print('stability criteria satisfied')\n",
    "else:\n",
    "    print('stability criteria not satisfied')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial and Boundary Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_init = 919.0\n",
    "# Initial temperature and phase fields\n",
    "temperature = np.full(num_points+2, 919.0)            # Initial temperature of the rod with ghost points at both ends\n",
    "phase = np.zeros(num_points+2)*0.0                    # Initial phase of the rod with ghost points at both ends\n",
    "\n",
    "# Set boundary conditions\n",
    "# temperature[-1] = 919.0 \n",
    "phase[-1] = 1.0\n",
    "\n",
    "# temperature[0] = 919.0 #(40 C)\n",
    "phase[0] = 1.0\n",
    "\n",
    "# Store initial state in history\n",
    "temperature_history = [temperature.copy()]    # List to store temperature at each time step\n",
    "phi_history = [phase.copy()]                    # List to store phase at each time step\n",
    "temp_init = temperature.copy()                 # Initial temperature of the rod\n",
    "# print(temperature_history,phi_history)\n",
    "# Array to store temperature at midpoint over time\n",
    "midpoint_index = num_points // 2                          # Index of the midpoint\n",
    "\n",
    "midpoint_temperature_history = [temperature[midpoint_index]]            # List to store temperature at midpoint over time\n",
    "dm = 60.0e-3                                                            # die thickness in m\n",
    "\n",
    "# r_m =  (k_mo / dm) + (1/htc)\n",
    "\n",
    "t_surr = 500.0                                        # Surrounding temperature in K\n",
    "# t_surr = h()\n",
    "\n",
    "def kramp(temp,v1,v2,T_L,T_s):                                      # Function to calculate thermal conductivity in Mushy Zone\n",
    "        slope = (v1-v2)/(T_L-T_S)\n",
    "        if temp > T_L:\n",
    "            k_m = k_l\n",
    "        elif temp < T_S:\n",
    "            k_m = k_s\n",
    "        else:\n",
    "            k_m = k_s + slope*(temp-T_S)\n",
    "        return k_m\n",
    "\n",
    "def cp_ramp(temp,v1,v2,T_L,T_s):                                    # Function to calculate specific heat capacity in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        cp_m = cp_l\n",
    "    elif temp < T_S:\n",
    "        cp_m = cp_s\n",
    "    else:\n",
    "        cp_m = cp_s + slope*(temp-T_S)\n",
    "    return cp_m\n",
    "\n",
    "def rho_ramp(temp,v1,v2,T_L,T_s):                                       # Function to calculate density in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        rho_m = rho_l\n",
    "    elif temp < T_S:\n",
    "        rho_m = rho_s\n",
    "    else:\n",
    "        rho_m = rho_s + slope*(temp-T_S)\n",
    "    return rho_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the HT equation and phase change numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for m in range(1, num_steps+1):                                                                            # time loop\n",
    "    htc = 10.0                   # htc of Still air in W/m^2-K\n",
    "    q1 = htc*(temp_init[0]-t_surr)   # Heat flux at the left boundary\n",
    "    \n",
    "    # print(f\"q1 is {q1}\")\n",
    "    temperature[0] = temp_init[0] + alpha_l * step_coeff * ((2.0*temp_init[1]) - (2.0 * temp_init[0])-(2.0*dx*(q1)))  # Update boundary condition temperature\n",
    "    \n",
    "    q2 = htc*(temp_init[-1]-t_surr)                   # Heat flux at the right boundary\n",
    "    temperature[-1] = temp_init[-1] + alpha_l * step_coeff * ((2.0*temp_init[-2]) - (2.0 * temp_init[-1])-(2.0*dx*(q2)))  # Update boundary condition temperature\n",
    "    \n",
    "    for n in range(1,num_points+1):              # space loop, adjusted range\n",
    "       \n",
    "        if temperature[n] >= T_L:\n",
    "            temperature[n] += ((alpha_l * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            phase[n] = 0\n",
    "            \n",
    "            # print(f\" Time-Step{m},Spatial point{n},Temperature{temperature[n]}\")\n",
    "        elif T_S < temperature[n] < T_L:\n",
    "            \n",
    "            k_m = kramp(temperature[n],k_l,k_s,T_L,T_S)\n",
    "            cp_m = cp_ramp(temperature[n],cp_l,cp_s,T_L,T_S)\n",
    "            rho_m = rho_ramp(temperature[n],rho_l,rho_s,T_L,T_S)\n",
    "            m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "            \n",
    "            temperature[n] += ((m_eff * step_coeff)* (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            \n",
    "            phase[n] = (T_L - temperature[n]) / (T_L - T_S)\n",
    "            # print(m,n,temperature[n],phase[n])\n",
    "         \n",
    "        elif temperature[n]<T_S:\n",
    "            temperature[n] += ((alpha_s * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n])+ temp_init[n-1]))\n",
    "            phase[n] = 1\n",
    "                     \n",
    "        else:\n",
    "            print(\"ERROR: should not be here\")\n",
    "\n",
    "     \n",
    "          \n",
    "    temperature = temperature.copy()                                                                # Update temperature\n",
    "    phase = phase.copy()                                                                            # Update phase\n",
    "    temp_init = temperature.copy()                                                                  # Update last time step temperature\n",
    "    temperature_history.append(temperature.copy())                                                  # Append the temperature history to add ghost points\n",
    "    phi_history.append(phase.copy())                                                                # Append the phase history to add ghost points\n",
    "    midpoint_temperature_history.append(temperature[midpoint_index])                                # Store midpoint temperature\n",
    "    \n",
    "    \n",
    "    # print(f\"Step {m}, Temperature: {temperature}\")\n",
    "    \n",
    "\n",
    "\n",
    "# print(midpoint_temperature_history)\n",
    "#print(phi_history)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot temperature history for debugging\n",
    "# temperature_history_1 = np.array(temperature_history)\n",
    "# print(temperature_history_1.shape)\n",
    "# time_ss= np.linspace(0, time_end, num_steps+1)\n",
    "# # print(time_ss.shape)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(time_ss, midpoint_temperature_history, label='Midpoint Temperature')\n",
    "# plt.axhline(y=T_L, color='r', linestyle='--', label='Liquidus Temperature')\n",
    "# plt.axhline(y=T_S, color='g', linestyle='--', label='Solidus Temperature')\n",
    "# plt.xlabel('Time(s)')\n",
    "# plt.ylabel('Temperature (K)')\n",
    "# plt.title('Temperature Distribution Over Time at x = 7.5mm') \n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data into Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_history = np.array(temperature_history)\n",
    "\n",
    "phi_history = np.array(phi_history)\n",
    "\n",
    "t_hist = np.array(temperature_history[:,1:-1])\n",
    "p_hist = np.array(phi_history[:,1:-1])\n",
    "\n",
    "t_hist_init = t_hist[0,:]\n",
    "t_hist_bc_l = t_hist[:,0]\n",
    "t_hist_bc_r = t_hist[:,-1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have temperature_history and phi_history as lists of arrays\n",
    "\n",
    "\n",
    "# # Check the new shape after transposing\n",
    "# print(\"Transposed Temperature History Shape:\", temperature_history.shape)\n",
    "# print(\"Transposed Phi History Shape:\", phi_history.shape)\n",
    "\n",
    "# # Create a meshgrid for space and time coordinates\n",
    "# space_coord, time_coord = np.meshgrid(np.arange(temperature_history.shape[1]), np.arange(temperature_history.shape[0]))\n",
    "\n",
    "# time_coord = time_coord * dt \n",
    "# # Create a figure with two subplots\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# # Plot the temperature history on the left subplot\n",
    "# im1 = ax1.pcolormesh(space_coord, time_coord, temperature_history, cmap='viridis')\n",
    "# ax1.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_title('Temperature Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im1, ax=ax1, label='Temperature')\n",
    "\n",
    "# # Plot the phase history on the right subplot\n",
    "# im2 = ax2.pcolormesh(space_coord, time_coord, phi_history, cmap='viridis')\n",
    "# ax2.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=18)\n",
    "# ax2.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax2.set_title('Phase Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im2, ax=ax2, label='Phase')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #plot the main\n",
    "# fig, ax = plt.subplots(figsize=(14, 6))\n",
    "# im = ax.pcolormesh(space_coord, time_coord, Dim_ny, cmap='viridis')\n",
    "# ax.set_xlabel('Space Coordinate')\n",
    "# ax.set_ylabel('Time')\n",
    "# ax.set_title('Niyama Variation Over Time')\n",
    "# fig.colorbar(im, ax=ax, label='Main')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU/CPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check for gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,) (31470,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "space = np.linspace(0, length, num_points) # Spatial points\n",
    "time = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "print(space.shape,time.shape)\n",
    "\n",
    "sp_i = np.linspace(0, length, num_points) # Spatial points\n",
    "time_i = np.zeros(num_points) # Time points\n",
    "\n",
    "sp_b_l = np.zeros(num_steps+1) # Spatial points\n",
    "time_b_l = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "\n",
    "sp_b_r = np.ones(num_steps+1)*length # Spatial points\n",
    "time_b_r = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = input_gen(space,time,'mgrid')\n",
    "inputs_i = input_gen(sp_i,time_i,'scr')\n",
    "inputs_b_l = input_gen(sp_b_l,time_b_l,'scr')\n",
    "inputs_b_r = input_gen(sp_b_r,time_b_r,'scr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1573500, 2) (50, 2) (31470, 2) (31470, 2)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape,inputs_i.shape,inputs_b_l.shape,inputs_b_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2])\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(inputs_init\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# label/temp data\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m temp_tr \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_hist\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Convert the temperature history to a tensor\u001b[39;00m\n\u001b[1;32m     11\u001b[0m temp_inp \u001b[38;5;241m=\u001b[39m temp_tr\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Reshape the temperature tensor to a column vector\u001b[39;00m\n\u001b[1;32m     12\u001b[0m temp_inp_init \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(t_hist_init)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Convert the temperature history to a tensor\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs = torch.tensor(inputs).float().to(device) # Convert the inputs to a tensor\n",
    "\n",
    "\n",
    "inputs_init = torch.tensor(inputs_i).float().to(device) # Convert the inputs to a tensor\n",
    "inputs_b_l = torch.tensor(inputs_b_l).float().to(device)# Convert the inputs to a tensor\n",
    "inputs_b_r = torch.tensor(inputs_b_r).float().to(device)# Convert the inputs to a tensor\n",
    "\n",
    "print(inputs_init.shape)\n",
    "# label/temp data\n",
    "temp_tr = torch.tensor(t_hist).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp = temp_tr.reshape(-1,1).float().to(device) # Reshape the temperature tensor to a column vector\n",
    "temp_inp_init = torch.tensor(t_hist_init).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp_bc_l = torch.tensor(t_hist_bc_l).float().to(device)# Convert the temperature history to a tensor\n",
    "temp_inp_bc_r = torch.tensor(t_hist_bc_r).float().to(device)# Convert the temperature history to a tensor\n",
    "print(temp_inp.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Data Splitting\n",
    "\n",
    "# train_inputs, val_test_inputs, train_temp_inp, val_test_temp_inp = train_test_split(inputs, temp_inp, test_size=0.2, random_state=42)\n",
    "# val_inputs, test_inputs, val_temp_inp, test_temp_inp = train_test_split(val_test_inputs, val_test_temp_inp, test_size=0.8, random_state=42)\n",
    "\n",
    "train_inputs, test_inputs, train_temp_inp, test_temp_inp = train_test_split(inputs, temp_inp, test_size=0.2, random_state=42)\n",
    "train_inputs_init, test_inputs_init, train_temp_inp_init, test_temp_inp_init = train_test_split(inputs_init, temp_inp_init, test_size=0.2, random_state=42)\n",
    "train_inputs_bc_l, test_inputs_bc_l, train_temp_inp_bc_l, test_temp_inp_bc_l = train_test_split(inputs_b_l, temp_inp_bc_l, test_size=0.2, random_state=42)\n",
    "train_inputs_bc_r, test_inputs_bc_r, train_temp_inp_bc_r, test_temp_inp_bc_r = train_test_split(inputs_b_r, temp_inp_bc_r, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "print(train_inputs.shape,train_temp_inp.shape)\n",
    "print(test_inputs.shape,test_temp_inp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, temp_inp,transform=None, target_transform =None):\n",
    "        self.inputs = inputs\n",
    "        self.temp_inp = temp_inp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.temp_inp[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "  \n",
    "train_dataset = TensorDataset(train_inputs, train_temp_inp) # Create the training dataset\n",
    "# val_dataset = TensorDataset(val_inputs, val_temp_inp) # Create the validation dataset\n",
    "test_dataset = TensorDataset(test_inputs, test_temp_inp) # Create the test dataset\n",
    "\n",
    "train_dataset_init = TensorDataset(train_inputs_init, train_temp_inp_init) # Create the training dataset\n",
    "test_dataset_init = TensorDataset(test_inputs_init, test_temp_inp_init) # Create the test dataset\n",
    "train_dataset_bc_l = TensorDataset(train_inputs_bc_l, train_temp_inp_bc_l) # Create the training dataset\n",
    "test_dataset_bc_l = TensorDataset(test_inputs_bc_l, test_temp_inp_bc_l) # Create the test dataset\n",
    "train_dataset_bc_r = TensorDataset(train_inputs_bc_r, train_temp_inp_bc_r) # Create the training dataset\n",
    "test_dataset_bc_r = TensorDataset(test_inputs_bc_r, test_temp_inp_bc_r) # Create the test dataset\n",
    "\n",
    "# print(train_dataset[:5])\n",
    "# print(test_dataset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "random_sampler_train = RandomSampler(train_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "# random_sampler_val = RandomSampler(val_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the validation dataset\n",
    "random_sampler_test = RandomSampler(test_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "\n",
    "random_sampler_train_init = RandomSampler(train_dataset_init, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "random_sampler_test_init = RandomSampler(test_dataset_init, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "random_sampler_train_bc_l = RandomSampler(train_dataset_bc_l, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "random_sampler_test_bc_l = RandomSampler(test_dataset_bc_l, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "random_sampler_train_bc_r = RandomSampler(train_dataset_bc_r, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "random_sampler_test_bc_r = RandomSampler(test_dataset_bc_r, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=random_sampler_train) # Create the training dataloader\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=random_sampler_val) # Create the validation dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=random_sampler_test) # Create the test dataloader\n",
    "\n",
    "train_loader_init = DataLoader(train_dataset_init, batch_size=batch_size, sampler=random_sampler_train_init) # Create the training dataloader\n",
    "test_loader_init = DataLoader(test_dataset_init, batch_size=batch_size, sampler=random_sampler_test_init) # Create the test dataloader\n",
    "train_loader_bc_l = DataLoader(train_dataset_bc_l, batch_size=batch_size, sampler=random_sampler_train_bc_l) # Create the training dataloader\n",
    "test_loader_bc_l = DataLoader(test_dataset_bc_l, batch_size=batch_size, sampler=random_sampler_test_bc_l) # Create the test dataloader\n",
    "train_loader_bc_r = DataLoader(train_dataset_bc_r, batch_size=batch_size, sampler=random_sampler_train_bc_r) # Create the training dataloader\n",
    "test_loader_bc_r = DataLoader(test_dataset_bc_r, batch_size=batch_size, sampler=random_sampler_test_bc_r) # Create the test dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural network architecture\n",
    "class Mushydata(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size): # This is the constructor\n",
    "        super(Mushydata, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):                               # This is the forward pass\n",
    "        input_features = torch.cat([x, t], dim=1)          # Concatenate the input features\n",
    "        m = self.base(input_features)                                 # Pass through the third layer\n",
    "        return m                    # Return the output of the network\n",
    "\n",
    "\n",
    "# features = torch.rand(1, 2)\n",
    "# model = HeatPINN(2, 20, 1)\n",
    "# output = model(features[:, 0:1], features[:, 1:2])\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# Loss function for data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 10\n",
    "learning_rate = 0.03\n",
    "epochs = 30000\n",
    "# alpha = 0.01  # Adjust this value based on your problem\n",
    "# boundary_value = 313.0\n",
    "# initial_value = init_temp\n",
    "# Initialize the model\n",
    "model = Mushydata(input_size=2, hidden_size=hidden_size,output_size=1).to(device)\n",
    "lambd = 0.1\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss List Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of train_loader is <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(f\"Datatype of train_loader is {type(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn_data(u_pred, u_true):\n",
    "#     return nn.MSELoss()(u_pred, u_true)\n",
    "\n",
    "# def l1_regularization(model, lambd):\n",
    "#     l1_reg = sum(param.abs().sum() for param in model.parameters())\n",
    "#     return l1_reg * lambd\n",
    "\n",
    "# def pde_loss(u_pred,x,t):\n",
    "#     # u_pred.requires_grad = True\n",
    "#     x.requires_grad = True\n",
    "#     t.requires_grad = True\n",
    "    \n",
    "#     u_pred = model(x,t).requires_grad_()\n",
    "#     u_t = torch.autograd.grad(u_pred, t, \n",
    "#                                 torch.ones_like(u_pred).to(device),\n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused=True,\n",
    "#                                 )[0] # Calculate the first time derivative\n",
    "#     if u_t is None:\n",
    "#         raise RuntimeError(\"u_t is None\")\n",
    "\n",
    "#     u_x = torch.autograd.grad(u_pred, \n",
    "#                                 x, \n",
    "#                                 torch.ones_like(u_pred).to(device), \n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused =True)[0] # Calculate the first space derivative\n",
    "            \n",
    "#     u_xx = torch.autograd.grad(u_x, \n",
    "#                                 x, \n",
    "#                                 torch.ones_like(u_x).to(device), \n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused=True)[0] \n",
    "    \n",
    "#     T_S_tensor = torch.tensor(T_S, device=device)\n",
    "#     T_L_tensor = torch.tensor(T_L, device=device)\n",
    "    \n",
    "#     k_m = torch.where((u_pred >= T_S_tensor) * (u_pred <= T_L_tensor),\\\n",
    "#                        kramp(u_pred, k_l,k_s,T_L,T_S),torch.tensor(0.0,device=device))\n",
    "#     cp_m = torch.where(u_pred >= T_S_tensor * u_pred <= T_L_tensor, cp_ramp((u_pred), cp_l,cp_s,T_L,T_S))\n",
    "#     rho_m = torch.where(u_pred >= T_S_tensor * u_pred <= T_L_tensor, rho_ramp((u_pred), rho_l,rho_s,T_L,T_S))\n",
    "#     m_eff = (k_m / (rho_m * (cp_m + (L_fusion / (T_L - T_S)))))\n",
    "\n",
    "#     alpha_T = torch.where(u_pred >= T_L_tensor, alpha_l, torch.where(u_pred<=T_S_tensor,alpha_s ,m_eff))\n",
    "#     alpha_T = 1\n",
    "#     residual = u_t - alpha_T * u_xx\n",
    "\n",
    "#     return nn.MSELoss()(residual,torch.zeros_like(residual))\n",
    "\n",
    "# def boundary_loss(u_pred,x,t,t_surr):\n",
    "    \n",
    "#     u_x = torch.autograd.grad(u_pred,x, \n",
    "#                                 torch.ones_like(u_pred).to(device), \n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused =True)[0] # Calculate the first space derivative\n",
    "#     t_surr_t = torch.tensor(t_surr, device=device)\n",
    "#     res_l = u_x -(htc* (u_pred-t_surr_t))\n",
    "   \n",
    "\n",
    "#     return nn.MSELoss()(res_l,torch.zeros_like(res_l))\n",
    "\n",
    "# def ic_loss(u_pred):\n",
    "#     temp_init_tsr = torch.tensor(temp_init[1:-1],device=device)\n",
    "#     ic = u_pred -temp_init_tsr\n",
    "#     return nn.MSELoss()(ic,torch.zeros_like(ic))\n",
    "\n",
    "def accuracy(u_pred, u_true):\n",
    "    return torch.mean(torch.abs(u_pred - u_true) / u_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Testing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, model, loss_fn_data, optimizer, train_dataloader,test_dataloader,train_loader_init,\\\n",
    "                  train_loader_bc_l,train_loader_bc_r):\n",
    "    train_losses = []  # Initialize the list to store the training losses\n",
    "    # val_losses = []    # Initialize the list to store the validation losses\n",
    "    test_losses = []   # Initialize the list to store the test losses\n",
    "    data_losses = []   # Initialize the list to store the data losses\n",
    "    pde_losses = []   # Initialize the list to store the PDE losses\n",
    "    ic_losses = []   # Initialize the list to store the initial condition losses\n",
    "    bc_losses = []   # Initialize the list to store the boundary condition losses\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                                                           # Set the model to training mode\n",
    "        train_loss = 0                                                                              # Initialize the training loss\n",
    "        train_accuracy = 0\n",
    "        for (batch,batch_init,batch_left,batch_right) in \\\n",
    "             zip (train_dataloader,train_loader_init,train_loader_bc_l,train_loader_bc_r):                                                          # Loop through the training dataloader\n",
    "            inputs, temp_inp= batch                                                             # Get the inputs and the true values\n",
    "            inputs_init, temp_inp_init= batch_init                                                             # Get the inputs and the true values \n",
    "            inputs_left, temp_inp_left= batch_left                                                             # Get the inputs and the true values\n",
    "            inputs_right, temp_inp_right= batch_right                                                             # Get the inputs and the true values\n",
    "\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device)                             # Move the inputs and true values to the GPU\n",
    "            inputs_init, temp_inp_init= inputs_init.to(device), temp_inp_init.to(device)                             # Move the initial condition inputs and temperature to the GPU\n",
    "            inputs_left, temp_inp_left= inputs_left.to(device), temp_inp_left.to(device)                             # Move the left boundary condition inputs and temperature values to the GPU\n",
    "            inputs_right, temp_inp_right= inputs_right.to(device), temp_inp_right.to(device)                             # Move the right boundary condition inputs and temperature values to the GPU\n",
    "\n",
    "            optimizer.zero_grad()                                                                    # Zero the gradients\n",
    "            \n",
    "            # Forward pass\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).to(device)                       # Get the predictions of data points\n",
    "            u_initl = model(inputs_init[:,0].unsqueeze(1), inputs_init[:,1].unsqueeze(1)).to(device)                       # Get the predictions of initial\n",
    "            \n",
    "            u_left = model(inputs_b_l[:,0].unsqueeze(1), inputs_b_l[:,1].unsqueeze(1)).to(device)               # Left boundary of the temperature\n",
    "            u_right = model(inputs_b_r[:,0].unsqueeze(1), inputs_b_r[:,1].unsqueeze(1)).to(device)             # Right boundary of the temperature\n",
    "\n",
    "            # Loss calculation\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp)                                              # Calculate the data loss\n",
    "            \n",
    "            pd_loss = pde_loss(model,inputs[:,0].unsqueeze(1),inputs[:,1].unsqueeze(1))             # Calculate the PDE loss\n",
    "            # pd_loss = 0\n",
    "            \n",
    "            initc_loss = ic_loss(u_initl) \n",
    "            # initc_loss =0                                                      # Calculate initial condition loss\n",
    "            \n",
    "            bc_loss_left = boundary_loss(model,inputs_b_l[:,0].unsqueeze(1),inputs_b_l[:,1].unsqueeze(1),t_surr) # Calculate the left boundary condition loss\n",
    "            bc_loss_right = boundary_loss(model,inputs_b_r[:,0].unsqueeze(1),inputs_b_r[:,1].unsqueeze(1),t_surr) # Calculate the right boundary condition loss\n",
    "            bc_loss = bc_loss_left + bc_loss_right\n",
    "            # l1_regularization_loss = l1_regularization(model, lambda_l1)                      # Calculate the L1 regularization loss\n",
    "            # loss = data_loss  + pd_loss + initc_loss + bc_loss                                              # Calculate the total loss\n",
    "            w1 = 0.0001\n",
    "            w2 = 0.0001\n",
    "            w3 = 0.0001\n",
    "            loss = data_loss + w1* pd_loss + w2 *initc_loss + w3* bc_loss\n",
    "            train_accuracy += accuracy(u_pred, temp_inp)                                                              # Calculate the total loss\n",
    "            # Backpropagation\n",
    "            loss.backward(retain_graph=True)                                                        # Backpropagate the gradients\n",
    "            \n",
    "            optimizer.step()                                                                           # Update the weights\n",
    "            \n",
    "            train_loss += loss.item()                                                           # Add the loss to the training set loss  \n",
    "            data_loss += data_loss.item()\n",
    "            pd_loss += pd_loss.item()\n",
    "            initc_loss += initc_loss.item()\n",
    "            bc_loss += bc_loss.item()\n",
    "            \n",
    "        \n",
    "\n",
    "        # model.eval()\n",
    "        # test_loss = 0\n",
    "        # test_accuracy = 0\n",
    "        # with torch.no_grad():   \n",
    "        #     for batch in test_dataloader:\n",
    "        #         inputs, temp_inp= batch\n",
    "        #         inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "        #         u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "        #         data_loss = loss_fn_data(u_pred, temp_inp)\n",
    "        #         # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "        #         # loss = data_loss  + l1_regularization_loss\n",
    "        #         loss = data_loss\n",
    "        #         test_accuracy = accuracy(u_pred, temp_inp)\n",
    "        #         test_loss += loss.item()\n",
    "        #     test_losses.append(test_loss)\n",
    "\n",
    "        train_losses.append(train_loss)                                                   # Append the training loss to the list of training losses\n",
    "        data_losses.append(data_loss)\n",
    "        pde_losses.append(pd_loss)\n",
    "        ic_losses.append(initc_loss)\n",
    "        bc_losses.append(bc_loss)\n",
    "        \n",
    "        # if epoch % 10 == 0:\n",
    "        #     print(f\"Epoch {epoch}, Training-Loss {train_loss:.4e}\")\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0\n",
    "        with torch.no_grad():   \n",
    "            for batch in test_dataloader:\n",
    "                inputs, temp_inp= batch\n",
    "                inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "                u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "                data_loss = loss_fn_data(u_pred, temp_inp)\n",
    "                # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "                # loss = data_loss  + l1_regularization_loss\n",
    "                loss = data_loss\n",
    "                test_accuracy = accuracy(u_pred, temp_inp)\n",
    "                test_loss += loss.item()\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training-loss {train_loss:.4e},Test-loss {test_loss:.4e}\") \n",
    "            print(f\" \")\n",
    "            print(f\"Data Loss {data_loss:.4e}, PDE Loss {pd_loss:.4e}, IC Loss {initc_loss:.4e}, BC Loss {bc_loss:.4e}\")\n",
    "\n",
    "    return train_losses, test_losses , pde_losses , bc_losses , ic_losses, data_losses                                                      # Return the training and validation losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(epochs, model, loss_fn_data, test_dataloader):\n",
    "    test_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0\n",
    "        with torch.no_grad():   \n",
    "            for batch in test_dataloader:\n",
    "                inputs, temp_inp= batch\n",
    "                inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "                u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "                data_loss = loss_fn_data(u_pred, temp_inp)\n",
    "                # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "                # loss = data_loss  + l1_regularization_loss\n",
    "                loss = data_loss\n",
    "                test_accuracy = accuracy(u_pred, temp_inp)\n",
    "                test_loss += loss.item()\n",
    "            test_losses.append(test_loss)\n",
    "        # if epochs % 10 == 0:\n",
    "        #     print(f\"Epoch {epoch}, Test-Loss {test_loss:.4e}\")      \n",
    "    return test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Button "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training-loss 6.6088e+05,Test-loss 6.5476e+05\n",
      " \n",
      "Data Loss 6.5476e+05, PDE Loss 1.5383e-06, IC Loss 1.6883e+06, BC Loss 9.9912e+07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Training-loss 6.5281e+05,Test-loss 6.4746e+05\n",
      " \n",
      "Data Loss 6.4746e+05, PDE Loss 1.1844e-05, IC Loss 1.6787e+06, BC Loss 9.8867e+07\n",
      "Epoch 20, Training-loss 6.5301e+05,Test-loss 6.4605e+05\n",
      " \n",
      "Data Loss 6.4605e+05, PDE Loss 1.0959e-07, IC Loss 1.6684e+06, BC Loss 9.7752e+07\n",
      "Epoch 30, Training-loss 6.4056e+05,Test-loss 6.4138e+05\n",
      " \n",
      "Data Loss 6.4138e+05, PDE Loss 1.3610e-08, IC Loss 1.6569e+06, BC Loss 9.6505e+07\n",
      "Epoch 40, Training-loss 6.4000e+05,Test-loss 6.3333e+05\n",
      " \n",
      "Data Loss 6.3333e+05, PDE Loss 1.1334e-08, IC Loss 1.6438e+06, BC Loss 9.5098e+07\n",
      "Epoch 50, Training-loss 6.3507e+05,Test-loss 6.2724e+05\n",
      " \n",
      "Data Loss 6.2724e+05, PDE Loss 3.8336e-09, IC Loss 1.6309e+06, BC Loss 9.3708e+07\n",
      "Epoch 60, Training-loss 6.2694e+05,Test-loss 6.2143e+05\n",
      " \n",
      "Data Loss 6.2143e+05, PDE Loss 6.4775e-09, IC Loss 1.6163e+06, BC Loss 9.2147e+07\n",
      "Epoch 70, Training-loss 6.2084e+05,Test-loss 6.1193e+05\n",
      " \n",
      "Data Loss 6.1193e+05, PDE Loss 6.1916e-10, IC Loss 1.6024e+06, BC Loss 9.0667e+07\n",
      "Epoch 80, Training-loss 6.1481e+05,Test-loss 6.0886e+05\n",
      " \n",
      "Data Loss 6.0886e+05, PDE Loss 4.3714e-10, IC Loss 1.5889e+06, BC Loss 8.9233e+07\n",
      "Epoch 90, Training-loss 6.1315e+05,Test-loss 6.0700e+05\n",
      " \n",
      "Data Loss 6.0700e+05, PDE Loss 3.6152e-10, IC Loss 1.5757e+06, BC Loss 8.7843e+07\n",
      "Epoch 100, Training-loss 6.0659e+05,Test-loss 6.0176e+05\n",
      " \n",
      "Data Loss 6.0176e+05, PDE Loss 3.2977e-10, IC Loss 1.5629e+06, BC Loss 8.6488e+07\n",
      "Epoch 110, Training-loss 5.9332e+05,Test-loss 5.9487e+05\n",
      " \n",
      "Data Loss 5.9487e+05, PDE Loss 3.1872e-10, IC Loss 1.5503e+06, BC Loss 8.5166e+07\n",
      "Epoch 120, Training-loss 5.9140e+05,Test-loss 5.9431e+05\n",
      " \n",
      "Data Loss 5.9431e+05, PDE Loss 2.5840e-10, IC Loss 1.5379e+06, BC Loss 8.3872e+07\n",
      "Epoch 130, Training-loss 5.8800e+05,Test-loss 5.8531e+05\n",
      " \n",
      "Data Loss 5.8531e+05, PDE Loss 1.9881e-10, IC Loss 1.5258e+06, BC Loss 8.2602e+07\n",
      "Epoch 140, Training-loss 5.8312e+05,Test-loss 5.7687e+05\n",
      " \n",
      "Data Loss 5.7687e+05, PDE Loss 1.4371e-10, IC Loss 1.5137e+06, BC Loss 8.1354e+07\n",
      "Epoch 150, Training-loss 5.7905e+05,Test-loss 5.7161e+05\n",
      " \n",
      "Data Loss 5.7161e+05, PDE Loss 1.1567e-10, IC Loss 1.5019e+06, BC Loss 8.0125e+07\n",
      "Epoch 160, Training-loss 5.6705e+05,Test-loss 5.6887e+05\n",
      " \n",
      "Data Loss 5.6887e+05, PDE Loss 1.0057e-10, IC Loss 1.4901e+06, BC Loss 7.8914e+07\n",
      "Epoch 170, Training-loss 5.6586e+05,Test-loss 5.6187e+05\n",
      " \n",
      "Data Loss 5.6187e+05, PDE Loss 8.5071e-11, IC Loss 1.4785e+06, BC Loss 7.7720e+07\n",
      "Epoch 180, Training-loss 5.5961e+05,Test-loss 5.5921e+05\n",
      " \n",
      "Data Loss 5.5921e+05, PDE Loss 8.2242e-11, IC Loss 1.4670e+06, BC Loss 7.6541e+07\n",
      "Epoch 190, Training-loss 5.5688e+05,Test-loss 5.4961e+05\n",
      " \n",
      "Data Loss 5.4961e+05, PDE Loss 6.0026e-11, IC Loss 1.4556e+06, BC Loss 7.5378e+07\n",
      "Epoch 200, Training-loss 5.5109e+05,Test-loss 5.4479e+05\n",
      " \n",
      "Data Loss 5.4479e+05, PDE Loss 5.3742e-11, IC Loss 1.4443e+06, BC Loss 7.4230e+07\n",
      "Epoch 210, Training-loss 5.4755e+05,Test-loss 5.4200e+05\n",
      " \n",
      "Data Loss 5.4200e+05, PDE Loss 4.6927e-11, IC Loss 1.4331e+06, BC Loss 7.3096e+07\n",
      "Epoch 220, Training-loss 5.4095e+05,Test-loss 5.4258e+05\n",
      " \n",
      "Data Loss 5.4258e+05, PDE Loss 4.0365e-11, IC Loss 1.4219e+06, BC Loss 7.1976e+07\n",
      "Epoch 230, Training-loss 5.3845e+05,Test-loss 5.3485e+05\n",
      " \n",
      "Data Loss 5.3485e+05, PDE Loss 3.2084e-11, IC Loss 1.4109e+06, BC Loss 7.0868e+07\n",
      "Epoch 240, Training-loss 5.3512e+05,Test-loss 5.3100e+05\n",
      " \n",
      "Data Loss 5.3100e+05, PDE Loss 2.9096e-11, IC Loss 1.4000e+06, BC Loss 6.9774e+07\n",
      "Epoch 250, Training-loss 5.2607e+05,Test-loss 5.2729e+05\n",
      " \n",
      "Data Loss 5.2729e+05, PDE Loss 2.8487e-11, IC Loss 1.3891e+06, BC Loss 6.8691e+07\n",
      "Epoch 260, Training-loss 5.2064e+05,Test-loss 5.2007e+05\n",
      " \n",
      "Data Loss 5.2007e+05, PDE Loss 2.6235e-11, IC Loss 1.3783e+06, BC Loss 6.7622e+07\n",
      "Epoch 270, Training-loss 5.1619e+05,Test-loss 5.1211e+05\n",
      " \n",
      "Data Loss 5.1211e+05, PDE Loss 2.1954e-11, IC Loss 1.3676e+06, BC Loss 6.6565e+07\n",
      "Epoch 280, Training-loss 5.1069e+05,Test-loss 5.1006e+05\n",
      " \n",
      "Data Loss 5.1006e+05, PDE Loss 1.9458e-11, IC Loss 1.3570e+06, BC Loss 6.5520e+07\n",
      "Epoch 290, Training-loss 5.1041e+05,Test-loss 5.0753e+05\n",
      " \n",
      "Data Loss 5.0753e+05, PDE Loss 1.6800e-11, IC Loss 1.3465e+06, BC Loss 6.4486e+07\n",
      "Epoch 300, Training-loss 5.0193e+05,Test-loss 4.9778e+05\n",
      " \n",
      "Data Loss 4.9778e+05, PDE Loss 1.5279e-11, IC Loss 1.3360e+06, BC Loss 6.3463e+07\n",
      "Epoch 310, Training-loss 5.0013e+05,Test-loss 4.9615e+05\n",
      " \n",
      "Data Loss 4.9615e+05, PDE Loss 1.3939e-11, IC Loss 1.3256e+06, BC Loss 6.2452e+07\n",
      "Epoch 320, Training-loss 4.9519e+05,Test-loss 4.9336e+05\n",
      " \n",
      "Data Loss 4.9336e+05, PDE Loss 1.2759e-11, IC Loss 1.3153e+06, BC Loss 6.1452e+07\n",
      "Epoch 330, Training-loss 4.9296e+05,Test-loss 4.8990e+05\n",
      " \n",
      "Data Loss 4.8990e+05, PDE Loss 1.1315e-11, IC Loss 1.3051e+06, BC Loss 6.0464e+07\n",
      "Epoch 340, Training-loss 4.8949e+05,Test-loss 4.8246e+05\n",
      " \n",
      "Data Loss 4.8246e+05, PDE Loss 1.0298e-11, IC Loss 1.2949e+06, BC Loss 5.9487e+07\n",
      "Epoch 350, Training-loss 4.8087e+05,Test-loss 4.8033e+05\n",
      " \n",
      "Data Loss 4.8033e+05, PDE Loss 9.0290e-12, IC Loss 1.2848e+06, BC Loss 5.8520e+07\n",
      "Epoch 360, Training-loss 4.7542e+05,Test-loss 4.7447e+05\n",
      " \n",
      "Data Loss 4.7447e+05, PDE Loss 8.4851e-12, IC Loss 1.2747e+06, BC Loss 5.7564e+07\n",
      "Epoch 370, Training-loss 4.7534e+05,Test-loss 4.7492e+05\n",
      " \n",
      "Data Loss 4.7492e+05, PDE Loss 7.2665e-12, IC Loss 1.2648e+06, BC Loss 5.6619e+07\n",
      "Epoch 380, Training-loss 4.6632e+05,Test-loss 4.6543e+05\n",
      " \n",
      "Data Loss 4.6543e+05, PDE Loss 7.1649e-12, IC Loss 1.2549e+06, BC Loss 5.5684e+07\n",
      "Epoch 390, Training-loss 4.6307e+05,Test-loss 4.6108e+05\n",
      " \n",
      "Data Loss 4.6108e+05, PDE Loss 6.7454e-12, IC Loss 1.2450e+06, BC Loss 5.4759e+07\n",
      "Epoch 400, Training-loss 4.5671e+05,Test-loss 4.5848e+05\n",
      " \n",
      "Data Loss 4.5848e+05, PDE Loss 6.1884e-12, IC Loss 1.2353e+06, BC Loss 5.3844e+07\n",
      "Epoch 410, Training-loss 4.5249e+05,Test-loss 4.5522e+05\n",
      " \n",
      "Data Loss 4.5522e+05, PDE Loss 5.2370e-12, IC Loss 1.2256e+06, BC Loss 5.2941e+07\n",
      "Epoch 420, Training-loss 4.5166e+05,Test-loss 4.4890e+05\n",
      " \n",
      "Data Loss 4.4890e+05, PDE Loss 4.8014e-12, IC Loss 1.2159e+06, BC Loss 5.2047e+07\n",
      "Epoch 430, Training-loss 4.4903e+05,Test-loss 4.4731e+05\n",
      " \n",
      "Data Loss 4.4731e+05, PDE Loss 4.0921e-12, IC Loss 1.2064e+06, BC Loss 5.1164e+07\n",
      "Epoch 440, Training-loss 4.4358e+05,Test-loss 4.4128e+05\n",
      " \n",
      "Data Loss 4.4128e+05, PDE Loss 3.8803e-12, IC Loss 1.1968e+06, BC Loss 5.0290e+07\n",
      "Epoch 450, Training-loss 4.4289e+05,Test-loss 4.3873e+05\n",
      " \n",
      "Data Loss 4.3873e+05, PDE Loss 3.3577e-12, IC Loss 1.1874e+06, BC Loss 4.9425e+07\n",
      "Epoch 460, Training-loss 4.4024e+05,Test-loss 4.3212e+05\n",
      " \n",
      "Data Loss 4.3212e+05, PDE Loss 3.0681e-12, IC Loss 1.1780e+06, BC Loss 4.8570e+07\n",
      "Epoch 470, Training-loss 4.2842e+05,Test-loss 4.2793e+05\n",
      " \n",
      "Data Loss 4.2793e+05, PDE Loss 3.1241e-12, IC Loss 1.1687e+06, BC Loss 4.7725e+07\n",
      "Epoch 480, Training-loss 4.2783e+05,Test-loss 4.2880e+05\n",
      " \n",
      "Data Loss 4.2880e+05, PDE Loss 2.9392e-12, IC Loss 1.1594e+06, BC Loss 4.6889e+07\n",
      "Epoch 490, Training-loss 4.2605e+05,Test-loss 4.2100e+05\n",
      " \n",
      "Data Loss 4.2100e+05, PDE Loss 2.4717e-12, IC Loss 1.1502e+06, BC Loss 4.6063e+07\n",
      "Epoch 500, Training-loss 4.1958e+05,Test-loss 4.1573e+05\n",
      " \n",
      "Data Loss 4.1573e+05, PDE Loss 2.5791e-12, IC Loss 1.1410e+06, BC Loss 4.5246e+07\n",
      "Epoch 510, Training-loss 4.1565e+05,Test-loss 4.1216e+05\n",
      " \n",
      "Data Loss 4.1216e+05, PDE Loss 2.3624e-12, IC Loss 1.1319e+06, BC Loss 4.4438e+07\n",
      "Epoch 520, Training-loss 4.1015e+05,Test-loss 4.0892e+05\n",
      " \n",
      "Data Loss 4.0892e+05, PDE Loss 2.2907e-12, IC Loss 1.1229e+06, BC Loss 4.3640e+07\n",
      "Epoch 530, Training-loss 4.0869e+05,Test-loss 4.0966e+05\n",
      " \n",
      "Data Loss 4.0966e+05, PDE Loss 2.0613e-12, IC Loss 1.1139e+06, BC Loss 4.2851e+07\n",
      "Epoch 540, Training-loss 4.0557e+05,Test-loss 4.0147e+05\n",
      " \n",
      "Data Loss 4.0147e+05, PDE Loss 2.0119e-12, IC Loss 1.1050e+06, BC Loss 4.2071e+07\n",
      "Epoch 550, Training-loss 4.0353e+05,Test-loss 3.9763e+05\n",
      " \n",
      "Data Loss 3.9763e+05, PDE Loss 1.7431e-12, IC Loss 1.0962e+06, BC Loss 4.1301e+07\n",
      "Epoch 560, Training-loss 3.9496e+05,Test-loss 3.9628e+05\n",
      " \n",
      "Data Loss 3.9628e+05, PDE Loss 1.7697e-12, IC Loss 1.0874e+06, BC Loss 4.0540e+07\n",
      "Epoch 570, Training-loss 3.9058e+05,Test-loss 3.9011e+05\n",
      " \n",
      "Data Loss 3.9011e+05, PDE Loss 1.6281e-12, IC Loss 1.0786e+06, BC Loss 3.9788e+07\n",
      "Epoch 580, Training-loss 3.9189e+05,Test-loss 3.8659e+05\n",
      " \n",
      "Data Loss 3.8659e+05, PDE Loss 1.4570e-12, IC Loss 1.0700e+06, BC Loss 3.9044e+07\n",
      "Epoch 590, Training-loss 3.8419e+05,Test-loss 3.8206e+05\n",
      " \n",
      "Data Loss 3.8206e+05, PDE Loss 1.4438e-12, IC Loss 1.0613e+06, BC Loss 3.8309e+07\n",
      "Epoch 600, Training-loss 3.8342e+05,Test-loss 3.8122e+05\n",
      " \n",
      "Data Loss 3.8122e+05, PDE Loss 1.2459e-12, IC Loss 1.0528e+06, BC Loss 3.7583e+07\n",
      "Epoch 610, Training-loss 3.7744e+05,Test-loss 3.7531e+05\n",
      " \n",
      "Data Loss 3.7531e+05, PDE Loss 1.2595e-12, IC Loss 1.0443e+06, BC Loss 3.6866e+07\n",
      "Epoch 620, Training-loss 3.7484e+05,Test-loss 3.6963e+05\n",
      " \n",
      "Data Loss 3.6963e+05, PDE Loss 1.1938e-12, IC Loss 1.0358e+06, BC Loss 3.6157e+07\n",
      "Epoch 630, Training-loss 3.7116e+05,Test-loss 3.7081e+05\n",
      " \n",
      "Data Loss 3.7081e+05, PDE Loss 1.1957e-12, IC Loss 1.0274e+06, BC Loss 3.5457e+07\n",
      "Epoch 640, Training-loss 3.6860e+05,Test-loss 3.6583e+05\n",
      " \n",
      "Data Loss 3.6583e+05, PDE Loss 1.0539e-12, IC Loss 1.0191e+06, BC Loss 3.4766e+07\n",
      "Epoch 650, Training-loss 3.6668e+05,Test-loss 3.6545e+05\n",
      " \n",
      "Data Loss 3.6545e+05, PDE Loss 9.9314e-13, IC Loss 1.0108e+06, BC Loss 3.4083e+07\n",
      "Epoch 660, Training-loss 3.5992e+05,Test-loss 3.6089e+05\n",
      " \n",
      "Data Loss 3.6089e+05, PDE Loss 1.0738e-12, IC Loss 1.0025e+06, BC Loss 3.3408e+07\n",
      "Epoch 670, Training-loss 3.5740e+05,Test-loss 3.5615e+05\n",
      " \n",
      "Data Loss 3.5615e+05, PDE Loss 9.6412e-13, IC Loss 9.9433e+05, BC Loss 3.2741e+07\n",
      "Epoch 680, Training-loss 3.5420e+05,Test-loss 3.4917e+05\n",
      " \n",
      "Data Loss 3.4917e+05, PDE Loss 8.7205e-13, IC Loss 9.8620e+05, BC Loss 3.2083e+07\n",
      "Epoch 690, Training-loss 3.5190e+05,Test-loss 3.4792e+05\n",
      " \n",
      "Data Loss 3.4792e+05, PDE Loss 8.5296e-13, IC Loss 9.7812e+05, BC Loss 3.1434e+07\n",
      "Epoch 700, Training-loss 3.4547e+05,Test-loss 3.4149e+05\n",
      " \n",
      "Data Loss 3.4149e+05, PDE Loss 8.4434e-13, IC Loss 9.7009e+05, BC Loss 3.0791e+07\n",
      "Epoch 710, Training-loss 3.4162e+05,Test-loss 3.4136e+05\n",
      " \n",
      "Data Loss 3.4136e+05, PDE Loss 8.6688e-13, IC Loss 9.6211e+05, BC Loss 3.0158e+07\n",
      "Epoch 720, Training-loss 3.3969e+05,Test-loss 3.3745e+05\n",
      " \n",
      "Data Loss 3.3745e+05, PDE Loss 7.9179e-13, IC Loss 9.5419e+05, BC Loss 2.9533e+07\n",
      "Epoch 730, Training-loss 3.3575e+05,Test-loss 3.3261e+05\n",
      " \n",
      "Data Loss 3.3261e+05, PDE Loss 7.4596e-13, IC Loss 9.4631e+05, BC Loss 2.8915e+07\n",
      "Epoch 740, Training-loss 3.3385e+05,Test-loss 3.3124e+05\n",
      " \n",
      "Data Loss 3.3124e+05, PDE Loss 7.4685e-13, IC Loss 9.3850e+05, BC Loss 2.8306e+07\n",
      "Epoch 750, Training-loss 3.3008e+05,Test-loss 3.2663e+05\n",
      " \n",
      "Data Loss 3.2663e+05, PDE Loss 6.8829e-13, IC Loss 9.3073e+05, BC Loss 2.7704e+07\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, test_losses, pde_losses, bc_losses,ic_losses, data_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtrain_loader_bc_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader_bc_r\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# test_losses = test_loop(epochs, model, loss_fn_data, test_loader)  # Test the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[82], line 56\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(epochs, model, loss_fn_data, optimizer, train_dataloader, test_dataloader, train_loader_init, train_loader_bc_l, train_loader_bc_r)\u001b[0m\n\u001b[1;32m     54\u001b[0m train_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy(u_pred, temp_inp)                                                              \u001b[38;5;66;03m# Calculate the total loss\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m                                                        \u001b[38;5;66;03m# Backpropagate the gradients\u001b[39;00m\n\u001b[1;32m     58\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()                                                                           \u001b[38;5;66;03m# Update the weights\u001b[39;00m\n\u001b[1;32m     60\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()                                                           \u001b[38;5;66;03m# Add the loss to the training set loss  \u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses, test_losses, pde_losses, bc_losses,ic_losses, data_losses = training_loop(epochs, model, loss_fn_data, optimizer,train_loader,test_loader,train_loader_init,\\\n",
    "                  train_loader_bc_l,train_loader_bc_r)  # Train the model\n",
    " \n",
    "# test_losses = test_loop(epochs, model, loss_fn_data, test_loader)  # Test the model\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_nn = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).cpu().detach().numpy() # Get the predictions from the model\n",
    "\n",
    "temp_nn = temp_nn.reshape(num_steps+1, num_points) # Reshape the predictions to a 2D array\n",
    "time_ss= np.linspace(0, time_end, num_steps+1 )\n",
    "plt.figure\n",
    "plt.plot(time_ss, temp_nn[:, num_points//2], label='Predicted Temperature')\n",
    "plt.plot(time_ss, temperature_history[:,num_points//2], label='Actual Temperature')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.title('Predicted vs Actual Temperature at x = 7.5mm')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='test Loss')\n",
    "# plt.xticks(np.arange(0, epochs, 10000))\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data_losses, label='Data Loss')\n",
    "plt.plot(pde_losses, label='Pde Loss')\n",
    "plt.plot(ic_losses, label='IC Loss')\n",
    "plt.plot(bc_losses, label='BC Loss')\n",
    "plt.yscale('log')\n",
    "# plt.axhline(y=1e-6, color='red', linestyle='--', label='Near-Zero Line')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_coord, time_coord = np.meshgrid(np.arange(t_hist.shape[1]), np.arange(t_hist.shape[0]))\n",
    "\n",
    "time_coord = time_coord * dt \n",
    "# Create a figure with two subplots\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "\n",
    "# Plot the temperature history on the left subplot\n",
    "im1 = ax1.pcolormesh(space_coord, time_coord, t_hist, cmap='viridis', shading='auto')\n",
    "ax1.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "ax1.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "ax1.set_title('Temperature Variation Over Time(Analytical Model)',fontname='Times New Roman', fontsize=20)\n",
    "ax1.contour(space_coord, time_coord, t_hist, colors='red', linewidths=1.0, alpha=0.9)\n",
    "\n",
    "ax1.grid(True)\n",
    "cbar = fig.colorbar(im1, ax=ax1)\n",
    "cbar.ax.invert_yaxis()\n",
    "cbar.set_label('Temperature (K)', rotation=270, labelpad=20, fontname='Times New Roman', fontsize=16)\n",
    "\n",
    "im2 = ax2.pcolormesh(space_coord, time_coord, temp_nn, cmap='viridis', shading='auto')\n",
    "ax2.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "ax2.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "ax2.set_title('Temperature Variation Over Time(PINN Approach)',fontname='Times New Roman', fontsize=20)\n",
    "ax2.contour(space_coord, time_coord, t_hist, colors='red', linewidths=1.0, alpha=0.9)\n",
    "\n",
    "ax2.grid(True)\n",
    "cbar = fig.colorbar(im2, ax=ax2)\n",
    "cbar.ax.invert_yaxis()\n",
    "cbar.set_label('Temperature (K)', rotation=270, labelpad=20, fontname='Times New Roman', fontsize=16)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
