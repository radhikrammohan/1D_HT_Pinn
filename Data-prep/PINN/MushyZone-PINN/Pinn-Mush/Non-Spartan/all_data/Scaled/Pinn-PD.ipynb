{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Three Phase Simulation of Alloys and PINN model development \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the simulation of 1D Phase change of aluminium alloy. There will be three phases (solid,liquid and mushy).   \n",
    "\n",
    "The approach used is finite difference method and the physics involved in heat conduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import csv\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "from pinn_loss import loss_fn_data, l1_regularization, pde_loss, boundary_loss, ic_loss, accuracy\n",
    "from Input_vec_gen import input_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the constants and inital geometric domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_l = 3.394878564540885e-05, alpha_s = 3.686205086349929e-05, m_eff = 6.296953764744878e-06\n",
      "dx is 0.0003061224489795918\n",
      "dt is  0.0012711033647622566\n",
      "num_steps is 31469\n",
      "cfl is 0.0012711033647622566\n",
      "stability criteria satisfied\n"
     ]
    }
   ],
   "source": [
    "# Geometry\n",
    "length = 15.0e-3             # Length of the rod\n",
    "\n",
    "# Material properties\n",
    "rho = 2300.0                     # Density of AL380 (kg/m^3)\n",
    "rho_l = 2460.0                   # Density of AL380 (kg/m^3)\n",
    "rho_s = 2710.0                    # Density of AL380 (kg/m^3)\n",
    "rho_m = (rho_l + rho_s )/2       # Desnity in mushy zone is taken as average of liquid and solid density\n",
    "\n",
    "k = 104.0                       # W/m-K\n",
    "k_l = k                       # W/m-K\n",
    "k_s = 96.2                    # W/m-K\n",
    "k_m =  (k_l+k_s)/2                     # W/m-K\n",
    "k_mo = 41.5\n",
    "\n",
    "\n",
    "cp = 1245.3                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_l = cp                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_s = 963.0                 # Specific heat of aluminum (J/kg-K)\n",
    "cp_m =  (cp_l+cp_s)/2                 # Specific heat of mushy zone is taken as average of liquid and solid specific heat\n",
    "# cp_m = cp\n",
    "           # Thermal diffusivity\n",
    "alpha_l = k_l / (rho_l * cp_l) \n",
    "alpha_s = k_s / (rho_s*cp_s)\n",
    "alpha_m = k_m / (rho_m * cp_m)          #`Thermal diffusivity in mushy zone is taken as average of liquid and solid thermal diffusivity`\n",
    "\n",
    "\n",
    "#L_fusion = 3.9e3                 # J/kg\n",
    "L_fusion = 389.0e3               # J/kg  # Latent heat of fusion of aluminum\n",
    "         # Thermal diffusivity\n",
    "\n",
    "\n",
    "T_L = 574.4 +273.0                       #  K -Liquidus Temperature (615 c) AL 380\n",
    "T_S = 497.3 +273.0                     # K- Solidus Temperature (550 C)\n",
    "m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "print (f\"alpha_l = {alpha_l}, alpha_s = {alpha_s}, m_eff = {m_eff}\")\n",
    "\n",
    "# htc = 10.0                   # W/m^2-K\n",
    "# q = htc*(919.0-723.0)\n",
    "# q = 10000.0\n",
    "\n",
    "\n",
    "num_points = 50                        # Number of spatial points\n",
    "dx = length / (num_points - 1)         # Distance between two spatial points\n",
    "print('dx is',dx)\n",
    "\n",
    "                                                              \n",
    "# Time Discretization  \n",
    "# \n",
    "time_end = 40        # seconds                         \n",
    "\n",
    "maxi = max(alpha_s,alpha_l,alpha_m)\n",
    "dt = abs(0.5*((dx**2) /maxi)) \n",
    "\n",
    "print('dt is ',dt)\n",
    "num_steps = round(time_end/dt)\n",
    "print('num_steps is',num_steps)\n",
    "cfl = 0.5 *(dx**2/max(alpha_l,alpha_s,alpha_m))\n",
    "print('cfl is',cfl)\n",
    "\n",
    "time_steps = np.linspace(0, time_end, num_steps + 1)\n",
    "step_coeff = dt / (dx ** 2)\n",
    "\n",
    "if dt <= cfl:\n",
    "    print('stability criteria satisfied')\n",
    "else:\n",
    "    print('stability criteria not satisfied')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial and Boundary Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_init = 919.0\n",
    "# Initial temperature and phase fields\n",
    "temperature = np.full(num_points+2, 919.0)            # Initial temperature of the rod with ghost points at both ends\n",
    "phase = np.zeros(num_points+2)*0.0                    # Initial phase of the rod with ghost points at both ends\n",
    "\n",
    "# Set boundary conditions\n",
    "# temperature[-1] = 919.0 \n",
    "phase[-1] = 1.0\n",
    "\n",
    "# temperature[0] = 919.0 #(40 C)\n",
    "phase[0] = 1.0\n",
    "\n",
    "# Store initial state in history\n",
    "temperature_history = [temperature.copy()]    # List to store temperature at each time step\n",
    "phi_history = [phase.copy()]                    # List to store phase at each time step\n",
    "temp_init = temperature.copy()                 # Initial temperature of the rod\n",
    "# print(temperature_history,phi_history)\n",
    "# Array to store temperature at midpoint over time\n",
    "midpoint_index = num_points // 2                          # Index of the midpoint\n",
    "\n",
    "midpoint_temperature_history = [temperature[midpoint_index]]            # List to store temperature at midpoint over time\n",
    "dm = 60.0e-3                                                            # die thickness in m\n",
    "\n",
    "# r_m =  (k_mo / dm) + (1/htc)\n",
    "\n",
    "t_surr = 500.0                                        # Surrounding temperature in K\n",
    "# t_surr = h()\n",
    "\n",
    "def kramp(temp,v1,v2,T_L,T_s):                                      # Function to calculate thermal conductivity in Mushy Zone\n",
    "        slope = (v1-v2)/(T_L-T_S)\n",
    "        if temp > T_L:\n",
    "            k_m = k_l\n",
    "        elif temp < T_S:\n",
    "            k_m = k_s\n",
    "        else:\n",
    "            k_m = k_s + slope*(temp-T_S)\n",
    "        return k_m\n",
    "\n",
    "def cp_ramp(temp,v1,v2,T_L,T_s):                                    # Function to calculate specific heat capacity in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        cp_m = cp_l\n",
    "    elif temp < T_S:\n",
    "        cp_m = cp_s\n",
    "    else:\n",
    "        cp_m = cp_s + slope*(temp-T_S)\n",
    "    return cp_m\n",
    "\n",
    "def rho_ramp(temp,v1,v2,T_L,T_s):                                       # Function to calculate density in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        rho_m = rho_l\n",
    "    elif temp < T_S:\n",
    "        rho_m = rho_s\n",
    "    else:\n",
    "        rho_m = rho_s + slope*(temp-T_S)\n",
    "    return rho_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the HT equation and phase change numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for m in range(1, num_steps+1):                                                                            # time loop\n",
    "    htc = 10.0                   # htc of Still air in W/m^2-K\n",
    "    q1 = htc*(temp_init[0]-t_surr)   # Heat flux at the left boundary\n",
    "    \n",
    "    # print(f\"q1 is {q1}\")\n",
    "    temperature[0] = temp_init[0] + alpha_l * step_coeff * ((2.0*temp_init[1]) - (2.0 * temp_init[0])-(2.0*dx*(q1)))  # Update boundary condition temperature\n",
    "    \n",
    "    q2 = htc*(temp_init[-1]-t_surr)                   # Heat flux at the right boundary\n",
    "    temperature[-1] = temp_init[-1] + alpha_l * step_coeff * ((2.0*temp_init[-2]) - (2.0 * temp_init[-1])-(2.0*dx*(q2)))  # Update boundary condition temperature\n",
    "    \n",
    "    for n in range(1,num_points+1):              # space loop, adjusted range\n",
    "       \n",
    "        if temperature[n] >= T_L:\n",
    "            temperature[n] += ((alpha_l * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            phase[n] = 0\n",
    "            \n",
    "            # print(f\" Time-Step{m},Spatial point{n},Temperature{temperature[n]}\")\n",
    "        elif T_S < temperature[n] < T_L:\n",
    "            \n",
    "            k_m = kramp(temperature[n],k_l,k_s,T_L,T_S)\n",
    "            cp_m = cp_ramp(temperature[n],cp_l,cp_s,T_L,T_S)\n",
    "            rho_m = rho_ramp(temperature[n],rho_l,rho_s,T_L,T_S)\n",
    "            m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "            \n",
    "            temperature[n] += ((m_eff * step_coeff)* (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            \n",
    "            phase[n] = (T_L - temperature[n]) / (T_L - T_S)\n",
    "            # print(m,n,temperature[n],phase[n])\n",
    "         \n",
    "        elif temperature[n]<T_S:\n",
    "            temperature[n] += ((alpha_s * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n])+ temp_init[n-1]))\n",
    "            phase[n] = 1\n",
    "                     \n",
    "        else:\n",
    "            print(\"ERROR: should not be here\")\n",
    "\n",
    "     \n",
    "          \n",
    "    temperature = temperature.copy()                                                                # Update temperature\n",
    "    phase = phase.copy()                                                                            # Update phase\n",
    "    temp_init = temperature.copy()                                                                  # Update last time step temperature\n",
    "    temperature_history.append(temperature.copy())                                                  # Append the temperature history to add ghost points\n",
    "    phi_history.append(phase.copy())                                                                # Append the phase history to add ghost points\n",
    "    midpoint_temperature_history.append(temperature[midpoint_index])                                # Store midpoint temperature\n",
    "    \n",
    "    \n",
    "    # print(f\"Step {m}, Temperature: {temperature}\")\n",
    "    \n",
    "\n",
    "\n",
    "# print(midpoint_temperature_history)\n",
    "#print(phi_history)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31470, 52)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot temperature history for debugging\n",
    "temperature_history_1 = np.array(temperature_history)\n",
    "print(temperature_history_1.shape)\n",
    "time_ss= np.linspace(0, time_end, num_steps+1)\n",
    "# print(time_ss.shape)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_ss, midpoint_temperature_history, label='Midpoint Temperature')\n",
    "plt.axhline(y=T_L, color='r', linestyle='--', label='Liquidus Temperature')\n",
    "plt.axhline(y=T_S, color='g', linestyle='--', label='Solidus Temperature')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.title('Temperature Distribution Over Time at x = 7.5mm') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data into Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31470, 52)\n",
      "(31470, 52)\n",
      "(1510512,)\n",
      "(1636440, 1)\n"
     ]
    }
   ],
   "source": [
    "temperature_history = np.array(temperature_history)\n",
    "\n",
    "phi_history = np.array(phi_history)\n",
    "\n",
    "t_hist = np.array(temperature_history)  # Remove ghost points\n",
    "# p_hist = np.array(phi_history[:,1:-1])\n",
    "# t_pde = t_hist[1:,1:-1]                # Remove initial condition and boundary condition\n",
    "# # t_pde = t_pde.flatten()\n",
    "# t_hist_init = t_hist[0,:]\n",
    "# t_hist_bc_l = t_hist[:,0]\n",
    "# t_hist_bc_r = t_hist[:,-1]\n",
    "\n",
    "print(t_hist.shape)\n",
    "\n",
    "scaler_temp = StandardScaler()\n",
    "t_hist_tr= scaler_temp.fit_transform(t_hist.reshape(-1,1)) # Normalize temperature history\n",
    "t_prep = t_hist_tr.reshape(num_steps+1,num_points+2)\n",
    "print(t_prep.shape)\n",
    "t_pde = t_prep[1:,2:-2]\n",
    "t_pde = t_pde.flatten() \n",
    "print(t_pde.shape)\n",
    "# t_pde_tr = scaler_temp.fit_transform(t_pde.reshape(-1,1)) # Normalize temperature PDE\n",
    "# t_hist_init_tr = scaler_temp.fit_transform(t_hist_init.reshape(-1,1)) # Normalize initial condition\n",
    "# t_hist_bc_l_tr = scaler_temp.fit_transform(t_hist_bc_l.reshape(-1,1)) # Normalize left boundary condition\n",
    "# t_hist_bc_r_tr = scaler_temp.fit_transform(t_hist_bc_r.reshape(-1,1)) # Normalize right boundary condition\n",
    "\n",
    "print(t_hist_tr.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have temperature_history and phi_history as lists of arrays\n",
    "\n",
    "\n",
    "# # Check the new shape after transposing\n",
    "# print(\"Transposed Temperature History Shape:\", temperature_history.shape)\n",
    "# print(\"Transposed Phi History Shape:\", phi_history.shape)\n",
    "\n",
    "# # Create a meshgrid for space and time coordinates\n",
    "# space_coord, time_coord = np.meshgrid(np.arange(temperature_history.shape[1]), np.arange(temperature_history.shape[0]))\n",
    "\n",
    "# time_coord = time_coord * dt \n",
    "# # Create a figure with two subplots\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# # Plot the temperature history on the left subplot\n",
    "# im1 = ax1.pcolormesh(space_coord, time_coord, temperature_history, cmap='viridis')\n",
    "# ax1.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_title('Temperature Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im1, ax=ax1, label='Temperature')\n",
    "\n",
    "# # Plot the phase history on the right subplot\n",
    "# im2 = ax2.pcolormesh(space_coord, time_coord, phi_history, cmap='viridis')\n",
    "# ax2.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=18)\n",
    "# ax2.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax2.set_title('Phase Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im2, ax=ax2, label='Phase')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #plot the main\n",
    "# fig, ax = plt.subplots(figsize=(14, 6))\n",
    "# im = ax.pcolormesh(space_coord, time_coord, Dim_ny, cmap='viridis')\n",
    "# ax.set_xlabel('Space Coordinate')\n",
    "# ax.set_ylabel('Time')\n",
    "# ax.set_title('Niyama Variation Over Time')\n",
    "# fig.colorbar(im, ax=ax, label='Main')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU/CPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check for gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31470, 52) (31470, 52)\n",
      "(31469, 48) (31469, 48)\n",
      "(1636440,) (1636440,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "space = np.linspace(0, length, num_points+2) # Spatial points\n",
    "time = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "\n",
    "space, time = np.meshgrid(space, time)\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "\n",
    "space = scaler1.fit_transform(space.reshape(-1,1)) # Normalize spatial points\n",
    "time = scaler1.fit_transform(time.reshape(-1,1)) # Normalize time points\n",
    "\n",
    "space = space.reshape(num_steps+1,num_points+2)\n",
    "time = time.reshape(num_steps+1,num_points+2)\n",
    "\n",
    "print(space.shape,time.shape)\n",
    "\n",
    "sp_pde = space[1:,2:-2] # Spatial points\n",
    "time_pde = time[1:,2:-2]\n",
    "print(sp_pde.shape,time_pde.shape)\n",
    "\n",
    "space = space.flatten()\n",
    "time = time.flatten()\n",
    "\n",
    "sp_pde = sp_pde.flatten()\n",
    "time_pde = time_pde.flatten()\n",
    "\n",
    "print(space.shape,time.shape)\n",
    "\n",
    "# space_pde = space[1:-1] # Spatial points \n",
    "# time_pde = time[1:] # Time points\n",
    "# print(space_pde.shape,time_pde.shape)\n",
    "\n",
    "# sp_i = np.linspace(0, length, num_points) # Spatial points\n",
    "# time_i = np.zeros(num_points) # Time points\n",
    "# print(sp_i.shape,time_i.shape)\n",
    "# sp_b_l = np.zeros(num_steps+1) # Spatial points\n",
    "# time_b_l = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "# print(sp_b_l.shape,time_b_l.shape)\n",
    "# sp_b_r = np.ones(num_steps+1)*length # Spatial points\n",
    "# time_b_r = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "# print(sp_b_r.shape,time_b_r.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.column_stack((space, time)) # Input vectors for training\n",
    "inputs_pde = np.column_stack((sp_pde, time_pde)) # Input vectors for PDE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# inputs = input_gen(space,time,'mgrid')\n",
    "# inputs_pde = input_gen(space_pde,time_pde,'mgrid')\n",
    "# inputs_i = input_gen(sp_i,time_i,'scr')\n",
    "# inputs_b_l = input_gen(sp_b_l,time_b_l,'scr')\n",
    "# inputs_b_r = input_gen(sp_b_r,time_b_r,'scr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1636440, 1])\n",
      "torch.Size([1510512, 1])\n"
     ]
    }
   ],
   "source": [
    "# Convert the inputs to a tensor\n",
    "\n",
    "inputs = torch.tensor(inputs).float().to(device) \n",
    "inputs_pde = torch.tensor(inputs_pde).float().to(device) \n",
    "# inputs_init = torch.tensor(inputs_i).float().to(device) \n",
    "# inputs_b_l = torch.tensor(inputs_b_l).float().to(device)\n",
    "# inputs_b_r = torch.tensor(inputs_b_r).float().to(device)\n",
    "\n",
    "# print(inputs.shape,inputs_pde.shape,inputs_init.shape,inputs_b_l.shape,inputs_b_r.shape)\n",
    "# label/temp data to tensor\n",
    "temp_tr = torch.tensor(t_hist_tr).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp = temp_tr.reshape(-1,1).float().to(device) # Reshape the temperature tensor to a column vector\n",
    "temp_pde = torch.tensor(t_pde).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_pde = temp_pde.reshape(-1,1).float().to(device) # Reshape the temperature tensor to a column vector    \n",
    "# temp_inp_init = torch.tensor(t_hist_init_tr).float().to(device) # Convert the temperature history to a tensor\n",
    "# temp_inp_bc_l = torch.tensor(t_hist_bc_l_tr).float().to(device)# Convert the temperature history to a tensor\n",
    "# temp_inp_bc_r = torch.tensor(t_hist_bc_r_tr).float().to(device)# Convert the temperature history to a tensor\n",
    "print(temp_inp.shape)\n",
    "print(temp_pde.shape)\n",
    "\n",
    "\n",
    "#Data Splitting\n",
    "\n",
    "# train_inputs, val_test_inputs, train_temp_inp, val_test_temp_inp = train_test_split(inputs, temp_inp, test_size=0.2, random_state=42)\n",
    "# val_inputs, test_inputs, val_temp_inp, test_temp_inp = train_test_split(val_test_inputs, val_test_temp_inp, test_size=0.8, random_state=42)\n",
    "\n",
    "train_inputs, test_inputs, train_temp_inp, test_temp_inp = train_test_split(inputs, temp_inp,\\\n",
    "                                                                             test_size=0.2, random_state=42)\n",
    "train_inputs_pde, test_inputs_pde, train_temp_inp_pde, test_temp_inp_pde = train_test_split(inputs_pde, temp_pde,\\\n",
    "                                                                                             test_size=0.2, random_state=42)\n",
    "# train_inputs_init, test_inputs_init, train_temp_inp_init, test_temp_inp_init = train_test_split(inputs_init, temp_inp_init, \\\n",
    "#                                                                                                 test_size=0.2, random_state=42)\n",
    "# train_inputs_bc_l, test_inputs_bc_l, train_temp_inp_bc_l, test_temp_inp_bc_l = train_test_split(inputs_b_l, temp_inp_bc_l, \\\n",
    "#                                                                                                  test_size=0.2, random_state=42)\n",
    "# train_inputs_bc_r, test_inputs_bc_r, train_temp_inp_bc_r, test_temp_inp_bc_r = train_test_split(inputs_b_r, temp_inp_bc_r, \\\n",
    "#                                                                                                 test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, temp_inp):\n",
    "        self.inputs = inputs\n",
    "        self.temp_inp = temp_inp\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.inputs[index]\n",
    "        y = self.temp_inp[index]\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    \n",
    "\n",
    "  \n",
    "train_dataset = TensorDataset(train_inputs, train_temp_inp) # Create the training dataset\n",
    "# val_dataset = TensorDataset(val_inputs, val_temp_inp) # Create the validation dataset\n",
    "test_dataset = TensorDataset(test_inputs, test_temp_inp) # Create the test dataset\n",
    "\n",
    "train_dataset_pde = TensorDataset(train_inputs_pde, train_temp_inp_pde) # Create the training dataset\n",
    "test_dataset_pde = TensorDataset(test_inputs_pde, test_temp_inp_pde) # Create the test dataset\n",
    "\n",
    "# train_dataset_init = TensorDataset(train_inputs_init, train_temp_inp_init) # Create the training dataset\n",
    "# test_dataset_init = TensorDataset(test_inputs_init, test_temp_inp_init) # Create the test dataset\n",
    "# train_dataset_bc_l = TensorDataset(train_inputs_bc_l, train_temp_inp_bc_l) # Create the training dataset\n",
    "# test_dataset_bc_l = TensorDataset(test_inputs_bc_l, test_temp_inp_bc_l) # Create the test dataset\n",
    "# train_dataset_bc_r = TensorDataset(train_inputs_bc_r, train_temp_inp_bc_r) # Create the training dataset\n",
    "# test_dataset_bc_r = TensorDataset(test_inputs_bc_r, test_temp_inp_bc_r) # Create the test dataset\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "random_sampler_train = RandomSampler(train_dataset, replacement=True, num_samples=10000) # Create a random sampler for the training dataset\n",
    "# random_sampler_val = RandomSampler(val_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the validation dataset\n",
    "random_sampler_test = RandomSampler(test_dataset, replacement=True, num_samples=10000) # Create a random sampler for the test dataset\n",
    "\n",
    "random_sampler_train_pde = RandomSampler(train_dataset_pde, replacement=True, num_samples=5000) # Create a random sampler for the training dataset\n",
    "random_sampler_test_pde = RandomSampler(test_dataset_pde, replacement=True, num_samples=5000) # Create a random sampler for the test dataset\n",
    "\n",
    "\n",
    "# random_sampler_train_init = RandomSampler(train_dataset_init, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "# random_sampler_test_init = RandomSampler(test_dataset_init, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "# random_sampler_train_bc_l = RandomSampler(train_dataset_bc_l, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "# random_sampler_test_bc_l = RandomSampler(test_dataset_bc_l, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "# random_sampler_train_bc_r = RandomSampler(train_dataset_bc_r, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "# random_sampler_test_bc_r = RandomSampler(test_dataset_bc_r, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=random_sampler_train) # Create the training dataloader\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=random_sampler_val) # Create the validation dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=random_sampler_test) # Create the test dataloader\n",
    "\n",
    "train_loader_pde = DataLoader(train_dataset_pde, batch_size=batch_size, sampler=random_sampler_train_pde) # Create the training dataloader\n",
    "test_loader_pde = DataLoader(test_dataset_pde, batch_size=batch_size, sampler=random_sampler_test_pde) # Create the test dataloader\n",
    "\n",
    "# train_loader_init = DataLoader(train_dataset_init, batch_size=batch_size, sampler=random_sampler_train_init) # Create the training dataloader\n",
    "# test_loader_init = DataLoader(test_dataset_init, batch_size=batch_size, sampler=random_sampler_test_init) # Create the test dataloader\n",
    "# train_loader_bc_l = DataLoader(train_dataset_bc_l, batch_size=batch_size, sampler=random_sampler_train_bc_l) # Create the training dataloader\n",
    "# test_loader_bc_l = DataLoader(test_dataset_bc_l, batch_size=batch_size, sampler=random_sampler_test_bc_l) # Create the test dataloader\n",
    "# train_loader_bc_r = DataLoader(train_dataset_bc_r, batch_size=batch_size, sampler=random_sampler_train_bc_r) # Create the training dataloader\n",
    "# test_loader_bc_r = DataLoader(test_dataset_bc_r, batch_size=batch_size, sampler=random_sampler_test_bc_r) # Create the test dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural network architecture\n",
    "class Mushydata(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size): # This is the constructor\n",
    "        super(Mushydata, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):                               # This is the forward pass\n",
    "        input_features = torch.cat([x, t], dim=1)          # Concatenate the input features\n",
    "        m = self.base(input_features)                                 # Pass through the third layer\n",
    "        return m                    # Return the output of the network\n",
    "\n",
    "\n",
    "# features = torch.rand(1, 2)\n",
    "# model = HeatPINN(2, 20, 1)\n",
    "# output = model(features[:, 0:1], features[:, 1:2])\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# Loss function for data \n",
    "\n",
    "# # Apply Xavier initialization\n",
    "# def init_weights(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         torch.nn.init.xavier_uniform_(m.weight)\n",
    "#         if m.bias is not None:\n",
    "#             m.bias.data.fill_(0.01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 80\n",
    "learning_rate = 0.004\n",
    "epochs = 5000\n",
    "# alpha = 0.01  # Adjust this value based on your problem\n",
    "# boundary_value = 313.0\n",
    "# initial_value = init_temp\n",
    "# Initialize the model\n",
    "model = Mushydata(input_size=2, hidden_size=hidden_size,output_size=1).to(device)\n",
    "\n",
    "# model.apply(init_weights)\n",
    "\n",
    "# Verify the initialization\n",
    "\n",
    "lambda_l1 = 0.9\n",
    "weight_decay = 0.6  \n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Create the optimizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss List Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of train_loader is <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(f\"Datatype of train_loader is {type(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Testing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, model, loss_fn_data, optimizer, train_dataloader,test_dataloader, train_loader_pde, ):\n",
    "    train_losses = []  # Initialize the list to store the training losses\n",
    "    # val_losses = []    # Initialize the list to store the validation losses\n",
    "    test_losses = []   # Initialize the list to store the test losses\n",
    "    data_losses = []   # Initialize the list to store the data losses\n",
    "    pde_losses = []   # Initialize the list to store the PDE losses\n",
    "    ic_losses = []   # Initialize the list to store the initial condition losses\n",
    "    bc_losses = []   # Initialize the list to store the boundary condition losses\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                                                           # Set the model to training mode\n",
    "        train_loss = 0                                                                              # Initialize the training loss\n",
    "        train_accuracy = 0\n",
    "        for (batch,batch_pde) in zip(train_dataloader ,train_loader_pde):                                                          # Loop through the training dataloader\n",
    "            \n",
    "            inputs, temp_inp= batch                                                             # Get the inputs and the true values\n",
    "            inputs_pde, temp_inp_pde= batch_pde                                                             # Get the inputs and the true values\n",
    "            # inputs_init, temp_inp_init= batch_init                                                             # Get the inputs and the true values \n",
    "            # inputs_left, temp_inp_left= batch_left                                                             # Get the inputs and the true values\n",
    "            # inputs_right, temp_inp_right= batch_right                                                             # Get the inputs and the true values\n",
    "\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device)                             # Move the inputs and true values to the GPU\n",
    "            inputs_pde, temp_inp_pde= inputs_pde.to(device), temp_inp_pde.to(device)                             # Move the inputs and true values to the GPU\n",
    "            \n",
    "           \n",
    "            # print(inputs[:5,:],temp_inp[:5,:])\n",
    "            # print(inputs_pde[:5,:],temp_inp_pde[:5,:])\n",
    "            # print(inputs_init[:5,:],temp_inp_init[:5,:])\n",
    "            # print(inputs_left[:5],temp_inp_left[:5])\n",
    "            # print(inputs_right[:5],temp_inp_right[:5])\n",
    "            \n",
    "            optimizer.zero_grad()                                                                    # Zero the gradients\n",
    "            \n",
    "            # Forward pass\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).to(device)                       # Get the predictions\n",
    "            \n",
    "\n",
    "            # Loss calculation\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp)                                              # Calculate the data loss\n",
    "            \n",
    "            pd_loss = pde_loss(model,inputs[:,0].unsqueeze(1),inputs[:,1].unsqueeze(1))             # Calculate the PDE loss\n",
    "            # pd_loss = 0\n",
    "            \n",
    "            # initc_loss = ic_loss(u_initl,temp_init) \n",
    "            # initc_loss =0                                                      # Calculate initial condition loss\n",
    "            \n",
    "            # bc_loss_left = boundary_loss(model,inputs_b_l[:,0].unsqueeze(1),inputs_b_l[:,1].unsqueeze(1),t_surr) # Calculate the left boundary condition loss\n",
    "            # bc_loss_right = boundary_loss(model,inputs_b_r[:,0].unsqueeze(1),inputs_b_r[:,1].unsqueeze(1),t_surr) # Calculate the right boundary condition loss\n",
    "            # bc_loss = bc_loss_left + bc_loss_right\n",
    "            # l1_regularization_loss = l1_regularization(model, lambda_l1)                      # Calculate the L1 regularization loss\n",
    "            # loss = data_loss  + pd_loss + initc_loss + bc_loss                                              # Calculate the total loss\n",
    "            # w0 = 1\n",
    "            # w1 = 0\n",
    "            # w2 = 0.0001\n",
    "            # w3 = 0.0001\n",
    "            loss = data_loss + pd_loss \n",
    "            train_accuracy += accuracy(u_pred, temp_inp)                                                              # Calculate the total loss\n",
    "            # Backpropagation\n",
    "            loss.backward()                                                        # Backpropagate the gradients\n",
    "            \n",
    "            optimizer.step()                                                                           # Update the weights\n",
    "            \n",
    "            train_loss += loss.item()                                                           # Add the loss to the training set loss  \n",
    "            data_losses.append(data_loss.item())               \n",
    "            pde_losses.append(pd_loss.item())\n",
    "            # ic_losses.append(initc_loss.item())\n",
    "            # bc_losses.append(bc_loss.item())\n",
    "        \n",
    "\n",
    "        # model.eval()\n",
    "        # test_loss = 0\n",
    "        # test_accuracy = 0\n",
    "        # with torch.no_grad():   \n",
    "        #     for batch in test_dataloader:\n",
    "        #         inputs, temp_inp= batch\n",
    "        #         inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "        #         u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "        #         data_loss = loss_fn_data(u_pred, temp_inp)\n",
    "        #         # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "        #         # loss = data_loss  + l1_regularization_loss\n",
    "        #         loss = data_loss\n",
    "        #         test_accuracy = accuracy(u_pred, temp_inp)\n",
    "        #         test_loss += loss.item()\n",
    "        #     test_losses.append(test_loss)\n",
    "\n",
    "        train_losses.append(train_loss)                                                   # Append the training loss to the list of training losses\n",
    "        \n",
    "        # if epoch % 10 == 0:\n",
    "        #     print(f\"Epoch {epoch}, Training-Loss {train_loss:.4e}\")\n",
    "        \n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        test_accuracy = 0\n",
    "        with torch.no_grad():   \n",
    "            for batch in test_dataloader:\n",
    "                inputs, temp_inp= batch\n",
    "                inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "                u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "                data_loss_t = loss_fn_data(u_pred, temp_inp)\n",
    "                # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "                # loss = data_loss  + l1_regularization_loss\n",
    "                loss = data_loss_t\n",
    "                test_accuracy = accuracy(u_pred, temp_inp)\n",
    "                test_loss += loss.item()\n",
    "            test_losses.append(test_loss)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"| Epoch {epoch},            | Training-Loss {train_loss:.4e},| Test-Loss {test_loss:.4e}   |\")\n",
    "            print(f\"--\"*40)\n",
    "            print(f\"| Data-loss {data_loss:.4e}, pde-loss {pd_loss:.4e}       | \") \n",
    "    \n",
    "    \n",
    "\n",
    "    return train_losses, test_losses , pde_losses, data_losses                                                      # Return the training and validation losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(epochs, model, loss_fn_data, optimizer, train_dataloader, test_dataloader):\n",
    "      \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    with torch.no_grad():   \n",
    "        for batch in test_dataloader:\n",
    "            inputs, temp_inp= batch\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp)\n",
    "            # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "            # loss = data_loss  + l1_regularization_loss\n",
    "            loss = data_loss\n",
    "            test_accuracy = accuracy(u_pred, temp_inp)\n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss)\n",
    "    if epochs % 10 == 0:\n",
    "        print(f\"Epoch {epochs}, Test-Loss {test_loss:.4e}, Test-Accuracy {test_accuracy:.4e}\")      \n",
    "    return test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Button "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student.unimelb.edu.au/rrammohan/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/autograd/graph.py:744: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at ../aten/src/ATen/cuda/CublasHandlePool.cpp:135.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch 0,            | Training-Loss 1.1624e+01,| Test-Loss 1.2792e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.8465e-01, pde-loss 1.7473e-01       | \n",
      "| Epoch 10,            | Training-Loss 1.0412e+01,| Test-Loss 1.6004e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9655e-01, pde-loss 1.8353e-01       | \n",
      "| Epoch 20,            | Training-Loss 1.0482e+01,| Test-Loss 1.1483e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3357e-01, pde-loss 1.9636e-01       | \n",
      "| Epoch 30,            | Training-Loss 1.0390e+01,| Test-Loss 1.2283e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1859e-01, pde-loss 2.2214e-01       | \n",
      "| Epoch 40,            | Training-Loss 1.0588e+01,| Test-Loss 1.2482e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7256e-01, pde-loss 2.1997e-01       | \n",
      "| Epoch 50,            | Training-Loss 1.0128e+01,| Test-Loss 1.0826e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5024e-01, pde-loss 2.5423e-01       | \n",
      "| Epoch 60,            | Training-Loss 9.9326e+00,| Test-Loss 1.1283e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1768e-01, pde-loss 2.1906e-01       | \n",
      "| Epoch 70,            | Training-Loss 1.0513e+01,| Test-Loss 1.2505e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2565e-01, pde-loss 1.9710e-01       | \n",
      "| Epoch 80,            | Training-Loss 1.0127e+01,| Test-Loss 1.2092e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4393e-01, pde-loss 2.0268e-01       | \n",
      "| Epoch 90,            | Training-Loss 1.0562e+01,| Test-Loss 1.1986e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4558e-01, pde-loss 2.2812e-01       | \n",
      "| Epoch 100,            | Training-Loss 1.0559e+01,| Test-Loss 1.1231e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7714e-01, pde-loss 2.3994e-01       | \n",
      "| Epoch 110,            | Training-Loss 1.0804e+01,| Test-Loss 1.1439e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5873e-01, pde-loss 2.1967e-01       | \n",
      "| Epoch 120,            | Training-Loss 1.0344e+01,| Test-Loss 1.1021e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1300e-01, pde-loss 2.5080e-01       | \n",
      "| Epoch 130,            | Training-Loss 1.0180e+01,| Test-Loss 1.1734e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4053e-01, pde-loss 2.1692e-01       | \n",
      "| Epoch 140,            | Training-Loss 1.0287e+01,| Test-Loss 1.1062e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3210e-01, pde-loss 2.8616e-01       | \n",
      "| Epoch 150,            | Training-Loss 1.0053e+01,| Test-Loss 1.2173e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4401e-01, pde-loss 1.9244e-01       | \n",
      "| Epoch 160,            | Training-Loss 1.0314e+01,| Test-Loss 1.2574e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6923e-01, pde-loss 2.0278e-01       | \n",
      "| Epoch 170,            | Training-Loss 1.0278e+01,| Test-Loss 1.1854e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9890e-01, pde-loss 1.9791e-01       | \n",
      "| Epoch 180,            | Training-Loss 1.0056e+01,| Test-Loss 1.1541e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0056e-01, pde-loss 2.0948e-01       | \n",
      "| Epoch 190,            | Training-Loss 1.0536e+01,| Test-Loss 1.1141e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8077e-01, pde-loss 2.2384e-01       | \n",
      "| Epoch 200,            | Training-Loss 1.0286e+01,| Test-Loss 1.1136e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7178e-01, pde-loss 2.0981e-01       | \n",
      "| Epoch 210,            | Training-Loss 1.0386e+01,| Test-Loss 1.1296e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2711e-01, pde-loss 2.1057e-01       | \n",
      "| Epoch 220,            | Training-Loss 1.0416e+01,| Test-Loss 1.2403e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6172e-01, pde-loss 2.0486e-01       | \n",
      "| Epoch 230,            | Training-Loss 1.0317e+01,| Test-Loss 1.3632e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8071e-01, pde-loss 2.0953e-01       | \n",
      "| Epoch 240,            | Training-Loss 1.0479e+01,| Test-Loss 1.0924e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5464e-01, pde-loss 2.2815e-01       | \n",
      "| Epoch 250,            | Training-Loss 1.0172e+01,| Test-Loss 1.1127e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2341e-01, pde-loss 2.1031e-01       | \n",
      "| Epoch 260,            | Training-Loss 1.0153e+01,| Test-Loss 1.1146e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9785e-01, pde-loss 2.2584e-01       | \n",
      "| Epoch 270,            | Training-Loss 1.0109e+01,| Test-Loss 1.1136e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4233e-01, pde-loss 2.4123e-01       | \n",
      "| Epoch 280,            | Training-Loss 1.0113e+01,| Test-Loss 1.1478e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5297e-01, pde-loss 2.0551e-01       | \n",
      "| Epoch 290,            | Training-Loss 1.0225e+01,| Test-Loss 1.3428e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2184e-01, pde-loss 1.9723e-01       | \n",
      "| Epoch 300,            | Training-Loss 1.0126e+01,| Test-Loss 1.1518e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4235e-01, pde-loss 2.0524e-01       | \n",
      "| Epoch 310,            | Training-Loss 1.0573e+01,| Test-Loss 1.1260e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5227e-01, pde-loss 2.3108e-01       | \n",
      "| Epoch 320,            | Training-Loss 1.0024e+01,| Test-Loss 1.3983e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.9222e-01, pde-loss 1.7422e-01       | \n",
      "| Epoch 330,            | Training-Loss 1.0172e+01,| Test-Loss 1.1659e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8466e-01, pde-loss 2.0708e-01       | \n",
      "| Epoch 340,            | Training-Loss 1.0505e+01,| Test-Loss 1.1873e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9548e-01, pde-loss 2.1234e-01       | \n",
      "| Epoch 350,            | Training-Loss 1.0366e+01,| Test-Loss 1.2290e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9302e-01, pde-loss 2.0812e-01       | \n",
      "| Epoch 360,            | Training-Loss 1.0530e+01,| Test-Loss 1.2870e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5149e-01, pde-loss 2.1659e-01       | \n",
      "| Epoch 370,            | Training-Loss 1.0184e+01,| Test-Loss 1.1920e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6257e-01, pde-loss 2.1053e-01       | \n",
      "| Epoch 380,            | Training-Loss 1.0511e+01,| Test-Loss 1.0108e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6961e-01, pde-loss 2.6660e-01       | \n",
      "| Epoch 390,            | Training-Loss 1.0124e+01,| Test-Loss 1.0801e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0846e-01, pde-loss 2.3594e-01       | \n",
      "| Epoch 400,            | Training-Loss 1.0063e+01,| Test-Loss 1.2051e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5783e-01, pde-loss 2.3187e-01       | \n",
      "| Epoch 410,            | Training-Loss 1.0384e+01,| Test-Loss 1.2647e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8764e-01, pde-loss 2.3068e-01       | \n",
      "| Epoch 420,            | Training-Loss 1.0239e+01,| Test-Loss 1.2656e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5357e-01, pde-loss 2.1320e-01       | \n",
      "| Epoch 430,            | Training-Loss 1.0033e+01,| Test-Loss 1.2513e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9609e-01, pde-loss 2.0198e-01       | \n",
      "| Epoch 440,            | Training-Loss 1.0329e+01,| Test-Loss 9.6461e+00   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.0790e-01, pde-loss 2.8484e-01       | \n",
      "| Epoch 450,            | Training-Loss 1.0299e+01,| Test-Loss 1.0966e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7811e-01, pde-loss 2.2775e-01       | \n",
      "| Epoch 460,            | Training-Loss 1.0539e+01,| Test-Loss 1.0316e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0255e-01, pde-loss 2.4668e-01       | \n",
      "| Epoch 470,            | Training-Loss 1.0246e+01,| Test-Loss 1.1960e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0580e-01, pde-loss 2.1381e-01       | \n",
      "| Epoch 480,            | Training-Loss 1.0160e+01,| Test-Loss 1.2776e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6394e-01, pde-loss 2.0266e-01       | \n",
      "| Epoch 490,            | Training-Loss 1.0135e+01,| Test-Loss 1.2360e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2887e-01, pde-loss 2.2497e-01       | \n",
      "| Epoch 500,            | Training-Loss 1.0373e+01,| Test-Loss 1.1379e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8914e-01, pde-loss 2.2782e-01       | \n",
      "| Epoch 510,            | Training-Loss 1.0303e+01,| Test-Loss 1.2553e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3329e-01, pde-loss 1.9937e-01       | \n",
      "| Epoch 520,            | Training-Loss 1.0136e+01,| Test-Loss 1.1634e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8552e-01, pde-loss 2.3507e-01       | \n",
      "| Epoch 530,            | Training-Loss 1.0135e+01,| Test-Loss 1.1391e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4117e-01, pde-loss 2.4902e-01       | \n",
      "| Epoch 540,            | Training-Loss 1.0387e+01,| Test-Loss 1.2591e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5592e-01, pde-loss 2.3535e-01       | \n",
      "| Epoch 550,            | Training-Loss 1.0692e+01,| Test-Loss 1.0946e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8703e-01, pde-loss 2.1580e-01       | \n",
      "| Epoch 560,            | Training-Loss 1.0293e+01,| Test-Loss 1.1415e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 4.2798e-01, pde-loss 1.9997e-01       | \n",
      "| Epoch 570,            | Training-Loss 1.0386e+01,| Test-Loss 1.1676e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0478e-01, pde-loss 2.2274e-01       | \n",
      "| Epoch 580,            | Training-Loss 1.0435e+01,| Test-Loss 1.2491e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1541e-01, pde-loss 2.1362e-01       | \n",
      "| Epoch 590,            | Training-Loss 1.0669e+01,| Test-Loss 1.2139e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7148e-01, pde-loss 2.2569e-01       | \n",
      "| Epoch 600,            | Training-Loss 1.0415e+01,| Test-Loss 1.1902e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4802e-01, pde-loss 2.0416e-01       | \n",
      "| Epoch 610,            | Training-Loss 1.0397e+01,| Test-Loss 1.1432e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3747e-01, pde-loss 2.1607e-01       | \n",
      "| Epoch 620,            | Training-Loss 1.0313e+01,| Test-Loss 1.2472e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.7601e-01, pde-loss 1.8570e-01       | \n",
      "| Epoch 630,            | Training-Loss 1.0090e+01,| Test-Loss 1.0644e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1583e-01, pde-loss 2.3118e-01       | \n",
      "| Epoch 640,            | Training-Loss 1.0200e+01,| Test-Loss 1.1184e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5113e-01, pde-loss 2.4926e-01       | \n",
      "| Epoch 650,            | Training-Loss 1.0450e+01,| Test-Loss 1.0598e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3449e-01, pde-loss 2.2158e-01       | \n",
      "| Epoch 660,            | Training-Loss 1.0188e+01,| Test-Loss 1.3359e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5224e-01, pde-loss 1.7720e-01       | \n",
      "| Epoch 670,            | Training-Loss 1.0225e+01,| Test-Loss 1.2822e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 4.3002e-01, pde-loss 1.8538e-01       | \n",
      "| Epoch 680,            | Training-Loss 1.0225e+01,| Test-Loss 1.1865e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1088e-01, pde-loss 2.3740e-01       | \n",
      "| Epoch 690,            | Training-Loss 1.0115e+01,| Test-Loss 1.3221e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9412e-01, pde-loss 2.0154e-01       | \n",
      "| Epoch 700,            | Training-Loss 1.0365e+01,| Test-Loss 1.0797e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2723e-01, pde-loss 2.2511e-01       | \n",
      "| Epoch 710,            | Training-Loss 1.0235e+01,| Test-Loss 1.2489e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0549e-01, pde-loss 2.1988e-01       | \n",
      "| Epoch 720,            | Training-Loss 1.0268e+01,| Test-Loss 1.1749e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9840e-01, pde-loss 2.3873e-01       | \n",
      "| Epoch 730,            | Training-Loss 1.0328e+01,| Test-Loss 1.1750e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5565e-01, pde-loss 2.0393e-01       | \n",
      "| Epoch 740,            | Training-Loss 9.9462e+00,| Test-Loss 1.1316e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4821e-01, pde-loss 2.4047e-01       | \n",
      "| Epoch 750,            | Training-Loss 1.0235e+01,| Test-Loss 1.3422e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2703e-01, pde-loss 1.7652e-01       | \n",
      "| Epoch 760,            | Training-Loss 9.8553e+00,| Test-Loss 1.3355e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2900e-01, pde-loss 1.9891e-01       | \n",
      "| Epoch 770,            | Training-Loss 1.0411e+01,| Test-Loss 1.1616e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4076e-01, pde-loss 2.4808e-01       | \n",
      "| Epoch 780,            | Training-Loss 1.0227e+01,| Test-Loss 1.1804e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2692e-01, pde-loss 2.1643e-01       | \n",
      "| Epoch 790,            | Training-Loss 1.0195e+01,| Test-Loss 1.2490e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7430e-01, pde-loss 2.0154e-01       | \n",
      "| Epoch 800,            | Training-Loss 1.0388e+01,| Test-Loss 1.0947e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8809e-01, pde-loss 2.3713e-01       | \n",
      "| Epoch 810,            | Training-Loss 1.0698e+01,| Test-Loss 1.3149e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9328e-01, pde-loss 2.0561e-01       | \n",
      "| Epoch 820,            | Training-Loss 1.0396e+01,| Test-Loss 1.1329e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8535e-01, pde-loss 2.2823e-01       | \n",
      "| Epoch 830,            | Training-Loss 1.0038e+01,| Test-Loss 1.1901e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1843e-01, pde-loss 1.8791e-01       | \n",
      "| Epoch 840,            | Training-Loss 1.0228e+01,| Test-Loss 1.2410e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2772e-01, pde-loss 2.0229e-01       | \n",
      "| Epoch 850,            | Training-Loss 1.0129e+01,| Test-Loss 1.2922e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3051e-01, pde-loss 1.9060e-01       | \n",
      "| Epoch 860,            | Training-Loss 1.0325e+01,| Test-Loss 1.0510e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6096e-01, pde-loss 2.6000e-01       | \n",
      "| Epoch 870,            | Training-Loss 1.0242e+01,| Test-Loss 1.0018e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9980e-01, pde-loss 2.5940e-01       | \n",
      "| Epoch 880,            | Training-Loss 1.0309e+01,| Test-Loss 1.1759e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0864e-01, pde-loss 2.1930e-01       | \n",
      "| Epoch 890,            | Training-Loss 1.0437e+01,| Test-Loss 1.1552e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5018e-01, pde-loss 2.0342e-01       | \n",
      "| Epoch 900,            | Training-Loss 1.0162e+01,| Test-Loss 1.2128e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6187e-01, pde-loss 2.2505e-01       | \n",
      "| Epoch 910,            | Training-Loss 1.0184e+01,| Test-Loss 1.2446e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7191e-01, pde-loss 2.1140e-01       | \n",
      "| Epoch 920,            | Training-Loss 1.0488e+01,| Test-Loss 9.4720e+00   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7795e-01, pde-loss 2.7462e-01       | \n",
      "| Epoch 930,            | Training-Loss 1.0253e+01,| Test-Loss 1.0828e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7922e-01, pde-loss 2.2713e-01       | \n",
      "| Epoch 940,            | Training-Loss 1.0300e+01,| Test-Loss 1.1989e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1318e-01, pde-loss 2.0873e-01       | \n",
      "| Epoch 950,            | Training-Loss 1.0400e+01,| Test-Loss 1.1603e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3148e-01, pde-loss 2.0927e-01       | \n",
      "| Epoch 960,            | Training-Loss 1.0144e+01,| Test-Loss 1.1582e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8068e-01, pde-loss 2.2472e-01       | \n",
      "| Epoch 970,            | Training-Loss 1.0041e+01,| Test-Loss 1.0663e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7759e-01, pde-loss 2.3231e-01       | \n",
      "| Epoch 980,            | Training-Loss 1.0293e+01,| Test-Loss 1.0494e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.0736e-01, pde-loss 2.4622e-01       | \n",
      "| Epoch 990,            | Training-Loss 1.0260e+01,| Test-Loss 1.1830e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4984e-01, pde-loss 1.9717e-01       | \n",
      "| Epoch 1000,            | Training-Loss 1.0234e+01,| Test-Loss 1.1107e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5374e-01, pde-loss 2.4497e-01       | \n",
      "| Epoch 1010,            | Training-Loss 1.0449e+01,| Test-Loss 1.0906e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1490e-01, pde-loss 2.2625e-01       | \n",
      "| Epoch 1020,            | Training-Loss 1.0286e+01,| Test-Loss 1.1580e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6136e-01, pde-loss 2.3034e-01       | \n",
      "| Epoch 1030,            | Training-Loss 9.7211e+00,| Test-Loss 1.1974e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3904e-01, pde-loss 2.0901e-01       | \n",
      "| Epoch 1040,            | Training-Loss 9.8743e+00,| Test-Loss 1.1623e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1711e-01, pde-loss 2.4617e-01       | \n",
      "| Epoch 1050,            | Training-Loss 1.0019e+01,| Test-Loss 1.2338e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0280e-01, pde-loss 2.2577e-01       | \n",
      "| Epoch 1060,            | Training-Loss 1.0344e+01,| Test-Loss 1.1296e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9643e-01, pde-loss 2.1256e-01       | \n",
      "| Epoch 1070,            | Training-Loss 1.0398e+01,| Test-Loss 1.1129e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7147e-01, pde-loss 2.0959e-01       | \n",
      "| Epoch 1080,            | Training-Loss 1.0324e+01,| Test-Loss 1.2235e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5167e-01, pde-loss 2.1377e-01       | \n",
      "| Epoch 1090,            | Training-Loss 1.0117e+01,| Test-Loss 1.1732e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9794e-01, pde-loss 2.1347e-01       | \n",
      "| Epoch 1100,            | Training-Loss 1.0034e+01,| Test-Loss 1.1996e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5308e-01, pde-loss 2.0483e-01       | \n",
      "| Epoch 1110,            | Training-Loss 1.0357e+01,| Test-Loss 1.1539e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1009e-01, pde-loss 2.2341e-01       | \n",
      "| Epoch 1120,            | Training-Loss 1.0330e+01,| Test-Loss 1.0793e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3377e-01, pde-loss 2.2457e-01       | \n",
      "| Epoch 1130,            | Training-Loss 1.0401e+01,| Test-Loss 1.0531e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4543e-01, pde-loss 2.4822e-01       | \n",
      "| Epoch 1140,            | Training-Loss 1.0203e+01,| Test-Loss 1.0219e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5598e-01, pde-loss 2.4217e-01       | \n",
      "| Epoch 1150,            | Training-Loss 1.0364e+01,| Test-Loss 1.2248e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4862e-01, pde-loss 2.0294e-01       | \n",
      "| Epoch 1160,            | Training-Loss 1.0534e+01,| Test-Loss 1.1404e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0638e-01, pde-loss 2.4321e-01       | \n",
      "| Epoch 1170,            | Training-Loss 1.0097e+01,| Test-Loss 1.2075e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5533e-01, pde-loss 2.2515e-01       | \n",
      "| Epoch 1180,            | Training-Loss 1.0628e+01,| Test-Loss 1.1776e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7343e-01, pde-loss 2.2032e-01       | \n",
      "| Epoch 1190,            | Training-Loss 1.0415e+01,| Test-Loss 9.5957e+00   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8317e-01, pde-loss 2.5647e-01       | \n",
      "| Epoch 1200,            | Training-Loss 1.0355e+01,| Test-Loss 1.0438e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7818e-01, pde-loss 2.4814e-01       | \n",
      "| Epoch 1210,            | Training-Loss 1.0305e+01,| Test-Loss 1.2202e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3255e-01, pde-loss 2.0693e-01       | \n",
      "| Epoch 1220,            | Training-Loss 1.0198e+01,| Test-Loss 1.3212e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 4.1214e-01, pde-loss 1.8844e-01       | \n",
      "| Epoch 1230,            | Training-Loss 1.0157e+01,| Test-Loss 1.1717e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1961e-01, pde-loss 2.1687e-01       | \n",
      "| Epoch 1240,            | Training-Loss 1.0193e+01,| Test-Loss 1.2515e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7601e-01, pde-loss 2.2150e-01       | \n",
      "| Epoch 1250,            | Training-Loss 1.0252e+01,| Test-Loss 1.1029e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6184e-01, pde-loss 2.4187e-01       | \n",
      "| Epoch 1260,            | Training-Loss 1.0335e+01,| Test-Loss 1.0844e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2578e-01, pde-loss 2.3466e-01       | \n",
      "| Epoch 1270,            | Training-Loss 1.0091e+01,| Test-Loss 1.2786e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1292e-01, pde-loss 1.8860e-01       | \n",
      "| Epoch 1280,            | Training-Loss 1.0130e+01,| Test-Loss 1.1726e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5585e-01, pde-loss 2.0632e-01       | \n",
      "| Epoch 1290,            | Training-Loss 1.0128e+01,| Test-Loss 1.1331e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5937e-01, pde-loss 2.4089e-01       | \n",
      "| Epoch 1300,            | Training-Loss 1.0182e+01,| Test-Loss 1.2056e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1129e-01, pde-loss 2.0974e-01       | \n",
      "| Epoch 1310,            | Training-Loss 1.0202e+01,| Test-Loss 1.0702e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9705e-01, pde-loss 2.2143e-01       | \n",
      "| Epoch 1320,            | Training-Loss 1.0452e+01,| Test-Loss 1.0725e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0856e-01, pde-loss 2.2926e-01       | \n",
      "| Epoch 1330,            | Training-Loss 1.0642e+01,| Test-Loss 1.2031e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9494e-01, pde-loss 2.2352e-01       | \n",
      "| Epoch 1340,            | Training-Loss 1.0211e+01,| Test-Loss 1.0821e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8484e-01, pde-loss 2.3845e-01       | \n",
      "| Epoch 1350,            | Training-Loss 1.0299e+01,| Test-Loss 1.1210e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4319e-01, pde-loss 2.0541e-01       | \n",
      "| Epoch 1360,            | Training-Loss 1.0324e+01,| Test-Loss 1.1352e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.0498e-01, pde-loss 2.4779e-01       | \n",
      "| Epoch 1370,            | Training-Loss 1.0407e+01,| Test-Loss 1.0759e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8498e-01, pde-loss 2.3936e-01       | \n",
      "| Epoch 1380,            | Training-Loss 1.0378e+01,| Test-Loss 1.1614e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.7268e-01, pde-loss 1.9561e-01       | \n",
      "| Epoch 1390,            | Training-Loss 1.0184e+01,| Test-Loss 1.1447e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9430e-01, pde-loss 2.2553e-01       | \n",
      "| Epoch 1400,            | Training-Loss 1.0243e+01,| Test-Loss 1.2826e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2978e-01, pde-loss 1.9161e-01       | \n",
      "| Epoch 1410,            | Training-Loss 1.0154e+01,| Test-Loss 1.1611e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9552e-01, pde-loss 2.1289e-01       | \n",
      "| Epoch 1420,            | Training-Loss 1.0223e+01,| Test-Loss 1.1351e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4283e-01, pde-loss 2.4583e-01       | \n",
      "| Epoch 1430,            | Training-Loss 1.0167e+01,| Test-Loss 1.0773e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 1.8429e-01, pde-loss 2.6317e-01       | \n",
      "| Epoch 1440,            | Training-Loss 1.0388e+01,| Test-Loss 1.2039e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8190e-01, pde-loss 2.1199e-01       | \n",
      "| Epoch 1450,            | Training-Loss 1.0324e+01,| Test-Loss 1.1138e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.8424e-01, pde-loss 2.1423e-01       | \n",
      "| Epoch 1460,            | Training-Loss 1.0324e+01,| Test-Loss 1.1922e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.7891e-01, pde-loss 2.0669e-01       | \n",
      "| Epoch 1470,            | Training-Loss 1.0063e+01,| Test-Loss 1.1901e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4871e-01, pde-loss 2.2839e-01       | \n",
      "| Epoch 1480,            | Training-Loss 1.0546e+01,| Test-Loss 1.1348e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3274e-01, pde-loss 2.2423e-01       | \n",
      "| Epoch 1490,            | Training-Loss 1.0347e+01,| Test-Loss 1.1245e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0364e-01, pde-loss 2.2114e-01       | \n",
      "| Epoch 1500,            | Training-Loss 1.0342e+01,| Test-Loss 1.0681e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1613e-01, pde-loss 2.4628e-01       | \n",
      "| Epoch 1510,            | Training-Loss 1.0164e+01,| Test-Loss 1.1987e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4460e-01, pde-loss 2.0957e-01       | \n",
      "| Epoch 1520,            | Training-Loss 1.0070e+01,| Test-Loss 1.2358e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5391e-01, pde-loss 2.0775e-01       | \n",
      "| Epoch 1530,            | Training-Loss 1.0263e+01,| Test-Loss 1.1762e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7432e-01, pde-loss 2.1644e-01       | \n",
      "| Epoch 1540,            | Training-Loss 1.0350e+01,| Test-Loss 1.1505e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2728e-01, pde-loss 2.1772e-01       | \n",
      "| Epoch 1550,            | Training-Loss 1.0160e+01,| Test-Loss 1.1395e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7850e-01, pde-loss 2.3790e-01       | \n",
      "| Epoch 1560,            | Training-Loss 1.0262e+01,| Test-Loss 1.1538e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4264e-01, pde-loss 2.2075e-01       | \n",
      "| Epoch 1570,            | Training-Loss 1.0543e+01,| Test-Loss 1.2621e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3567e-01, pde-loss 2.5384e-01       | \n",
      "| Epoch 1580,            | Training-Loss 1.0210e+01,| Test-Loss 1.0713e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6674e-01, pde-loss 2.4464e-01       | \n",
      "| Epoch 1590,            | Training-Loss 1.0410e+01,| Test-Loss 1.1892e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3429e-01, pde-loss 2.1187e-01       | \n",
      "| Epoch 1600,            | Training-Loss 1.0096e+01,| Test-Loss 1.1363e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.9209e-01, pde-loss 2.1586e-01       | \n",
      "| Epoch 1610,            | Training-Loss 1.0226e+01,| Test-Loss 1.1381e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1117e-01, pde-loss 2.1888e-01       | \n",
      "| Epoch 1620,            | Training-Loss 1.0266e+01,| Test-Loss 1.1200e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5679e-01, pde-loss 2.3277e-01       | \n",
      "| Epoch 1630,            | Training-Loss 1.0151e+01,| Test-Loss 1.1587e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5461e-01, pde-loss 2.2223e-01       | \n",
      "| Epoch 1640,            | Training-Loss 1.0255e+01,| Test-Loss 1.2757e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0564e-01, pde-loss 1.8835e-01       | \n",
      "| Epoch 1650,            | Training-Loss 1.0366e+01,| Test-Loss 1.1072e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8643e-01, pde-loss 2.4882e-01       | \n",
      "| Epoch 1660,            | Training-Loss 1.0190e+01,| Test-Loss 1.2673e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6916e-01, pde-loss 2.1115e-01       | \n",
      "| Epoch 1670,            | Training-Loss 1.0262e+01,| Test-Loss 1.1650e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1071e-01, pde-loss 2.3111e-01       | \n",
      "| Epoch 1680,            | Training-Loss 1.0481e+01,| Test-Loss 1.1810e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2510e-01, pde-loss 2.1669e-01       | \n",
      "| Epoch 1690,            | Training-Loss 1.0488e+01,| Test-Loss 1.1609e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.7635e-01, pde-loss 2.2568e-01       | \n",
      "| Epoch 1700,            | Training-Loss 1.0314e+01,| Test-Loss 1.1474e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7623e-01, pde-loss 2.3827e-01       | \n",
      "| Epoch 1710,            | Training-Loss 1.0095e+01,| Test-Loss 1.1644e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9771e-01, pde-loss 2.1387e-01       | \n",
      "| Epoch 1720,            | Training-Loss 1.0484e+01,| Test-Loss 1.1054e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8637e-01, pde-loss 2.3232e-01       | \n",
      "| Epoch 1730,            | Training-Loss 1.0641e+01,| Test-Loss 1.0899e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1897e-01, pde-loss 2.4000e-01       | \n",
      "| Epoch 1740,            | Training-Loss 1.0147e+01,| Test-Loss 1.2962e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1963e-01, pde-loss 1.9974e-01       | \n",
      "| Epoch 1750,            | Training-Loss 1.0282e+01,| Test-Loss 1.1348e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3611e-01, pde-loss 2.1075e-01       | \n",
      "| Epoch 1760,            | Training-Loss 1.0375e+01,| Test-Loss 1.0627e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2129e-01, pde-loss 2.5584e-01       | \n",
      "| Epoch 1770,            | Training-Loss 1.0120e+01,| Test-Loss 1.2285e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6658e-01, pde-loss 1.9269e-01       | \n",
      "| Epoch 1780,            | Training-Loss 1.0224e+01,| Test-Loss 1.1624e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6418e-01, pde-loss 2.0523e-01       | \n",
      "| Epoch 1790,            | Training-Loss 1.0316e+01,| Test-Loss 1.1691e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7400e-01, pde-loss 2.2839e-01       | \n",
      "| Epoch 1800,            | Training-Loss 1.0273e+01,| Test-Loss 1.1979e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1761e-01, pde-loss 2.3022e-01       | \n",
      "| Epoch 1810,            | Training-Loss 1.0383e+01,| Test-Loss 1.2410e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4039e-01, pde-loss 2.2473e-01       | \n",
      "| Epoch 1820,            | Training-Loss 1.0138e+01,| Test-Loss 1.1837e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.7733e-01, pde-loss 1.9867e-01       | \n",
      "| Epoch 1830,            | Training-Loss 1.0361e+01,| Test-Loss 1.1134e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6140e-01, pde-loss 2.3358e-01       | \n",
      "| Epoch 1840,            | Training-Loss 1.0289e+01,| Test-Loss 1.0586e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9361e-01, pde-loss 2.4671e-01       | \n",
      "| Epoch 1850,            | Training-Loss 1.0171e+01,| Test-Loss 1.1723e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3247e-01, pde-loss 2.2176e-01       | \n",
      "| Epoch 1860,            | Training-Loss 1.0432e+01,| Test-Loss 1.2262e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5285e-01, pde-loss 2.2672e-01       | \n",
      "| Epoch 1870,            | Training-Loss 1.0307e+01,| Test-Loss 1.1070e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6929e-01, pde-loss 2.2354e-01       | \n",
      "| Epoch 1880,            | Training-Loss 1.0234e+01,| Test-Loss 1.1886e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.7108e-01, pde-loss 2.1154e-01       | \n",
      "| Epoch 1890,            | Training-Loss 1.0426e+01,| Test-Loss 1.1640e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3415e-01, pde-loss 2.3972e-01       | \n",
      "| Epoch 1900,            | Training-Loss 1.0282e+01,| Test-Loss 1.1674e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6406e-01, pde-loss 2.1156e-01       | \n",
      "| Epoch 1910,            | Training-Loss 1.0216e+01,| Test-Loss 1.1376e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3820e-01, pde-loss 2.1769e-01       | \n",
      "| Epoch 1920,            | Training-Loss 1.0284e+01,| Test-Loss 1.2255e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3799e-01, pde-loss 2.1668e-01       | \n",
      "| Epoch 1930,            | Training-Loss 1.0406e+01,| Test-Loss 1.1736e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6047e-01, pde-loss 2.3459e-01       | \n",
      "| Epoch 1940,            | Training-Loss 1.0392e+01,| Test-Loss 1.1130e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5162e-01, pde-loss 2.0006e-01       | \n",
      "| Epoch 1950,            | Training-Loss 1.0488e+01,| Test-Loss 1.1545e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3701e-01, pde-loss 2.1017e-01       | \n",
      "| Epoch 1960,            | Training-Loss 1.0301e+01,| Test-Loss 1.1724e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1314e-01, pde-loss 2.1817e-01       | \n",
      "| Epoch 1970,            | Training-Loss 1.0381e+01,| Test-Loss 1.2147e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5043e-01, pde-loss 2.2525e-01       | \n",
      "| Epoch 1980,            | Training-Loss 9.8723e+00,| Test-Loss 1.1419e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8797e-01, pde-loss 2.2176e-01       | \n",
      "| Epoch 1990,            | Training-Loss 1.0347e+01,| Test-Loss 1.2262e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9248e-01, pde-loss 2.0297e-01       | \n",
      "| Epoch 2000,            | Training-Loss 1.0172e+01,| Test-Loss 1.1980e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3783e-01, pde-loss 2.3612e-01       | \n",
      "| Epoch 2010,            | Training-Loss 1.0540e+01,| Test-Loss 1.2266e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8678e-01, pde-loss 2.2468e-01       | \n",
      "| Epoch 2020,            | Training-Loss 1.0302e+01,| Test-Loss 1.1629e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5211e-01, pde-loss 2.3399e-01       | \n",
      "| Epoch 2030,            | Training-Loss 1.0402e+01,| Test-Loss 1.1215e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9799e-01, pde-loss 2.3346e-01       | \n",
      "| Epoch 2040,            | Training-Loss 1.0240e+01,| Test-Loss 1.1523e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6503e-01, pde-loss 2.0741e-01       | \n",
      "| Epoch 2050,            | Training-Loss 1.0094e+01,| Test-Loss 1.1957e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7888e-01, pde-loss 2.0460e-01       | \n",
      "| Epoch 2060,            | Training-Loss 1.0415e+01,| Test-Loss 1.2479e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4249e-01, pde-loss 2.3338e-01       | \n",
      "| Epoch 2070,            | Training-Loss 1.0269e+01,| Test-Loss 1.0981e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8680e-01, pde-loss 2.5698e-01       | \n",
      "| Epoch 2080,            | Training-Loss 1.0487e+01,| Test-Loss 1.0399e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3908e-01, pde-loss 2.4505e-01       | \n",
      "| Epoch 2090,            | Training-Loss 1.0370e+01,| Test-Loss 1.1599e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9870e-01, pde-loss 2.0826e-01       | \n",
      "| Epoch 2100,            | Training-Loss 1.0169e+01,| Test-Loss 1.2294e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5692e-01, pde-loss 2.1301e-01       | \n",
      "| Epoch 2110,            | Training-Loss 1.0429e+01,| Test-Loss 1.1284e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0390e-01, pde-loss 2.3321e-01       | \n",
      "| Epoch 2120,            | Training-Loss 9.9646e+00,| Test-Loss 1.2285e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6490e-01, pde-loss 2.1742e-01       | \n",
      "| Epoch 2130,            | Training-Loss 1.0238e+01,| Test-Loss 1.2188e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8432e-01, pde-loss 2.1602e-01       | \n",
      "| Epoch 2140,            | Training-Loss 1.0453e+01,| Test-Loss 1.0654e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3394e-01, pde-loss 2.3718e-01       | \n",
      "| Epoch 2150,            | Training-Loss 1.0442e+01,| Test-Loss 1.1367e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1053e-01, pde-loss 2.3898e-01       | \n",
      "| Epoch 2160,            | Training-Loss 1.0311e+01,| Test-Loss 1.1922e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2438e-01, pde-loss 2.0893e-01       | \n",
      "| Epoch 2170,            | Training-Loss 1.0585e+01,| Test-Loss 1.1809e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5260e-01, pde-loss 2.0949e-01       | \n",
      "| Epoch 2180,            | Training-Loss 1.0584e+01,| Test-Loss 1.1247e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3427e-01, pde-loss 2.4412e-01       | \n",
      "| Epoch 2190,            | Training-Loss 1.0326e+01,| Test-Loss 1.1491e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2993e-01, pde-loss 2.3462e-01       | \n",
      "| Epoch 2200,            | Training-Loss 1.0310e+01,| Test-Loss 1.1501e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2577e-01, pde-loss 2.4227e-01       | \n",
      "| Epoch 2210,            | Training-Loss 1.0350e+01,| Test-Loss 1.1021e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2448e-01, pde-loss 2.3795e-01       | \n",
      "| Epoch 2220,            | Training-Loss 1.0009e+01,| Test-Loss 1.1165e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6594e-01, pde-loss 2.4924e-01       | \n",
      "| Epoch 2230,            | Training-Loss 1.0298e+01,| Test-Loss 1.1330e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1141e-01, pde-loss 2.2066e-01       | \n",
      "| Epoch 2240,            | Training-Loss 1.0139e+01,| Test-Loss 1.1746e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4766e-01, pde-loss 2.1881e-01       | \n",
      "| Epoch 2250,            | Training-Loss 1.0099e+01,| Test-Loss 1.1458e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2883e-01, pde-loss 2.3025e-01       | \n",
      "| Epoch 2260,            | Training-Loss 1.0076e+01,| Test-Loss 1.1458e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3711e-01, pde-loss 2.3073e-01       | \n",
      "| Epoch 2270,            | Training-Loss 1.0192e+01,| Test-Loss 1.1361e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4568e-01, pde-loss 2.3966e-01       | \n",
      "| Epoch 2280,            | Training-Loss 1.0034e+01,| Test-Loss 1.1155e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3399e-01, pde-loss 2.0727e-01       | \n",
      "| Epoch 2290,            | Training-Loss 1.0308e+01,| Test-Loss 1.2357e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.7405e-01, pde-loss 1.8578e-01       | \n",
      "| Epoch 2300,            | Training-Loss 1.0192e+01,| Test-Loss 1.2127e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5296e-01, pde-loss 2.1444e-01       | \n",
      "| Epoch 2310,            | Training-Loss 1.0243e+01,| Test-Loss 1.1632e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8273e-01, pde-loss 2.2701e-01       | \n",
      "| Epoch 2320,            | Training-Loss 1.0495e+01,| Test-Loss 1.0899e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2610e-01, pde-loss 2.3644e-01       | \n",
      "| Epoch 2330,            | Training-Loss 1.0543e+01,| Test-Loss 1.0944e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1404e-01, pde-loss 2.5279e-01       | \n",
      "| Epoch 2340,            | Training-Loss 1.0223e+01,| Test-Loss 1.1580e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2902e-01, pde-loss 2.2721e-01       | \n",
      "| Epoch 2350,            | Training-Loss 1.0264e+01,| Test-Loss 1.1851e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2909e-01, pde-loss 2.1333e-01       | \n",
      "| Epoch 2360,            | Training-Loss 1.0314e+01,| Test-Loss 1.2130e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3514e-01, pde-loss 2.0918e-01       | \n",
      "| Epoch 2370,            | Training-Loss 1.0694e+01,| Test-Loss 1.1047e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0084e-01, pde-loss 2.3862e-01       | \n",
      "| Epoch 2380,            | Training-Loss 1.0153e+01,| Test-Loss 1.3285e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5433e-01, pde-loss 2.1524e-01       | \n",
      "| Epoch 2390,            | Training-Loss 1.0317e+01,| Test-Loss 1.0571e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4708e-01, pde-loss 2.6619e-01       | \n",
      "| Epoch 2400,            | Training-Loss 1.0363e+01,| Test-Loss 1.2123e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2247e-01, pde-loss 2.1850e-01       | \n",
      "| Epoch 2410,            | Training-Loss 1.0272e+01,| Test-Loss 1.0683e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7956e-01, pde-loss 2.3020e-01       | \n",
      "| Epoch 2420,            | Training-Loss 1.0270e+01,| Test-Loss 1.2951e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9594e-01, pde-loss 2.1345e-01       | \n",
      "| Epoch 2430,            | Training-Loss 1.0500e+01,| Test-Loss 1.0851e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6391e-01, pde-loss 2.5834e-01       | \n",
      "| Epoch 2440,            | Training-Loss 1.0333e+01,| Test-Loss 1.1786e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5577e-01, pde-loss 2.0836e-01       | \n",
      "| Epoch 2450,            | Training-Loss 1.0101e+01,| Test-Loss 1.2098e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8326e-01, pde-loss 2.1757e-01       | \n",
      "| Epoch 2460,            | Training-Loss 1.0073e+01,| Test-Loss 1.1707e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6153e-01, pde-loss 2.1747e-01       | \n",
      "| Epoch 2470,            | Training-Loss 1.0193e+01,| Test-Loss 1.0672e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2311e-01, pde-loss 2.5404e-01       | \n",
      "| Epoch 2480,            | Training-Loss 1.0418e+01,| Test-Loss 1.1437e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6772e-01, pde-loss 2.0114e-01       | \n",
      "| Epoch 2490,            | Training-Loss 1.0145e+01,| Test-Loss 1.1760e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 4.0243e-01, pde-loss 2.0931e-01       | \n",
      "| Epoch 2500,            | Training-Loss 1.0332e+01,| Test-Loss 1.1589e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8619e-01, pde-loss 2.2701e-01       | \n",
      "| Epoch 2510,            | Training-Loss 1.0381e+01,| Test-Loss 1.0806e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3517e-01, pde-loss 2.4622e-01       | \n",
      "| Epoch 2520,            | Training-Loss 1.0124e+01,| Test-Loss 1.2348e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0198e-01, pde-loss 2.1807e-01       | \n",
      "| Epoch 2530,            | Training-Loss 1.0287e+01,| Test-Loss 1.1581e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7553e-01, pde-loss 2.2581e-01       | \n",
      "| Epoch 2540,            | Training-Loss 1.0237e+01,| Test-Loss 1.1761e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7389e-01, pde-loss 2.2842e-01       | \n",
      "| Epoch 2550,            | Training-Loss 1.0374e+01,| Test-Loss 1.1475e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5039e-01, pde-loss 2.1855e-01       | \n",
      "| Epoch 2560,            | Training-Loss 1.0439e+01,| Test-Loss 1.1440e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8043e-01, pde-loss 2.3180e-01       | \n",
      "| Epoch 2570,            | Training-Loss 1.0275e+01,| Test-Loss 1.2534e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1533e-01, pde-loss 2.0770e-01       | \n",
      "| Epoch 2580,            | Training-Loss 1.0110e+01,| Test-Loss 1.2388e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0747e-01, pde-loss 2.0637e-01       | \n",
      "| Epoch 2590,            | Training-Loss 1.0293e+01,| Test-Loss 1.1121e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8085e-01, pde-loss 2.3043e-01       | \n",
      "| Epoch 2600,            | Training-Loss 1.0132e+01,| Test-Loss 1.2586e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2605e-01, pde-loss 2.2434e-01       | \n",
      "| Epoch 2610,            | Training-Loss 1.0375e+01,| Test-Loss 1.0921e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9389e-01, pde-loss 2.2854e-01       | \n",
      "| Epoch 2620,            | Training-Loss 1.0267e+01,| Test-Loss 1.1592e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3921e-01, pde-loss 2.0270e-01       | \n",
      "| Epoch 2630,            | Training-Loss 1.0476e+01,| Test-Loss 9.7236e+00   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7232e-01, pde-loss 2.4823e-01       | \n",
      "| Epoch 2640,            | Training-Loss 1.0441e+01,| Test-Loss 1.2030e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5746e-01, pde-loss 2.2144e-01       | \n",
      "| Epoch 2650,            | Training-Loss 1.0410e+01,| Test-Loss 1.1288e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4151e-01, pde-loss 2.3980e-01       | \n",
      "| Epoch 2660,            | Training-Loss 1.0026e+01,| Test-Loss 1.2324e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7915e-01, pde-loss 2.2097e-01       | \n",
      "| Epoch 2670,            | Training-Loss 1.0095e+01,| Test-Loss 1.2422e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7195e-01, pde-loss 1.9764e-01       | \n",
      "| Epoch 2680,            | Training-Loss 1.0218e+01,| Test-Loss 1.2398e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6264e-01, pde-loss 2.0864e-01       | \n",
      "| Epoch 2690,            | Training-Loss 1.0435e+01,| Test-Loss 1.1834e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6266e-01, pde-loss 2.1938e-01       | \n",
      "| Epoch 2700,            | Training-Loss 1.0249e+01,| Test-Loss 1.1805e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6323e-01, pde-loss 2.2361e-01       | \n",
      "| Epoch 2710,            | Training-Loss 1.0279e+01,| Test-Loss 1.1510e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0885e-01, pde-loss 2.2498e-01       | \n",
      "| Epoch 2720,            | Training-Loss 1.0142e+01,| Test-Loss 1.1292e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4864e-01, pde-loss 2.1824e-01       | \n",
      "| Epoch 2730,            | Training-Loss 1.0672e+01,| Test-Loss 1.1063e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3772e-01, pde-loss 2.3112e-01       | \n",
      "| Epoch 2740,            | Training-Loss 1.0429e+01,| Test-Loss 1.1014e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.8270e-01, pde-loss 2.0478e-01       | \n",
      "| Epoch 2750,            | Training-Loss 1.0306e+01,| Test-Loss 1.0901e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4859e-01, pde-loss 2.2525e-01       | \n",
      "| Epoch 2760,            | Training-Loss 1.0202e+01,| Test-Loss 1.1984e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4532e-01, pde-loss 2.2744e-01       | \n",
      "| Epoch 2770,            | Training-Loss 1.0472e+01,| Test-Loss 1.2136e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3298e-01, pde-loss 2.2047e-01       | \n",
      "| Epoch 2780,            | Training-Loss 1.0443e+01,| Test-Loss 1.2875e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3344e-01, pde-loss 2.0203e-01       | \n",
      "| Epoch 2790,            | Training-Loss 1.0078e+01,| Test-Loss 1.2495e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4005e-01, pde-loss 2.0818e-01       | \n",
      "| Epoch 2800,            | Training-Loss 1.0245e+01,| Test-Loss 1.1844e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6616e-01, pde-loss 2.0356e-01       | \n",
      "| Epoch 2810,            | Training-Loss 1.0271e+01,| Test-Loss 1.1305e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5304e-01, pde-loss 2.1734e-01       | \n",
      "| Epoch 2820,            | Training-Loss 1.0161e+01,| Test-Loss 1.1123e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3272e-01, pde-loss 2.5199e-01       | \n",
      "| Epoch 2830,            | Training-Loss 1.0094e+01,| Test-Loss 1.1193e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8437e-01, pde-loss 2.3639e-01       | \n",
      "| Epoch 2840,            | Training-Loss 1.0577e+01,| Test-Loss 1.2246e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6504e-01, pde-loss 1.9837e-01       | \n",
      "| Epoch 2850,            | Training-Loss 1.0261e+01,| Test-Loss 1.2091e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6775e-01, pde-loss 1.9191e-01       | \n",
      "| Epoch 2860,            | Training-Loss 1.0444e+01,| Test-Loss 1.2279e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 4.5883e-01, pde-loss 1.8782e-01       | \n",
      "| Epoch 2870,            | Training-Loss 1.0316e+01,| Test-Loss 1.2428e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0461e-01, pde-loss 2.1267e-01       | \n",
      "| Epoch 2880,            | Training-Loss 1.0493e+01,| Test-Loss 1.1514e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1126e-01, pde-loss 2.1123e-01       | \n",
      "| Epoch 2890,            | Training-Loss 9.9973e+00,| Test-Loss 1.1527e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9519e-01, pde-loss 2.1494e-01       | \n",
      "| Epoch 2900,            | Training-Loss 1.0066e+01,| Test-Loss 1.1351e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5130e-01, pde-loss 2.1480e-01       | \n",
      "| Epoch 2910,            | Training-Loss 1.0459e+01,| Test-Loss 1.1490e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1363e-01, pde-loss 2.1280e-01       | \n",
      "| Epoch 2920,            | Training-Loss 1.0099e+01,| Test-Loss 1.1583e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9996e-01, pde-loss 2.2270e-01       | \n",
      "| Epoch 2930,            | Training-Loss 1.0565e+01,| Test-Loss 1.1863e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0974e-01, pde-loss 2.2612e-01       | \n",
      "| Epoch 2940,            | Training-Loss 1.0305e+01,| Test-Loss 1.1423e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2373e-01, pde-loss 2.5772e-01       | \n",
      "| Epoch 2950,            | Training-Loss 1.0402e+01,| Test-Loss 1.1306e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6367e-01, pde-loss 2.0790e-01       | \n",
      "| Epoch 2960,            | Training-Loss 1.0054e+01,| Test-Loss 1.2627e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2910e-01, pde-loss 1.8746e-01       | \n",
      "| Epoch 2970,            | Training-Loss 1.0099e+01,| Test-Loss 1.2726e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7585e-01, pde-loss 2.2676e-01       | \n",
      "| Epoch 2980,            | Training-Loss 1.0166e+01,| Test-Loss 1.1341e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6248e-01, pde-loss 2.3171e-01       | \n",
      "| Epoch 2990,            | Training-Loss 1.0068e+01,| Test-Loss 1.1784e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1741e-01, pde-loss 2.0749e-01       | \n",
      "| Epoch 3000,            | Training-Loss 1.0208e+01,| Test-Loss 1.1204e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6282e-01, pde-loss 2.2167e-01       | \n",
      "| Epoch 3010,            | Training-Loss 1.0201e+01,| Test-Loss 1.2158e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4437e-01, pde-loss 2.2779e-01       | \n",
      "| Epoch 3020,            | Training-Loss 1.0281e+01,| Test-Loss 1.0480e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7581e-01, pde-loss 2.3439e-01       | \n",
      "| Epoch 3030,            | Training-Loss 1.0085e+01,| Test-Loss 1.1277e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5106e-01, pde-loss 2.2138e-01       | \n",
      "| Epoch 3040,            | Training-Loss 9.5384e+00,| Test-Loss 1.2350e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5460e-01, pde-loss 2.1203e-01       | \n",
      "| Epoch 3050,            | Training-Loss 1.0239e+01,| Test-Loss 1.2628e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9011e-01, pde-loss 2.0268e-01       | \n",
      "| Epoch 3060,            | Training-Loss 1.0343e+01,| Test-Loss 1.1225e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4854e-01, pde-loss 2.0886e-01       | \n",
      "| Epoch 3070,            | Training-Loss 1.0470e+01,| Test-Loss 1.1777e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1289e-01, pde-loss 2.1236e-01       | \n",
      "| Epoch 3080,            | Training-Loss 1.0073e+01,| Test-Loss 1.1530e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2562e-01, pde-loss 2.4133e-01       | \n",
      "| Epoch 3090,            | Training-Loss 1.0218e+01,| Test-Loss 1.1586e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6033e-01, pde-loss 2.0645e-01       | \n",
      "| Epoch 3100,            | Training-Loss 1.0161e+01,| Test-Loss 1.2438e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.7071e-01, pde-loss 1.9443e-01       | \n",
      "| Epoch 3110,            | Training-Loss 1.0712e+01,| Test-Loss 1.1334e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6897e-01, pde-loss 2.1351e-01       | \n",
      "| Epoch 3120,            | Training-Loss 1.0035e+01,| Test-Loss 1.2772e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4201e-01, pde-loss 2.0943e-01       | \n",
      "| Epoch 3130,            | Training-Loss 1.0399e+01,| Test-Loss 1.2067e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5391e-01, pde-loss 2.2297e-01       | \n",
      "| Epoch 3140,            | Training-Loss 1.0112e+01,| Test-Loss 1.1981e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9527e-01, pde-loss 2.0244e-01       | \n",
      "| Epoch 3150,            | Training-Loss 1.0129e+01,| Test-Loss 1.1602e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9412e-01, pde-loss 2.2674e-01       | \n",
      "| Epoch 3160,            | Training-Loss 1.0423e+01,| Test-Loss 1.1893e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5249e-01, pde-loss 2.1729e-01       | \n",
      "| Epoch 3170,            | Training-Loss 1.0568e+01,| Test-Loss 1.0676e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1102e-01, pde-loss 2.3975e-01       | \n",
      "| Epoch 3180,            | Training-Loss 1.0520e+01,| Test-Loss 1.1926e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.7543e-01, pde-loss 2.0958e-01       | \n",
      "| Epoch 3190,            | Training-Loss 1.0521e+01,| Test-Loss 1.1904e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2598e-01, pde-loss 2.2395e-01       | \n",
      "| Epoch 3200,            | Training-Loss 1.0342e+01,| Test-Loss 1.2200e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5936e-01, pde-loss 2.2046e-01       | \n",
      "| Epoch 3210,            | Training-Loss 1.0319e+01,| Test-Loss 1.1107e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1281e-01, pde-loss 2.2303e-01       | \n",
      "| Epoch 3220,            | Training-Loss 1.0616e+01,| Test-Loss 1.2051e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1961e-01, pde-loss 2.1937e-01       | \n",
      "| Epoch 3230,            | Training-Loss 1.0362e+01,| Test-Loss 1.2046e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8338e-01, pde-loss 2.3686e-01       | \n",
      "| Epoch 3240,            | Training-Loss 1.0338e+01,| Test-Loss 1.1058e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6830e-01, pde-loss 2.4056e-01       | \n",
      "| Epoch 3250,            | Training-Loss 1.0281e+01,| Test-Loss 1.1018e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6694e-01, pde-loss 2.4190e-01       | \n",
      "| Epoch 3260,            | Training-Loss 1.0333e+01,| Test-Loss 1.1649e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6368e-01, pde-loss 2.2129e-01       | \n",
      "| Epoch 3270,            | Training-Loss 1.0394e+01,| Test-Loss 1.1979e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5225e-01, pde-loss 2.2893e-01       | \n",
      "| Epoch 3280,            | Training-Loss 1.0279e+01,| Test-Loss 1.1160e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4385e-01, pde-loss 2.3551e-01       | \n",
      "| Epoch 3290,            | Training-Loss 1.0285e+01,| Test-Loss 1.1764e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0914e-01, pde-loss 2.2001e-01       | \n",
      "| Epoch 3300,            | Training-Loss 1.0256e+01,| Test-Loss 1.0999e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1351e-01, pde-loss 2.2347e-01       | \n",
      "| Epoch 3310,            | Training-Loss 1.0433e+01,| Test-Loss 1.1914e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5578e-01, pde-loss 2.1135e-01       | \n",
      "| Epoch 3320,            | Training-Loss 1.0099e+01,| Test-Loss 1.1183e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1612e-01, pde-loss 2.4543e-01       | \n",
      "| Epoch 3330,            | Training-Loss 1.0161e+01,| Test-Loss 1.1517e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9482e-01, pde-loss 2.2909e-01       | \n",
      "| Epoch 3340,            | Training-Loss 1.0597e+01,| Test-Loss 1.0575e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 1.7981e-01, pde-loss 2.5918e-01       | \n",
      "| Epoch 3350,            | Training-Loss 1.0362e+01,| Test-Loss 1.1774e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2026e-01, pde-loss 2.2591e-01       | \n",
      "| Epoch 3360,            | Training-Loss 1.0434e+01,| Test-Loss 1.0975e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1113e-01, pde-loss 2.3293e-01       | \n",
      "| Epoch 3370,            | Training-Loss 9.9818e+00,| Test-Loss 1.2820e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9841e-01, pde-loss 1.9367e-01       | \n",
      "| Epoch 3380,            | Training-Loss 1.0380e+01,| Test-Loss 1.2098e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2089e-01, pde-loss 2.1719e-01       | \n",
      "| Epoch 3390,            | Training-Loss 1.0503e+01,| Test-Loss 1.1543e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7464e-01, pde-loss 2.2985e-01       | \n",
      "| Epoch 3400,            | Training-Loss 1.0409e+01,| Test-Loss 1.1401e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1293e-01, pde-loss 2.1841e-01       | \n",
      "| Epoch 3410,            | Training-Loss 1.0462e+01,| Test-Loss 1.0790e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1224e-01, pde-loss 2.3485e-01       | \n",
      "| Epoch 3420,            | Training-Loss 1.0293e+01,| Test-Loss 1.2805e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0730e-01, pde-loss 2.0033e-01       | \n",
      "| Epoch 3430,            | Training-Loss 1.0474e+01,| Test-Loss 1.2341e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3790e-01, pde-loss 2.2281e-01       | \n",
      "| Epoch 3440,            | Training-Loss 1.0309e+01,| Test-Loss 1.2129e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4726e-01, pde-loss 2.1790e-01       | \n",
      "| Epoch 3450,            | Training-Loss 9.9837e+00,| Test-Loss 1.1438e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2425e-01, pde-loss 2.2516e-01       | \n",
      "| Epoch 3460,            | Training-Loss 1.0203e+01,| Test-Loss 1.1930e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1207e-01, pde-loss 2.2850e-01       | \n",
      "| Epoch 3470,            | Training-Loss 9.9333e+00,| Test-Loss 1.1910e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8109e-01, pde-loss 1.9883e-01       | \n",
      "| Epoch 3480,            | Training-Loss 1.0079e+01,| Test-Loss 1.2865e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1293e-01, pde-loss 2.2196e-01       | \n",
      "| Epoch 3490,            | Training-Loss 1.0246e+01,| Test-Loss 1.2155e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1010e-01, pde-loss 2.0124e-01       | \n",
      "| Epoch 3500,            | Training-Loss 1.0087e+01,| Test-Loss 1.1879e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7164e-01, pde-loss 2.3925e-01       | \n",
      "| Epoch 3510,            | Training-Loss 1.0348e+01,| Test-Loss 1.1592e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6953e-01, pde-loss 2.2546e-01       | \n",
      "| Epoch 3520,            | Training-Loss 1.0304e+01,| Test-Loss 1.1404e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7964e-01, pde-loss 2.3097e-01       | \n",
      "| Epoch 3530,            | Training-Loss 1.0334e+01,| Test-Loss 1.1928e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4006e-01, pde-loss 2.2025e-01       | \n",
      "| Epoch 3540,            | Training-Loss 1.0230e+01,| Test-Loss 1.2116e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0032e-01, pde-loss 2.2062e-01       | \n",
      "| Epoch 3550,            | Training-Loss 1.0291e+01,| Test-Loss 1.1407e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6043e-01, pde-loss 2.1510e-01       | \n",
      "| Epoch 3560,            | Training-Loss 1.0219e+01,| Test-Loss 1.0506e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2139e-01, pde-loss 2.4146e-01       | \n",
      "| Epoch 3570,            | Training-Loss 1.0664e+01,| Test-Loss 1.1973e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4246e-01, pde-loss 2.0979e-01       | \n",
      "| Epoch 3580,            | Training-Loss 1.0189e+01,| Test-Loss 1.1193e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9983e-01, pde-loss 2.1863e-01       | \n",
      "| Epoch 3590,            | Training-Loss 9.8998e+00,| Test-Loss 1.2056e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0048e-01, pde-loss 2.1131e-01       | \n",
      "| Epoch 3600,            | Training-Loss 1.0237e+01,| Test-Loss 1.0967e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3286e-01, pde-loss 2.4304e-01       | \n",
      "| Epoch 3610,            | Training-Loss 1.0147e+01,| Test-Loss 1.1750e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8942e-01, pde-loss 2.2772e-01       | \n",
      "| Epoch 3620,            | Training-Loss 1.0303e+01,| Test-Loss 1.1786e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 1.7913e-01, pde-loss 2.3623e-01       | \n",
      "| Epoch 3630,            | Training-Loss 1.0096e+01,| Test-Loss 1.1646e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5931e-01, pde-loss 2.1145e-01       | \n",
      "| Epoch 3640,            | Training-Loss 1.0324e+01,| Test-Loss 1.1836e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5433e-01, pde-loss 2.2970e-01       | \n",
      "| Epoch 3650,            | Training-Loss 1.0145e+01,| Test-Loss 1.1775e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8674e-01, pde-loss 2.2043e-01       | \n",
      "| Epoch 3660,            | Training-Loss 1.0416e+01,| Test-Loss 1.1515e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8421e-01, pde-loss 2.2412e-01       | \n",
      "| Epoch 3670,            | Training-Loss 1.0368e+01,| Test-Loss 1.1508e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3672e-01, pde-loss 2.2087e-01       | \n",
      "| Epoch 3680,            | Training-Loss 1.0011e+01,| Test-Loss 1.0937e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 1.5775e-01, pde-loss 2.5266e-01       | \n",
      "| Epoch 3690,            | Training-Loss 1.0283e+01,| Test-Loss 1.1620e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2625e-01, pde-loss 2.0616e-01       | \n",
      "| Epoch 3700,            | Training-Loss 1.0134e+01,| Test-Loss 1.1706e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7535e-01, pde-loss 2.3140e-01       | \n",
      "| Epoch 3710,            | Training-Loss 1.0444e+01,| Test-Loss 1.2768e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.0459e-01, pde-loss 2.2191e-01       | \n",
      "| Epoch 3720,            | Training-Loss 1.0049e+01,| Test-Loss 1.2838e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3270e-01, pde-loss 2.1575e-01       | \n",
      "| Epoch 3730,            | Training-Loss 9.9406e+00,| Test-Loss 1.1169e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.0491e-01, pde-loss 2.3934e-01       | \n",
      "| Epoch 3740,            | Training-Loss 1.0386e+01,| Test-Loss 1.0919e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2737e-01, pde-loss 2.4226e-01       | \n",
      "| Epoch 3750,            | Training-Loss 1.0019e+01,| Test-Loss 1.1976e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3447e-01, pde-loss 2.2635e-01       | \n",
      "| Epoch 3760,            | Training-Loss 1.0383e+01,| Test-Loss 1.1808e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4989e-01, pde-loss 2.1839e-01       | \n",
      "| Epoch 3770,            | Training-Loss 1.0519e+01,| Test-Loss 1.1135e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5955e-01, pde-loss 2.4362e-01       | \n",
      "| Epoch 3780,            | Training-Loss 1.0345e+01,| Test-Loss 1.1939e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2509e-01, pde-loss 2.4512e-01       | \n",
      "| Epoch 3790,            | Training-Loss 1.0400e+01,| Test-Loss 1.0829e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9470e-01, pde-loss 2.3125e-01       | \n",
      "| Epoch 3800,            | Training-Loss 1.0322e+01,| Test-Loss 1.2089e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4236e-01, pde-loss 1.9495e-01       | \n",
      "| Epoch 3810,            | Training-Loss 1.0376e+01,| Test-Loss 1.2032e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2652e-01, pde-loss 2.2851e-01       | \n",
      "| Epoch 3820,            | Training-Loss 1.0156e+01,| Test-Loss 1.1847e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8174e-01, pde-loss 2.0989e-01       | \n",
      "| Epoch 3830,            | Training-Loss 1.0285e+01,| Test-Loss 1.1360e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5351e-01, pde-loss 2.3033e-01       | \n",
      "| Epoch 3840,            | Training-Loss 1.0319e+01,| Test-Loss 1.1253e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7813e-01, pde-loss 2.3378e-01       | \n",
      "| Epoch 3850,            | Training-Loss 1.0225e+01,| Test-Loss 1.0836e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1604e-01, pde-loss 2.2843e-01       | \n",
      "| Epoch 3860,            | Training-Loss 1.0242e+01,| Test-Loss 1.1530e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3644e-01, pde-loss 2.0296e-01       | \n",
      "| Epoch 3870,            | Training-Loss 1.0506e+01,| Test-Loss 1.1392e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8551e-01, pde-loss 2.3049e-01       | \n",
      "| Epoch 3880,            | Training-Loss 1.0147e+01,| Test-Loss 1.1681e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1900e-01, pde-loss 2.2601e-01       | \n",
      "| Epoch 3890,            | Training-Loss 1.0250e+01,| Test-Loss 1.1703e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8778e-01, pde-loss 2.3172e-01       | \n",
      "| Epoch 3900,            | Training-Loss 1.0145e+01,| Test-Loss 1.1931e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6843e-01, pde-loss 2.2472e-01       | \n",
      "| Epoch 3910,            | Training-Loss 1.0420e+01,| Test-Loss 1.2433e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8826e-01, pde-loss 2.1594e-01       | \n",
      "| Epoch 3920,            | Training-Loss 1.0000e+01,| Test-Loss 1.1996e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4351e-01, pde-loss 2.1202e-01       | \n",
      "| Epoch 3930,            | Training-Loss 1.0418e+01,| Test-Loss 1.1663e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8972e-01, pde-loss 2.2837e-01       | \n",
      "| Epoch 3940,            | Training-Loss 9.8888e+00,| Test-Loss 1.1712e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.0941e-01, pde-loss 2.2655e-01       | \n",
      "| Epoch 3950,            | Training-Loss 1.0148e+01,| Test-Loss 1.1096e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3343e-01, pde-loss 2.1589e-01       | \n",
      "| Epoch 3960,            | Training-Loss 1.0671e+01,| Test-Loss 1.1835e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5626e-01, pde-loss 2.3903e-01       | \n",
      "| Epoch 3970,            | Training-Loss 1.0381e+01,| Test-Loss 1.0963e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9824e-01, pde-loss 2.2620e-01       | \n",
      "| Epoch 3980,            | Training-Loss 9.9329e+00,| Test-Loss 1.2093e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2991e-01, pde-loss 2.0233e-01       | \n",
      "| Epoch 3990,            | Training-Loss 1.0378e+01,| Test-Loss 1.1245e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4471e-01, pde-loss 2.4400e-01       | \n",
      "| Epoch 4000,            | Training-Loss 1.0485e+01,| Test-Loss 1.1601e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8461e-01, pde-loss 2.3143e-01       | \n",
      "| Epoch 4010,            | Training-Loss 1.0000e+01,| Test-Loss 1.1693e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.0642e-01, pde-loss 2.2559e-01       | \n",
      "| Epoch 4020,            | Training-Loss 1.0275e+01,| Test-Loss 1.1642e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0085e-01, pde-loss 2.1630e-01       | \n",
      "| Epoch 4030,            | Training-Loss 1.0397e+01,| Test-Loss 1.1285e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6560e-01, pde-loss 2.2390e-01       | \n",
      "| Epoch 4040,            | Training-Loss 1.0360e+01,| Test-Loss 1.1562e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3256e-01, pde-loss 2.3060e-01       | \n",
      "| Epoch 4050,            | Training-Loss 1.0430e+01,| Test-Loss 1.2933e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1675e-01, pde-loss 2.0225e-01       | \n",
      "| Epoch 4060,            | Training-Loss 1.0360e+01,| Test-Loss 1.0780e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4811e-01, pde-loss 2.4450e-01       | \n",
      "| Epoch 4070,            | Training-Loss 1.0135e+01,| Test-Loss 1.2546e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5967e-01, pde-loss 2.1519e-01       | \n",
      "| Epoch 4080,            | Training-Loss 1.0320e+01,| Test-Loss 1.2141e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 4.1301e-01, pde-loss 1.9048e-01       | \n",
      "| Epoch 4090,            | Training-Loss 1.0344e+01,| Test-Loss 1.2381e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5926e-01, pde-loss 2.2906e-01       | \n",
      "| Epoch 4100,            | Training-Loss 1.0369e+01,| Test-Loss 1.2825e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.7789e-01, pde-loss 1.8962e-01       | \n",
      "| Epoch 4110,            | Training-Loss 1.0004e+01,| Test-Loss 1.1682e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2437e-01, pde-loss 2.1589e-01       | \n",
      "| Epoch 4120,            | Training-Loss 1.0215e+01,| Test-Loss 1.1289e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1244e-01, pde-loss 2.1947e-01       | \n",
      "| Epoch 4130,            | Training-Loss 1.0056e+01,| Test-Loss 1.1531e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9940e-01, pde-loss 2.2638e-01       | \n",
      "| Epoch 4140,            | Training-Loss 1.0150e+01,| Test-Loss 1.1355e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9573e-01, pde-loss 2.3972e-01       | \n",
      "| Epoch 4150,            | Training-Loss 1.0176e+01,| Test-Loss 1.1314e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0409e-01, pde-loss 2.3187e-01       | \n",
      "| Epoch 4160,            | Training-Loss 1.0188e+01,| Test-Loss 1.1927e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6664e-01, pde-loss 2.3913e-01       | \n",
      "| Epoch 4170,            | Training-Loss 1.0447e+01,| Test-Loss 1.1514e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2452e-01, pde-loss 2.1871e-01       | \n",
      "| Epoch 4180,            | Training-Loss 1.0193e+01,| Test-Loss 1.1468e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9384e-01, pde-loss 2.2532e-01       | \n",
      "| Epoch 4190,            | Training-Loss 1.0405e+01,| Test-Loss 1.1263e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6427e-01, pde-loss 2.2647e-01       | \n",
      "| Epoch 4200,            | Training-Loss 1.0683e+01,| Test-Loss 1.0199e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9605e-01, pde-loss 2.6906e-01       | \n",
      "| Epoch 4210,            | Training-Loss 1.0326e+01,| Test-Loss 1.1493e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8054e-01, pde-loss 2.3172e-01       | \n",
      "| Epoch 4220,            | Training-Loss 1.0348e+01,| Test-Loss 1.2191e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3731e-01, pde-loss 2.0150e-01       | \n",
      "| Epoch 4230,            | Training-Loss 1.0372e+01,| Test-Loss 1.1025e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6820e-01, pde-loss 2.5499e-01       | \n",
      "| Epoch 4240,            | Training-Loss 9.7543e+00,| Test-Loss 1.0875e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8505e-01, pde-loss 2.2843e-01       | \n",
      "| Epoch 4250,            | Training-Loss 1.0224e+01,| Test-Loss 1.1827e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.8459e-01, pde-loss 2.0590e-01       | \n",
      "| Epoch 4260,            | Training-Loss 1.0223e+01,| Test-Loss 1.2175e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5106e-01, pde-loss 1.9957e-01       | \n",
      "| Epoch 4270,            | Training-Loss 1.0248e+01,| Test-Loss 1.2543e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1778e-01, pde-loss 1.9989e-01       | \n",
      "| Epoch 4280,            | Training-Loss 9.9977e+00,| Test-Loss 1.2787e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3625e-01, pde-loss 2.0028e-01       | \n",
      "| Epoch 4290,            | Training-Loss 1.0319e+01,| Test-Loss 1.1712e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7187e-01, pde-loss 2.1739e-01       | \n",
      "| Epoch 4300,            | Training-Loss 1.0437e+01,| Test-Loss 1.2073e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2513e-01, pde-loss 2.2626e-01       | \n",
      "| Epoch 4310,            | Training-Loss 1.0006e+01,| Test-Loss 1.1690e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7720e-01, pde-loss 2.1735e-01       | \n",
      "| Epoch 4320,            | Training-Loss 1.0279e+01,| Test-Loss 1.1678e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5265e-01, pde-loss 2.2433e-01       | \n",
      "| Epoch 4330,            | Training-Loss 1.0470e+01,| Test-Loss 1.0574e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6888e-01, pde-loss 2.3621e-01       | \n",
      "| Epoch 4340,            | Training-Loss 1.0132e+01,| Test-Loss 1.2245e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4673e-01, pde-loss 2.0372e-01       | \n",
      "| Epoch 4350,            | Training-Loss 1.0164e+01,| Test-Loss 1.1942e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9812e-01, pde-loss 2.1555e-01       | \n",
      "| Epoch 4360,            | Training-Loss 1.0287e+01,| Test-Loss 1.2091e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.6013e-01, pde-loss 2.0170e-01       | \n",
      "| Epoch 4370,            | Training-Loss 1.0021e+01,| Test-Loss 1.1946e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.0767e-01, pde-loss 2.1962e-01       | \n",
      "| Epoch 4380,            | Training-Loss 1.0184e+01,| Test-Loss 1.2064e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5447e-01, pde-loss 1.9133e-01       | \n",
      "| Epoch 4390,            | Training-Loss 1.0355e+01,| Test-Loss 1.2483e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2941e-01, pde-loss 2.3701e-01       | \n",
      "| Epoch 4400,            | Training-Loss 9.6954e+00,| Test-Loss 1.1756e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0546e-01, pde-loss 2.0703e-01       | \n",
      "| Epoch 4410,            | Training-Loss 1.0425e+01,| Test-Loss 1.1254e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2923e-01, pde-loss 2.4725e-01       | \n",
      "| Epoch 4420,            | Training-Loss 1.0250e+01,| Test-Loss 1.1215e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3305e-01, pde-loss 2.1702e-01       | \n",
      "| Epoch 4430,            | Training-Loss 1.0177e+01,| Test-Loss 1.2021e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3558e-01, pde-loss 1.9854e-01       | \n",
      "| Epoch 4440,            | Training-Loss 1.0291e+01,| Test-Loss 1.2223e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5122e-01, pde-loss 2.1424e-01       | \n",
      "| Epoch 4450,            | Training-Loss 1.0492e+01,| Test-Loss 1.2037e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3055e-01, pde-loss 2.2528e-01       | \n",
      "| Epoch 4460,            | Training-Loss 1.0378e+01,| Test-Loss 1.0885e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3605e-01, pde-loss 2.0754e-01       | \n",
      "| Epoch 4470,            | Training-Loss 1.0191e+01,| Test-Loss 1.2922e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0123e-01, pde-loss 2.1049e-01       | \n",
      "| Epoch 4480,            | Training-Loss 1.0172e+01,| Test-Loss 1.1156e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6028e-01, pde-loss 2.4913e-01       | \n",
      "| Epoch 4490,            | Training-Loss 1.0363e+01,| Test-Loss 1.1092e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6496e-01, pde-loss 2.3493e-01       | \n",
      "| Epoch 4500,            | Training-Loss 1.0460e+01,| Test-Loss 1.1415e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7818e-01, pde-loss 2.4426e-01       | \n",
      "| Epoch 4510,            | Training-Loss 1.0281e+01,| Test-Loss 1.1944e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0446e-01, pde-loss 2.1415e-01       | \n",
      "| Epoch 4520,            | Training-Loss 1.0470e+01,| Test-Loss 1.1262e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7474e-01, pde-loss 2.3173e-01       | \n",
      "| Epoch 4530,            | Training-Loss 1.0297e+01,| Test-Loss 1.1621e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1210e-01, pde-loss 2.2040e-01       | \n",
      "| Epoch 4540,            | Training-Loss 1.0372e+01,| Test-Loss 1.0721e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9383e-01, pde-loss 2.2186e-01       | \n",
      "| Epoch 4550,            | Training-Loss 1.0257e+01,| Test-Loss 1.1515e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2222e-01, pde-loss 2.1908e-01       | \n",
      "| Epoch 4560,            | Training-Loss 1.0273e+01,| Test-Loss 1.2047e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.6037e-01, pde-loss 2.0574e-01       | \n",
      "| Epoch 4570,            | Training-Loss 9.9798e+00,| Test-Loss 1.1638e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.4503e-01, pde-loss 2.2258e-01       | \n",
      "| Epoch 4580,            | Training-Loss 1.0115e+01,| Test-Loss 1.1785e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7536e-01, pde-loss 2.2055e-01       | \n",
      "| Epoch 4590,            | Training-Loss 1.0425e+01,| Test-Loss 1.1046e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2243e-01, pde-loss 2.2912e-01       | \n",
      "| Epoch 4600,            | Training-Loss 1.0184e+01,| Test-Loss 1.0818e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5369e-01, pde-loss 2.4212e-01       | \n",
      "| Epoch 4610,            | Training-Loss 9.9126e+00,| Test-Loss 1.2424e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8159e-01, pde-loss 2.1654e-01       | \n",
      "| Epoch 4620,            | Training-Loss 1.0177e+01,| Test-Loss 1.1398e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1234e-01, pde-loss 2.1529e-01       | \n",
      "| Epoch 4630,            | Training-Loss 1.0475e+01,| Test-Loss 1.2418e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1275e-01, pde-loss 2.0242e-01       | \n",
      "| Epoch 4640,            | Training-Loss 1.0364e+01,| Test-Loss 1.1484e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8721e-01, pde-loss 2.1589e-01       | \n",
      "| Epoch 4650,            | Training-Loss 1.0131e+01,| Test-Loss 1.1905e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7362e-01, pde-loss 2.2252e-01       | \n",
      "| Epoch 4660,            | Training-Loss 1.0034e+01,| Test-Loss 1.2676e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2692e-01, pde-loss 2.2228e-01       | \n",
      "| Epoch 4670,            | Training-Loss 1.0435e+01,| Test-Loss 1.1119e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2408e-01, pde-loss 2.3377e-01       | \n",
      "| Epoch 4680,            | Training-Loss 1.0243e+01,| Test-Loss 1.2653e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8864e-01, pde-loss 2.1060e-01       | \n",
      "| Epoch 4690,            | Training-Loss 1.0222e+01,| Test-Loss 1.2070e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0905e-01, pde-loss 2.1517e-01       | \n",
      "| Epoch 4700,            | Training-Loss 1.0270e+01,| Test-Loss 1.1358e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2282e-01, pde-loss 2.5122e-01       | \n",
      "| Epoch 4710,            | Training-Loss 1.0218e+01,| Test-Loss 1.2223e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9432e-01, pde-loss 2.1283e-01       | \n",
      "| Epoch 4720,            | Training-Loss 1.0473e+01,| Test-Loss 1.2278e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3352e-01, pde-loss 2.0912e-01       | \n",
      "| Epoch 4730,            | Training-Loss 1.0123e+01,| Test-Loss 1.1942e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.3655e-01, pde-loss 2.2315e-01       | \n",
      "| Epoch 4740,            | Training-Loss 1.0266e+01,| Test-Loss 1.1910e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0843e-01, pde-loss 2.0881e-01       | \n",
      "| Epoch 4750,            | Training-Loss 1.0423e+01,| Test-Loss 1.0805e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5255e-01, pde-loss 2.1540e-01       | \n",
      "| Epoch 4760,            | Training-Loss 1.0194e+01,| Test-Loss 1.2003e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.0834e-01, pde-loss 2.1998e-01       | \n",
      "| Epoch 4770,            | Training-Loss 1.0432e+01,| Test-Loss 1.0789e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.5260e-01, pde-loss 2.2517e-01       | \n",
      "| Epoch 4780,            | Training-Loss 1.0447e+01,| Test-Loss 1.1145e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9362e-01, pde-loss 2.3108e-01       | \n",
      "| Epoch 4790,            | Training-Loss 1.0409e+01,| Test-Loss 1.1109e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8480e-01, pde-loss 2.2277e-01       | \n",
      "| Epoch 4800,            | Training-Loss 1.0283e+01,| Test-Loss 1.1019e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.3021e-01, pde-loss 2.3055e-01       | \n",
      "| Epoch 4810,            | Training-Loss 1.0426e+01,| Test-Loss 1.2593e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 4.3688e-01, pde-loss 1.8688e-01       | \n",
      "| Epoch 4820,            | Training-Loss 1.0388e+01,| Test-Loss 1.1744e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.8003e-01, pde-loss 2.0938e-01       | \n",
      "| Epoch 4830,            | Training-Loss 1.0362e+01,| Test-Loss 1.2725e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1972e-01, pde-loss 2.0401e-01       | \n",
      "| Epoch 4840,            | Training-Loss 1.0154e+01,| Test-Loss 1.1913e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8292e-01, pde-loss 2.1497e-01       | \n",
      "| Epoch 4850,            | Training-Loss 1.0118e+01,| Test-Loss 1.2709e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1964e-01, pde-loss 2.1235e-01       | \n",
      "| Epoch 4860,            | Training-Loss 1.0365e+01,| Test-Loss 1.1807e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9583e-01, pde-loss 2.1950e-01       | \n",
      "| Epoch 4870,            | Training-Loss 1.0393e+01,| Test-Loss 1.1359e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.2550e-01, pde-loss 2.1568e-01       | \n",
      "| Epoch 4880,            | Training-Loss 1.0083e+01,| Test-Loss 1.2012e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9505e-01, pde-loss 2.0531e-01       | \n",
      "| Epoch 4890,            | Training-Loss 1.0293e+01,| Test-Loss 1.1939e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9019e-01, pde-loss 2.2469e-01       | \n",
      "| Epoch 4900,            | Training-Loss 1.0342e+01,| Test-Loss 1.2177e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8832e-01, pde-loss 2.1251e-01       | \n",
      "| Epoch 4910,            | Training-Loss 1.0389e+01,| Test-Loss 1.2170e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.5864e-01, pde-loss 2.2478e-01       | \n",
      "| Epoch 4920,            | Training-Loss 1.0170e+01,| Test-Loss 1.1463e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.1705e-01, pde-loss 2.3873e-01       | \n",
      "| Epoch 4930,            | Training-Loss 1.0037e+01,| Test-Loss 1.1805e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.7494e-01, pde-loss 1.9702e-01       | \n",
      "| Epoch 4940,            | Training-Loss 1.0117e+01,| Test-Loss 1.2500e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.1388e-01, pde-loss 1.9776e-01       | \n",
      "| Epoch 4950,            | Training-Loss 1.0075e+01,| Test-Loss 1.2584e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.8054e-01, pde-loss 2.1200e-01       | \n",
      "| Epoch 4960,            | Training-Loss 1.0152e+01,| Test-Loss 1.1702e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.4705e-01, pde-loss 1.9344e-01       | \n",
      "| Epoch 4970,            | Training-Loss 1.0128e+01,| Test-Loss 1.1920e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.9227e-01, pde-loss 2.0406e-01       | \n",
      "| Epoch 4980,            | Training-Loss 1.0135e+01,| Test-Loss 1.0639e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 2.2152e-01, pde-loss 2.4301e-01       | \n",
      "| Epoch 4990,            | Training-Loss 1.0156e+01,| Test-Loss 1.1757e+01   |\n",
      "--------------------------------------------------------------------------------\n",
      "| Data-loss 3.8208e-01, pde-loss 1.9292e-01       | \n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses, test_losses, pde_losses, data_losses = training_loop(epochs, model, loss_fn_data, optimizer,train_loader,test_loader,train_loader_pde)  # Train the model\n",
    " \n",
    "# test_losses = test_loop(epochs, model, loss_fn_data, optimizer, train_loader, test_loader)  # Test the model\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2074288/2162303466.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs).float().to(device) # Convert the inputs to a tensor\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 500.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[1;32m      2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(inputs)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Convert the inputs to a tensor\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m temp_nn \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;66;03m# Get the predictions from the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m temp_nn \u001b[38;5;241m=\u001b[39m scaler_temp\u001b[38;5;241m.\u001b[39minverse_transform(temp_nn) \u001b[38;5;66;03m# Inverse transform the predictions\u001b[39;00m\n\u001b[1;32m      6\u001b[0m temp_nn \u001b[38;5;241m=\u001b[39m temp_nn\u001b[38;5;241m.\u001b[39mreshape(num_steps\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, num_points\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# Reshape the predictions to a 2D array\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m, in \u001b[0;36mMushydata.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):                               \u001b[38;5;66;03m# This is the forward pass\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, t], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)          \u001b[38;5;66;03m# Concatenate the input features\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m                                 \u001b[38;5;66;03m# Pass through the third layer\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 500.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "torch.cuda.synchronize()\n",
    "inputs = torch.tensor(inputs).float().to(device) # Convert the inputs to a tensor\n",
    "temp_nn = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).cpu().detach().numpy() # Get the predictions from the model\n",
    "\n",
    "temp_nn = scaler_temp.inverse_transform(temp_nn) # Inverse transform the predictions\n",
    "temp_nn = temp_nn.reshape(num_steps+1, num_points+2) # Reshape the predictions to a 2D array\n",
    "time_ss= np.linspace(0, time_end, num_steps+1 )\n",
    "\n",
    "plt.figure\n",
    "plt.plot(time_ss, temp_nn[:, num_points//2], label='Predicted Temperature')\n",
    "plt.plot(time_ss, temperature_history[:,num_points//2], label='Actual Temperature')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.yscale('linear')\n",
    "plt.title('Predicted vs Actual Temperature at x = 7.5mm')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(test_losses, label='test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(data_losses, label='Data Loss')\n",
    "plt.plot(pde_losses, label='Pde Loss')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_coord, time_coord = np.meshgrid(np.arange(t_hist.shape[1]), np.arange(t_hist.shape[0]))\n",
    "\n",
    "time_coord = time_coord * dt \n",
    "# Create a figure with two subplots\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "\n",
    "# Plot the temperature history on the left subplot\n",
    "im1 = ax1.pcolormesh(space_coord, time_coord, t_hist, cmap='viridis', shading='auto')\n",
    "ax1.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "ax1.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "ax1.set_title('Temperature Variation Over Time(Analytical Model)',fontname='Times New Roman', fontsize=20)\n",
    "ax1.contour(space_coord, time_coord, t_hist, colors='red', linewidths=1.0, alpha=0.9)\n",
    "\n",
    "ax1.grid(True)\n",
    "cbar = fig.colorbar(im1, ax=ax1)\n",
    "cbar.ax.invert_yaxis()\n",
    "cbar.set_label('Temperature (K)', rotation=270, labelpad=20, fontname='Times New Roman', fontsize=16)\n",
    "\n",
    "im2 = ax2.pcolormesh(space_coord, time_coord, temp_nn, cmap='viridis', shading='auto')\n",
    "ax2.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "ax2.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "ax2.set_title('Temperature Variation Over Time(PINN model)',fontname='Times New Roman', fontsize=20)\n",
    "ax2.contour(space_coord, time_coord, temp_nn, colors='red', linewidths=1.0, alpha=0.9)\n",
    "\n",
    "ax2.grid(True)\n",
    "cbar = fig.colorbar(im2, ax=ax2)\n",
    "cbar.ax.invert_yaxis()\n",
    "cbar.set_label('Temperature (K)', rotation=270, labelpad=20, fontname='Times New Roman', fontsize=16)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
