{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Three Phase Simulation of Alloys and PINN model development \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the simulation of 1D Phase change of aluminium alloy. There will be three phases (solid,liquid and mushy).   \n",
    "\n",
    "The approach used is finite difference method and the physics involved in heat conduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import csv\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "\n",
    "from pinn_loss import loss_fn_data, l1_regularization, pde_loss, boundary_loss, ic_loss, accuracy\n",
    "from Input_vec_gen import input_gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the constants and inital geometric domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_l = 3.394878564540885e-05, alpha_s = 3.686205086349929e-05, m_eff = 6.296953764744878e-06\n",
      "dx is 0.0003061224489795918\n",
      "dt is  0.0012711033647622566\n",
      "num_steps is 31469\n",
      "cfl is 0.0012711033647622566\n",
      "stability criteria satisfied\n"
     ]
    }
   ],
   "source": [
    "# Geometry\n",
    "length = 15.0e-3             # Length of the rod\n",
    "\n",
    "# Material properties\n",
    "rho = 2300.0                     # Density of AL380 (kg/m^3)\n",
    "rho_l = 2460.0                   # Density of AL380 (kg/m^3)\n",
    "rho_s = 2710.0                    # Density of AL380 (kg/m^3)\n",
    "rho_m = (rho_l + rho_s )/2       # Desnity in mushy zone is taken as average of liquid and solid density\n",
    "\n",
    "k = 104.0                       # W/m-K\n",
    "k_l = k                       # W/m-K\n",
    "k_s = 96.2                    # W/m-K\n",
    "k_m =  (k_l+k_s)/2                     # W/m-K\n",
    "k_mo = 41.5\n",
    "\n",
    "\n",
    "cp = 1245.3                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_l = cp                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_s = 963.0                 # Specific heat of aluminum (J/kg-K)\n",
    "cp_m =  (cp_l+cp_s)/2                 # Specific heat of mushy zone is taken as average of liquid and solid specific heat\n",
    "# cp_m = cp\n",
    "           # Thermal diffusivity\n",
    "alpha_l = k_l / (rho_l * cp_l) \n",
    "alpha_s = k_s / (rho_s*cp_s)\n",
    "alpha_m = k_m / (rho_m * cp_m)          #`Thermal diffusivity in mushy zone is taken as average of liquid and solid thermal diffusivity`\n",
    "\n",
    "\n",
    "#L_fusion = 3.9e3                 # J/kg\n",
    "L_fusion = 389.0e3               # J/kg  # Latent heat of fusion of aluminum\n",
    "         # Thermal diffusivity\n",
    "\n",
    "\n",
    "T_L = 574.4 +273.0                       #  K -Liquidus Temperature (615 c) AL 380\n",
    "T_S = 497.3 +273.0                     # K- Solidus Temperature (550 C)\n",
    "m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "print (f\"alpha_l = {alpha_l}, alpha_s = {alpha_s}, m_eff = {m_eff}\")\n",
    "\n",
    "# htc = 10.0                   # W/m^2-K\n",
    "# q = htc*(919.0-723.0)\n",
    "# q = 10000.0\n",
    "\n",
    "\n",
    "num_points = 50                        # Number of spatial points\n",
    "dx = length / (num_points - 1)         # Distance between two spatial points\n",
    "print('dx is',dx)\n",
    "\n",
    "                                                              \n",
    "# Time Discretization  \n",
    "# \n",
    "time_end = 40        # seconds                         \n",
    "\n",
    "maxi = max(alpha_s,alpha_l,alpha_m)\n",
    "dt = abs(0.5*((dx**2) /maxi)) \n",
    "\n",
    "print('dt is ',dt)\n",
    "num_steps = round(time_end/dt)\n",
    "print('num_steps is',num_steps)\n",
    "cfl = 0.5 *(dx**2/max(alpha_l,alpha_s,alpha_m))\n",
    "print('cfl is',cfl)\n",
    "\n",
    "time_steps = np.linspace(0, time_end, num_steps + 1)\n",
    "step_coeff = dt / (dx ** 2)\n",
    "\n",
    "if dt <= cfl:\n",
    "    print('stability criteria satisfied')\n",
    "else:\n",
    "    print('stability criteria not satisfied')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial and Boundary Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_init = 919.0\n",
    "# Initial temperature and phase fields\n",
    "temperature = np.full(num_points+2, 919.0)            # Initial temperature of the rod with ghost points at both ends\n",
    "phase = np.zeros(num_points+2)*0.0                    # Initial phase of the rod with ghost points at both ends\n",
    "\n",
    "# Set boundary conditions\n",
    "# temperature[-1] = 919.0 \n",
    "phase[-1] = 1.0\n",
    "\n",
    "# temperature[0] = 919.0 #(40 C)\n",
    "phase[0] = 1.0\n",
    "\n",
    "# Store initial state in history\n",
    "temperature_history = [temperature.copy()]    # List to store temperature at each time step\n",
    "phi_history = [phase.copy()]                    # List to store phase at each time step\n",
    "temp_init = temperature.copy()                 # Initial temperature of the rod\n",
    "# print(temperature_history,phi_history)\n",
    "# Array to store temperature at midpoint over time\n",
    "midpoint_index = num_points // 2                          # Index of the midpoint\n",
    "\n",
    "midpoint_temperature_history = [temperature[midpoint_index]]            # List to store temperature at midpoint over time\n",
    "dm = 60.0e-3                                                            # die thickness in m\n",
    "\n",
    "# r_m =  (k_mo / dm) + (1/htc)\n",
    "\n",
    "t_surr = 500.0                                        # Surrounding temperature in K\n",
    "# t_surr = h()\n",
    "\n",
    "def kramp(temp,v1,v2,T_L,T_s):                                      # Function to calculate thermal conductivity in Mushy Zone\n",
    "        slope = (v1-v2)/(T_L-T_S)\n",
    "        if temp > T_L:\n",
    "            k_m = k_l\n",
    "        elif temp < T_S:\n",
    "            k_m = k_s\n",
    "        else:\n",
    "            k_m = k_s + slope*(temp-T_S)\n",
    "        return k_m\n",
    "\n",
    "def cp_ramp(temp,v1,v2,T_L,T_s):                                    # Function to calculate specific heat capacity in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        cp_m = cp_l\n",
    "    elif temp < T_S:\n",
    "        cp_m = cp_s\n",
    "    else:\n",
    "        cp_m = cp_s + slope*(temp-T_S)\n",
    "    return cp_m\n",
    "\n",
    "def rho_ramp(temp,v1,v2,T_L,T_s):                                       # Function to calculate density in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        rho_m = rho_l\n",
    "    elif temp < T_S:\n",
    "        rho_m = rho_s\n",
    "    else:\n",
    "        rho_m = rho_s + slope*(temp-T_S)\n",
    "    return rho_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the HT equation and phase change numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for m in range(1, num_steps+1):                                                                            # time loop\n",
    "    htc = 10.0                   # htc of Still air in W/m^2-K\n",
    "    q1 = htc*(temp_init[0]-t_surr)   # Heat flux at the left boundary\n",
    "    \n",
    "    # print(f\"q1 is {q1}\")\n",
    "    temperature[0] = temp_init[0] + alpha_l * step_coeff * ((2.0*temp_init[1]) - (2.0 * temp_init[0])-(2.0*dx*(q1)))  # Update boundary condition temperature\n",
    "    \n",
    "    q2 = htc*(temp_init[-1]-t_surr)                   # Heat flux at the right boundary\n",
    "    temperature[-1] = temp_init[-1] + alpha_l * step_coeff * ((2.0*temp_init[-2]) - (2.0 * temp_init[-1])-(2.0*dx*(q2)))  # Update boundary condition temperature\n",
    "    \n",
    "    for n in range(1,num_points+1):              # space loop, adjusted range\n",
    "       \n",
    "        if temperature[n] >= T_L:\n",
    "            temperature[n] += ((alpha_l * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            phase[n] = 0\n",
    "            \n",
    "            # print(f\" Time-Step{m},Spatial point{n},Temperature{temperature[n]}\")\n",
    "        elif T_S < temperature[n] < T_L:\n",
    "            \n",
    "            k_m = kramp(temperature[n],k_l,k_s,T_L,T_S)\n",
    "            cp_m = cp_ramp(temperature[n],cp_l,cp_s,T_L,T_S)\n",
    "            rho_m = rho_ramp(temperature[n],rho_l,rho_s,T_L,T_S)\n",
    "            m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "            \n",
    "            temperature[n] += ((m_eff * step_coeff)* (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            \n",
    "            phase[n] = (T_L - temperature[n]) / (T_L - T_S)\n",
    "            # print(m,n,temperature[n],phase[n])\n",
    "         \n",
    "        elif temperature[n]<T_S:\n",
    "            temperature[n] += ((alpha_s * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n])+ temp_init[n-1]))\n",
    "            phase[n] = 1\n",
    "                     \n",
    "        else:\n",
    "            print(\"ERROR: should not be here\")\n",
    "\n",
    "     \n",
    "          \n",
    "    temperature = temperature.copy()                                                                # Update temperature\n",
    "    phase = phase.copy()                                                                            # Update phase\n",
    "    temp_init = temperature.copy()                                                                  # Update last time step temperature\n",
    "    temperature_history.append(temperature.copy())                                                  # Append the temperature history to add ghost points\n",
    "    phi_history.append(phase.copy())                                                                # Append the phase history to add ghost points\n",
    "    midpoint_temperature_history.append(temperature[midpoint_index])                                # Store midpoint temperature\n",
    "    \n",
    "    \n",
    "    # print(f\"Step {m}, Temperature: {temperature}\")\n",
    "    \n",
    "\n",
    "\n",
    "# print(midpoint_temperature_history)\n",
    "#print(phi_history)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot temperature history for debugging\n",
    "# temperature_history_1 = np.array(temperature_history)\n",
    "# print(temperature_history_1.shape)\n",
    "# time_ss= np.linspace(0, time_end, num_steps+1)\n",
    "# # print(time_ss.shape)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(time_ss, midpoint_temperature_history, label='Midpoint Temperature')\n",
    "# plt.axhline(y=T_L, color='r', linestyle='--', label='Liquidus Temperature')\n",
    "# plt.axhline(y=T_S, color='g', linestyle='--', label='Solidus Temperature')\n",
    "# plt.xlabel('Time(s)')\n",
    "# plt.ylabel('Temperature (K)')\n",
    "# plt.title('Temperature Distribution Over Time at x = 7.5mm') \n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data into Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_history = np.array(temperature_history)\n",
    "\n",
    "phi_history = np.array(phi_history)\n",
    "\n",
    "t_hist = np.array(temperature_history[:,1:-1])\n",
    "p_hist = np.array(phi_history[:,1:-1])\n",
    "\n",
    "t_hist_init = t_hist[0,:]\n",
    "t_hist_bc_l = t_hist[:,0]\n",
    "t_hist_bc_r = t_hist[:,-1]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have temperature_history and phi_history as lists of arrays\n",
    "\n",
    "\n",
    "# # Check the new shape after transposing\n",
    "# print(\"Transposed Temperature History Shape:\", temperature_history.shape)\n",
    "# print(\"Transposed Phi History Shape:\", phi_history.shape)\n",
    "\n",
    "# # Create a meshgrid for space and time coordinates\n",
    "# space_coord, time_coord = np.meshgrid(np.arange(temperature_history.shape[1]), np.arange(temperature_history.shape[0]))\n",
    "\n",
    "# time_coord = time_coord * dt \n",
    "# # Create a figure with two subplots\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# # Plot the temperature history on the left subplot\n",
    "# im1 = ax1.pcolormesh(space_coord, time_coord, temperature_history, cmap='viridis')\n",
    "# ax1.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_title('Temperature Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im1, ax=ax1, label='Temperature')\n",
    "\n",
    "# # Plot the phase history on the right subplot\n",
    "# im2 = ax2.pcolormesh(space_coord, time_coord, phi_history, cmap='viridis')\n",
    "# ax2.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=18)\n",
    "# ax2.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax2.set_title('Phase Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im2, ax=ax2, label='Phase')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #plot the main\n",
    "# fig, ax = plt.subplots(figsize=(14, 6))\n",
    "# im = ax.pcolormesh(space_coord, time_coord, Dim_ny, cmap='viridis')\n",
    "# ax.set_xlabel('Space Coordinate')\n",
    "# ax.set_ylabel('Time')\n",
    "# ax.set_title('Niyama Variation Over Time')\n",
    "# fig.colorbar(im, ax=ax, label='Main')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU/CPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check for gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,) (31470,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "space = np.linspace(0, length, num_points) # Spatial points\n",
    "time = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "print(space.shape,time.shape)\n",
    "\n",
    "sp_i = np.linspace(0, length, num_points) # Spatial points\n",
    "time_i = np.zeros(num_points) # Time points\n",
    "\n",
    "sp_b_l = np.zeros(num_steps+1) # Spatial points\n",
    "time_b_l = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "\n",
    "sp_b_r = np.ones(num_steps+1)*length # Spatial points\n",
    "time_b_r = np.linspace(0, time_end, num_steps+1) # Time points\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = input_gen(space,time,'mgrid')\n",
    "inputs_i = input_gen(sp_i,time_i,'scr')\n",
    "inputs_b_l = input_gen(sp_b_l,time_b_l,'scr')\n",
    "inputs_b_r = input_gen(sp_b_r,time_b_r,'scr')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1573500, 2) (50, 2) (31470, 2) (31470, 2)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape,inputs_i.shape,inputs_b_l.shape,inputs_b_r.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Convert the inputs to a tensor\u001b[39;00m\n\u001b[1;32m      4\u001b[0m inputs_init \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(inputs_i)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Convert the inputs to a tensor\u001b[39;00m\n\u001b[1;32m      5\u001b[0m inputs_b_l \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(inputs_b_l)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# Convert the inputs to a tensor\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "inputs = torch.tensor(inputs).float().to(device) # Convert the inputs to a tensor\n",
    "\n",
    "\n",
    "inputs_init = torch.tensor(inputs_i).float().to(device) # Convert the inputs to a tensor\n",
    "inputs_b_l = torch.tensor(inputs_b_l).float().to(device) # Convert the inputs to a tensor\n",
    "inputs_b_r = torch.tensor(inputs_b_r).float().to(device) # Convert the inputs to a tensor\n",
    "\n",
    "print(inputs_init.shape)\n",
    "# label/temp data\n",
    "temp_tr = torch.tensor(t_hist).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp = temp_tr.reshape(-1,1) # Reshape the temperature tensor to a column vector\n",
    "temp_inp_init = torch.tensor(t_hist_init).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp_bc_l = torch.tensor(t_hist_bc_l).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp_bc_r = torch.tensor(t_hist_bc_r).float().to(device) # Convert the temperature history to a tensor\n",
    "print(temp_inp.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Data Splitting\n",
    "\n",
    "# train_inputs, val_test_inputs, train_temp_inp, val_test_temp_inp = train_test_split(inputs, temp_inp, test_size=0.2, random_state=42)\n",
    "# val_inputs, test_inputs, val_temp_inp, test_temp_inp = train_test_split(val_test_inputs, val_test_temp_inp, test_size=0.8, random_state=42)\n",
    "\n",
    "train_inputs, test_inputs, train_temp_inp, test_temp_inp = train_test_split(inputs, temp_inp, test_size=0.2, random_state=42)\n",
    "train_inputs_init, test_inputs_init, train_temp_inp_init, test_temp_inp_init = train_test_split(inputs_init, temp_inp_init, test_size=0.2, random_state=42)\n",
    "train_inputs_bc_l, test_inputs_bc_l, train_temp_inp_bc_l, test_temp_inp_bc_l = train_test_split(inputs_b_l, temp_inp_bc_l, test_size=0.2, random_state=42)\n",
    "train_inputs_bc_r, test_inputs_bc_r, train_temp_inp_bc_r, test_temp_inp_bc_r = train_test_split(inputs_b_r, temp_inp_bc_r, test_size=0.2, random_state=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, temp_inp,transform=None, target_transform =None):\n",
    "        self.inputs = inputs\n",
    "        self.temp_inp = temp_inp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.temp_inp[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "  \n",
    "train_dataset = TensorDataset(train_inputs, train_temp_inp) # Create the training dataset\n",
    "# val_dataset = TensorDataset(val_inputs, val_temp_inp) # Create the validation dataset\n",
    "test_dataset = TensorDataset(test_inputs, test_temp_inp) # Create the test dataset\n",
    "\n",
    "train_dataset_init = TensorDataset(train_inputs_init, train_temp_inp_init) # Create the training dataset\n",
    "test_dataset_init = TensorDataset(test_inputs_init, test_temp_inp_init) # Create the test dataset\n",
    "train_dataset_bc_l = TensorDataset(train_inputs_bc_l, train_temp_inp_bc_l) # Create the training dataset\n",
    "test_dataset_bc_l = TensorDataset(test_inputs_bc_l, test_temp_inp_bc_l) # Create the test dataset\n",
    "train_dataset_bc_r = TensorDataset(train_inputs_bc_r, train_temp_inp_bc_r) # Create the training dataset\n",
    "test_dataset_bc_r = TensorDataset(test_inputs_bc_r, test_temp_inp_bc_r) # Create the test dataset\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "random_sampler_train = RandomSampler(train_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "# random_sampler_val = RandomSampler(val_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the validation dataset\n",
    "random_sampler_test = RandomSampler(test_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "\n",
    "random_sampler_train_init = RandomSampler(train_dataset_init, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "random_sampler_test_init = RandomSampler(test_dataset_init, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "random_sampler_train_bc_l = RandomSampler(train_dataset_bc_l, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "random_sampler_test_bc_l = RandomSampler(test_dataset_bc_l, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "random_sampler_train_bc_r = RandomSampler(train_dataset_bc_r, replacement=True, num_samples=batch_size) # Create a random sampler for the training dataset\n",
    "random_sampler_test_bc_r = RandomSampler(test_dataset_bc_r, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=random_sampler_train) # Create the training dataloader\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=random_sampler_val) # Create the validation dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=random_sampler_test) # Create the test dataloader\n",
    "\n",
    "train_loader_init = DataLoader(train_dataset_init, batch_size=batch_size, sampler=random_sampler_train_init) # Create the training dataloader\n",
    "test_loader_init = DataLoader(test_dataset_init, batch_size=batch_size, sampler=random_sampler_test_init) # Create the test dataloader\n",
    "train_loader_bc_l = DataLoader(train_dataset_bc_l, batch_size=batch_size, sampler=random_sampler_train_bc_l) # Create the training dataloader\n",
    "test_loader_bc_l = DataLoader(test_dataset_bc_l, batch_size=batch_size, sampler=random_sampler_test_bc_l) # Create the test dataloader\n",
    "train_loader_bc_r = DataLoader(train_dataset_bc_r, batch_size=batch_size, sampler=random_sampler_train_bc_r) # Create the training dataloader\n",
    "test_loader_bc_r = DataLoader(test_dataset_bc_r, batch_size=batch_size, sampler=random_sampler_test_bc_r) # Create the test dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural network architecture\n",
    "class Mushydata(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size): # This is the constructor\n",
    "        super(Mushydata, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            # nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):                               # This is the forward pass\n",
    "        input_features = torch.cat([x, t], dim=1)          # Concatenate the input features\n",
    "        m = self.base(input_features)                                 # Pass through the third layer\n",
    "        return m                    # Return the output of the network\n",
    "\n",
    "\n",
    "# features = torch.rand(1, 2)\n",
    "# model = HeatPINN(2, 20, 1)\n",
    "# output = model(features[:, 0:1], features[:, 1:2])\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# Loss function for data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 40\n",
    "learning_rate = 0.004\n",
    "epochs = 30000\n",
    "# alpha = 0.01  # Adjust this value based on your problem\n",
    "# boundary_value = 313.0\n",
    "# initial_value = init_temp\n",
    "# Initialize the model\n",
    "model = Mushydata(input_size=2, hidden_size=hidden_size,output_size=1).to(device)\n",
    "lambd = 0.1\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss List Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of train_loader is <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(f\"Datatype of train_loader is {type(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn_data(u_pred, u_true):\n",
    "#     return nn.MSELoss()(u_pred, u_true)\n",
    "\n",
    "# def l1_regularization(model, lambd):\n",
    "#     l1_reg = sum(param.abs().sum() for param in model.parameters())\n",
    "#     return l1_reg * lambd\n",
    "\n",
    "# def pde_loss(u_pred,x,t):\n",
    "#     # u_pred.requires_grad = True\n",
    "#     x.requires_grad = True\n",
    "#     t.requires_grad = True\n",
    "    \n",
    "#     u_pred = model(x,t).requires_grad_()\n",
    "#     u_t = torch.autograd.grad(u_pred, t, \n",
    "#                                 torch.ones_like(u_pred).to(device),\n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused=True,\n",
    "#                                 )[0] # Calculate the first time derivative\n",
    "#     if u_t is None:\n",
    "#         raise RuntimeError(\"u_t is None\")\n",
    "\n",
    "#     u_x = torch.autograd.grad(u_pred, \n",
    "#                                 x, \n",
    "#                                 torch.ones_like(u_pred).to(device), \n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused =True)[0] # Calculate the first space derivative\n",
    "            \n",
    "#     u_xx = torch.autograd.grad(u_x, \n",
    "#                                 x, \n",
    "#                                 torch.ones_like(u_x).to(device), \n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused=True)[0] \n",
    "    \n",
    "#     T_S_tensor = torch.tensor(T_S, device=device)\n",
    "#     T_L_tensor = torch.tensor(T_L, device=device)\n",
    "    \n",
    "#     k_m = torch.where((u_pred >= T_S_tensor) * (u_pred <= T_L_tensor),\\\n",
    "#                        kramp(u_pred, k_l,k_s,T_L,T_S),torch.tensor(0.0,device=device))\n",
    "#     cp_m = torch.where(u_pred >= T_S_tensor * u_pred <= T_L_tensor, cp_ramp((u_pred), cp_l,cp_s,T_L,T_S))\n",
    "#     rho_m = torch.where(u_pred >= T_S_tensor * u_pred <= T_L_tensor, rho_ramp((u_pred), rho_l,rho_s,T_L,T_S))\n",
    "#     m_eff = (k_m / (rho_m * (cp_m + (L_fusion / (T_L - T_S)))))\n",
    "\n",
    "#     alpha_T = torch.where(u_pred >= T_L_tensor, alpha_l, torch.where(u_pred<=T_S_tensor,alpha_s ,m_eff))\n",
    "#     alpha_T = 1\n",
    "#     residual = u_t - alpha_T * u_xx\n",
    "\n",
    "#     return nn.MSELoss()(residual,torch.zeros_like(residual))\n",
    "\n",
    "# def boundary_loss(u_pred,x,t,t_surr):\n",
    "    \n",
    "#     u_x = torch.autograd.grad(u_pred,x, \n",
    "#                                 torch.ones_like(u_pred).to(device), \n",
    "#                                 create_graph=True,\n",
    "#                                 allow_unused =True)[0] # Calculate the first space derivative\n",
    "#     t_surr_t = torch.tensor(t_surr, device=device)\n",
    "#     res_l = u_x -(htc* (u_pred-t_surr_t))\n",
    "   \n",
    "\n",
    "#     return nn.MSELoss()(res_l,torch.zeros_like(res_l))\n",
    "\n",
    "# def ic_loss(u_pred):\n",
    "#     temp_init_tsr = torch.tensor(temp_init[1:-1],device=device)\n",
    "#     ic = u_pred -temp_init_tsr\n",
    "#     return nn.MSELoss()(ic,torch.zeros_like(ic))\n",
    "\n",
    "def accuracy(u_pred, u_true):\n",
    "    return torch.mean(torch.abs(u_pred - u_true) / u_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Testing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, model, loss_fn_data, optimizer, train_dataloader,):\n",
    "    train_losses = []  # Initialize the list to store the training losses\n",
    "    val_losses = []    # Initialize the list to store the validation losses\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                                                           # Set the model to training mode\n",
    "        train_loss = 0                                                                              # Initialize the training loss\n",
    "        train_accuracy = 0\n",
    "        for (batch,batch_init,batch_left,batch_right) in \\\n",
    "             zip (train_dataloader,train_loader_init,train_loader_bc_l,train_inputs_bc_r):                                                          # Loop through the training dataloader\n",
    "            inputs, temp_inp= batch                                                             # Get the inputs and the true values\n",
    "            inputs_init, temp_inp_init= batch_init                                                             # Get the inputs and the true values \n",
    "            inputs_left, temp_inp_left= batch_left                                                             # Get the inputs and the true values\n",
    "            inputs_right, temp_inp_right= batch_right                                                             # Get the inputs and the true values\n",
    "\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device)                             # Move the inputs and true values to the GPU\n",
    "            inputs_init, temp_inp_init= inputs_init.to(device), temp_inp_init.to(device)                             # Move the initial condition inputs and temperature to the GPU\n",
    "            inputs_left, temp_inp_left= inputs_left.to(device), temp_inp_left.to(device)                             # Move the left boundary condition inputs and temperature values to the GPU\n",
    "            inputs_right, temp_inp_right= inputs_right.to(device), temp_inp_right.to(device)                             # Move the right boundary condition inputs and temperature values to the GPU\n",
    "\n",
    "            optimizer.zero_grad()                                                                    # Zero the gradients\n",
    "            \n",
    "            # Forward pass\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).to(device)                       # Get the predictions\n",
    "            u_initl = model(inputs_init[:,0].unsqueeze(1), inputs_init[:,1].unsqueeze(1)).to(device)                       # Get the predictions\n",
    "            \n",
    "            u_left = model(inputs_b_l[:,0].unsqueeze(1), inputs_b_l[:,1].unsqueeze(1)).to(device)               # Left boundary of the temperature\n",
    "            u_right = model(inputs_b_r[:,0].unsqueeze(1), inputs_b_r[:,1].unsqueeze(1)).to(device)             # Right boundary of the temperature\n",
    "\n",
    "            # Loss calculation\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp)                                              # Calculate the data loss\n",
    "            \n",
    "            pd_loss = pde_loss(model,inputs[:,0].unsqueeze(1),inputs[:,1].unsqueeze(1))             # Calculate the PDE loss\n",
    "            # pd_loss = 0\n",
    "            \n",
    "            initc_loss = ic_loss(u_initl) \n",
    "            # initc_loss =0                                                      # Calculate initial condition loss\n",
    "            \n",
    "            bc_loss_left = boundary_loss(model,inputs_b_l[:,0].unsqueeze(1),inputs_b_l[:,1].unsqueeze(1),t_surr) # Calculate the left boundary condition loss\n",
    "            bc_loss_right = boundary_loss(model,inputs_b_r[:,0].unsqueeze(1),inputs_b_r[:,1].unsqueeze(1),t_surr) # Calculate the right boundary condition loss\n",
    "            bc_loss = bc_loss_left + bc_loss_right\n",
    "            # l1_regularization_loss = l1_regularization(model, lambda_l1)                      # Calculate the L1 regularization loss\n",
    "            # loss = data_loss  + pd_loss + initc_loss + bc_loss                                              # Calculate the total loss\n",
    "            w1 = 0.01\n",
    "            w2 = 0.01\n",
    "            w3 = 0.01\n",
    "            loss = data_loss + w1* pd_loss + w2 *initc_loss + w3* bc_loss\n",
    "            train_accuracy += accuracy(u_pred, temp_inp)                                                              # Calculate the total loss\n",
    "            # Backpropagation\n",
    "            loss.backward(retain_graph=True)                                                        # Backpropagate the gradients\n",
    "            \n",
    "            optimizer.step()                                                                           # Update the weights\n",
    "            \n",
    "            train_loss += loss.item()                                                           # Add the loss to the training set loss                 \n",
    "\n",
    "        \n",
    "\n",
    "        # model.eval()\n",
    "        # test_loss = 0\n",
    "        # test_accuracy = 0\n",
    "        # with torch.no_grad():   \n",
    "        #     for batch in test_dataloader:\n",
    "        #         inputs, temp_inp= batch\n",
    "        #         inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "        #         u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "        #         data_loss = loss_fn_data(u_pred, temp_inp)\n",
    "        #         # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "        #         # loss = data_loss  + l1_regularization_loss\n",
    "        #         loss = data_loss\n",
    "        #         test_accuracy = accuracy(u_pred, temp_inp)\n",
    "        #         test_loss += loss.item()\n",
    "        #     test_losses.append(test_loss)\n",
    "\n",
    "        train_losses.append(train_loss)                                                   # Append the training loss to the list of training losses\n",
    "        \n",
    "        # if epoch % 10 == 0:\n",
    "        #     print(f\"Epoch {epoch}, Training-Loss {train_loss:.4e}\")\n",
    "        \n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training-Loss {train_loss:.4e}, Data-loss {data_loss:.4e}\\\n",
    "                  , pde-loss {pd_loss:.4e}, initc-loss {initc_loss:.4e}\\\n",
    "                    bc_loss {bc_loss:.4e}\") \n",
    "\n",
    "    return train_losses, val_losses                                                             # Return the training and validation losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(epochs, model, loss_fn_data, optimizer, train_dataloader, test_dataloader):\n",
    "      \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    with torch.no_grad():   \n",
    "        for batch in test_dataloader:\n",
    "            inputs, temp_inp= batch\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp)\n",
    "            # l1_regularization_loss = l1_regularization(model, lambd)\n",
    "            # loss = data_loss  + l1_regularization_loss\n",
    "            loss = data_loss\n",
    "            test_accuracy = accuracy(u_pred, temp_inp)\n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss)\n",
    "    if epochs % 10 == 0:\n",
    "        print(f\"Epoch {epochs}, Test-Loss {test_loss:.4e}, Test-Accuracy {test_accuracy:.4e}\")      \n",
    "    return test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Button "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training-Loss 1.1530e+06, Data-loss 6.4418e+05                  , pde-loss 6.8376e-04, initc-loss 8.4485e+05                    bc_loss 5.0033e+07\n",
      "Epoch 10, Training-Loss 1.1569e+06, Data-loss 6.5024e+05                  , pde-loss 1.1475e-01, initc-loss 8.4292e+05                    bc_loss 4.9827e+07\n",
      "Epoch 20, Training-Loss 1.1493e+06, Data-loss 6.5129e+05                  , pde-loss 3.6416e+00, initc-loss 8.3536e+05                    bc_loss 4.8961e+07\n",
      "Epoch 30, Training-Loss 1.0856e+06, Data-loss 6.1513e+05                  , pde-loss 5.7624e+01, initc-loss 8.0974e+05                    bc_loss 4.6241e+07\n",
      "Epoch 40, Training-Loss 9.6300e+05, Data-loss 5.6076e+05                  , pde-loss 4.2640e+02, initc-loss 7.4554e+05                    bc_loss 3.9478e+07\n",
      "Epoch 50, Training-Loss 6.7971e+05, Data-loss 4.1115e+05                  , pde-loss 2.4621e+03, initc-loss 6.1294e+05                    bc_loss 2.6241e+07\n",
      "Epoch 60, Training-Loss 2.9964e+05, Data-loss 2.0887e+05                  , pde-loss 1.2325e+04, initc-loss 3.9071e+05                    bc_loss 8.6746e+06\n",
      "Epoch 70, Training-Loss 7.8259e+04, Data-loss 4.2449e+04                  , pde-loss 4.1889e+04, initc-loss 1.4419e+05                    bc_loss 3.3949e+06\n",
      "Epoch 80, Training-Loss 1.0107e+05, Data-loss 2.3388e+04                  , pde-loss 4.5562e+04, initc-loss 8.4535e+04                    bc_loss 7.6381e+06\n",
      "Epoch 90, Training-Loss 6.1183e+04, Data-loss 3.3490e+04                  , pde-loss 2.9725e+04, initc-loss 1.2730e+05                    bc_loss 2.6123e+06\n",
      "Epoch 100, Training-Loss 5.7762e+04, Data-loss 3.9994e+04                  , pde-loss 2.0305e+04, initc-loss 1.3543e+05                    bc_loss 1.6211e+06\n",
      "Epoch 110, Training-Loss 5.3060e+04, Data-loss 3.2141e+04                  , pde-loss 1.7130e+04, initc-loss 1.1155e+05                    bc_loss 1.9632e+06\n",
      "Epoch 120, Training-Loss 4.9042e+04, Data-loss 2.7804e+04                  , pde-loss 1.5189e+04, initc-loss 8.3298e+04                    bc_loss 2.0254e+06\n",
      "Epoch 130, Training-Loss 4.6441e+04, Data-loss 3.0450e+04                  , pde-loss 1.1707e+04, initc-loss 9.5219e+04                    bc_loss 1.4921e+06\n",
      "Epoch 140, Training-Loss 4.6635e+04, Data-loss 3.3551e+04                  , pde-loss 8.9728e+03, initc-loss 9.9257e+04                    bc_loss 1.2002e+06\n",
      "Epoch 150, Training-Loss 4.6344e+04, Data-loss 3.3076e+04                  , pde-loss 8.9314e+03, initc-loss 8.1338e+04                    bc_loss 1.2366e+06\n",
      "Epoch 160, Training-Loss 4.7463e+04, Data-loss 3.4635e+04                  , pde-loss 9.0344e+03, initc-loss 7.5763e+04                    bc_loss 1.1979e+06\n",
      "Epoch 170, Training-Loss 3.5495e+04, Data-loss 2.4573e+04                  , pde-loss 6.3353e+03, initc-loss 8.2825e+04                    bc_loss 1.0030e+06\n",
      "Epoch 180, Training-Loss 3.7885e+04, Data-loss 2.8545e+04                  , pde-loss 8.4117e+03, initc-loss 7.5728e+04                    bc_loss 8.4983e+05\n",
      "Epoch 190, Training-Loss 3.9004e+04, Data-loss 3.0003e+04                  , pde-loss 5.8275e+03, initc-loss 7.5268e+04                    bc_loss 8.1901e+05\n",
      "Epoch 200, Training-Loss 4.0181e+04, Data-loss 3.1525e+04                  , pde-loss 5.8413e+03, initc-loss 7.6035e+04                    bc_loss 7.8372e+05\n",
      "Epoch 210, Training-Loss 3.6703e+04, Data-loss 2.7930e+04                  , pde-loss 5.5230e+03, initc-loss 6.6934e+04                    bc_loss 8.0487e+05\n",
      "Epoch 220, Training-Loss 3.5637e+04, Data-loss 2.7698e+04                  , pde-loss 4.1783e+03, initc-loss 7.2699e+04                    bc_loss 7.1706e+05\n",
      "Epoch 230, Training-Loss 3.0675e+04, Data-loss 2.3748e+04                  , pde-loss 4.2424e+03, initc-loss 7.2434e+04                    bc_loss 6.1603e+05\n",
      "Epoch 240, Training-Loss 3.4831e+04, Data-loss 2.8201e+04                  , pde-loss 3.6610e+03, initc-loss 7.8492e+04                    bc_loss 5.8088e+05\n",
      "Epoch 250, Training-Loss 3.5775e+04, Data-loss 2.8257e+04                  , pde-loss 4.0230e+03, initc-loss 6.7704e+04                    bc_loss 6.8006e+05\n",
      "Epoch 260, Training-Loss 3.7115e+04, Data-loss 3.0365e+04                  , pde-loss 4.4423e+03, initc-loss 7.2943e+04                    bc_loss 5.9764e+05\n",
      "Epoch 270, Training-Loss 3.5356e+04, Data-loss 2.8976e+04                  , pde-loss 3.9041e+03, initc-loss 5.4580e+04                    bc_loss 5.7955e+05\n",
      "Epoch 280, Training-Loss 3.3501e+04, Data-loss 2.7449e+04                  , pde-loss 3.9574e+03, initc-loss 6.6996e+04                    bc_loss 5.3426e+05\n",
      "Epoch 290, Training-Loss 2.9653e+04, Data-loss 2.2650e+04                  , pde-loss 3.1649e+03, initc-loss 5.7475e+04                    bc_loss 6.3971e+05\n",
      "Epoch 300, Training-Loss 2.7587e+04, Data-loss 2.2002e+04                  , pde-loss 3.8434e+03, initc-loss 6.3031e+04                    bc_loss 4.9172e+05\n",
      "Epoch 310, Training-Loss 3.0507e+04, Data-loss 2.4856e+04                  , pde-loss 3.4261e+03, initc-loss 6.0629e+04                    bc_loss 5.0103e+05\n",
      "Epoch 320, Training-Loss 2.8333e+04, Data-loss 2.2910e+04                  , pde-loss 3.2333e+03, initc-loss 5.7066e+04                    bc_loss 4.8197e+05\n",
      "Epoch 330, Training-Loss 3.3848e+04, Data-loss 2.8532e+04                  , pde-loss 3.3113e+03, initc-loss 6.4959e+04                    bc_loss 4.6330e+05\n",
      "Epoch 340, Training-Loss 3.1703e+04, Data-loss 2.6536e+04                  , pde-loss 2.9148e+03, initc-loss 5.0208e+04                    bc_loss 4.6357e+05\n",
      "Epoch 350, Training-Loss 3.3467e+04, Data-loss 2.8477e+04                  , pde-loss 2.2513e+03, initc-loss 5.6806e+04                    bc_loss 4.3988e+05\n",
      "Epoch 360, Training-Loss 2.6602e+04, Data-loss 2.1574e+04                  , pde-loss 2.3961e+03, initc-loss 5.4295e+04                    bc_loss 4.4610e+05\n",
      "Epoch 370, Training-Loss 2.9670e+04, Data-loss 2.5163e+04                  , pde-loss 2.1300e+03, initc-loss 4.3731e+04                    bc_loss 4.0485e+05\n",
      "Epoch 380, Training-Loss 2.5441e+04, Data-loss 2.0896e+04                  , pde-loss 2.3301e+03, initc-loss 5.7966e+04                    bc_loss 3.9417e+05\n",
      "Epoch 390, Training-Loss 3.2235e+04, Data-loss 2.7266e+04                  , pde-loss 1.9488e+03, initc-loss 5.4504e+04                    bc_loss 4.4048e+05\n",
      "Epoch 400, Training-Loss 3.1629e+04, Data-loss 2.6109e+04                  , pde-loss 1.8593e+03, initc-loss 5.1231e+04                    bc_loss 4.9891e+05\n",
      "Epoch 410, Training-Loss 2.4611e+04, Data-loss 2.1239e+04                  , pde-loss 2.0607e+03, initc-loss 5.5057e+04                    bc_loss 2.8003e+05\n",
      "Epoch 420, Training-Loss 2.7742e+04, Data-loss 2.3059e+04                  , pde-loss 2.0792e+03, initc-loss 5.0837e+04                    bc_loss 4.1537e+05\n",
      "Epoch 430, Training-Loss 2.6901e+04, Data-loss 2.2451e+04                  , pde-loss 1.7470e+03, initc-loss 5.7281e+04                    bc_loss 3.8600e+05\n",
      "Epoch 440, Training-Loss 2.8484e+04, Data-loss 2.4812e+04                  , pde-loss 2.1943e+03, initc-loss 4.8939e+04                    bc_loss 3.1608e+05\n",
      "Epoch 450, Training-Loss 3.0512e+04, Data-loss 2.4578e+04                  , pde-loss 1.7569e+03, initc-loss 4.5824e+04                    bc_loss 5.4587e+05\n",
      "Epoch 460, Training-Loss 2.5605e+04, Data-loss 1.9792e+04                  , pde-loss 1.5048e+03, initc-loss 4.5788e+04                    bc_loss 5.3397e+05\n",
      "Epoch 470, Training-Loss 2.7857e+04, Data-loss 2.3574e+04                  , pde-loss 1.0557e+03, initc-loss 4.9549e+04                    bc_loss 3.7765e+05\n",
      "Epoch 480, Training-Loss 2.8448e+04, Data-loss 2.3758e+04                  , pde-loss 1.5589e+03, initc-loss 5.1887e+04                    bc_loss 4.1559e+05\n",
      "Epoch 490, Training-Loss 2.4634e+04, Data-loss 2.0336e+04                  , pde-loss 1.8329e+03, initc-loss 5.2452e+04                    bc_loss 3.7550e+05\n",
      "Epoch 500, Training-Loss 2.5161e+04, Data-loss 2.1482e+04                  , pde-loss 1.3374e+03, initc-loss 4.7942e+04                    bc_loss 3.1857e+05\n",
      "Epoch 510, Training-Loss 2.4891e+04, Data-loss 2.0428e+04                  , pde-loss 1.6407e+03, initc-loss 4.8230e+04                    bc_loss 3.9646e+05\n",
      "Epoch 520, Training-Loss 2.5649e+04, Data-loss 2.2441e+04                  , pde-loss 1.6364e+03, initc-loss 4.7700e+04                    bc_loss 2.7147e+05\n",
      "Epoch 530, Training-Loss 2.5449e+04, Data-loss 2.1189e+04                  , pde-loss 1.4516e+03, initc-loss 3.8818e+04                    bc_loss 3.8579e+05\n",
      "Epoch 540, Training-Loss 2.2371e+04, Data-loss 1.8460e+04                  , pde-loss 1.2369e+03, initc-loss 4.7226e+04                    bc_loss 3.4267e+05\n",
      "Epoch 550, Training-Loss 2.8032e+04, Data-loss 2.4130e+04                  , pde-loss 1.7648e+03, initc-loss 4.4505e+04                    bc_loss 3.4385e+05\n",
      "Epoch 560, Training-Loss 2.0609e+04, Data-loss 1.8635e+04                  , pde-loss 1.4074e+03, initc-loss 4.3819e+04                    bc_loss 1.5222e+05\n",
      "Epoch 570, Training-Loss 2.4578e+04, Data-loss 2.0631e+04                  , pde-loss 8.4265e+02, initc-loss 4.0148e+04                    bc_loss 3.5375e+05\n",
      "Epoch 580, Training-Loss 2.4677e+04, Data-loss 2.1532e+04                  , pde-loss 1.3429e+03, initc-loss 4.7614e+04                    bc_loss 2.6550e+05\n",
      "Epoch 590, Training-Loss 2.0571e+04, Data-loss 1.7346e+04                  , pde-loss 1.1899e+03, initc-loss 4.7600e+04                    bc_loss 2.7371e+05\n",
      "Epoch 600, Training-Loss 2.2043e+04, Data-loss 1.7640e+04                  , pde-loss 1.6200e+03, initc-loss 4.4247e+04                    bc_loss 3.9443e+05\n",
      "Epoch 610, Training-Loss 2.2431e+04, Data-loss 1.9748e+04                  , pde-loss 1.0401e+03, initc-loss 4.6398e+04                    bc_loss 2.2087e+05\n",
      "Epoch 620, Training-Loss 2.4533e+04, Data-loss 1.9364e+04                  , pde-loss 1.3992e+03, initc-loss 3.7014e+04                    bc_loss 4.7856e+05\n",
      "Epoch 630, Training-Loss 2.0162e+04, Data-loss 1.7462e+04                  , pde-loss 1.4769e+03, initc-loss 4.9321e+04                    bc_loss 2.1925e+05\n",
      "Epoch 640, Training-Loss 2.2887e+04, Data-loss 1.8773e+04                  , pde-loss 1.2516e+03, initc-loss 4.0109e+04                    bc_loss 3.7004e+05\n",
      "Epoch 650, Training-Loss 2.6842e+04, Data-loss 2.3132e+04                  , pde-loss 6.7990e+02, initc-loss 3.9669e+04                    bc_loss 3.3065e+05\n",
      "Epoch 660, Training-Loss 2.2305e+04, Data-loss 1.8967e+04                  , pde-loss 1.2317e+03, initc-loss 4.3310e+04                    bc_loss 2.8933e+05\n",
      "Epoch 670, Training-Loss 2.7841e+04, Data-loss 2.5480e+04                  , pde-loss 1.3139e+03, initc-loss 4.4753e+04                    bc_loss 1.9003e+05\n",
      "Epoch 680, Training-Loss 2.0003e+04, Data-loss 1.6721e+04                  , pde-loss 1.4815e+03, initc-loss 3.2928e+04                    bc_loss 2.9380e+05\n",
      "Epoch 690, Training-Loss 2.1595e+04, Data-loss 1.8611e+04                  , pde-loss 1.1813e+03, initc-loss 3.8767e+04                    bc_loss 2.5846e+05\n",
      "Epoch 700, Training-Loss 2.1006e+04, Data-loss 1.6750e+04                  , pde-loss 1.0765e+03, initc-loss 4.1592e+04                    bc_loss 3.8294e+05\n",
      "Epoch 710, Training-Loss 2.2928e+04, Data-loss 1.7156e+04                  , pde-loss 1.2712e+03, initc-loss 4.3036e+04                    bc_loss 5.3296e+05\n",
      "Epoch 720, Training-Loss 2.1203e+04, Data-loss 1.9339e+04                  , pde-loss 1.5117e+03, initc-loss 4.5681e+04                    bc_loss 1.3917e+05\n",
      "Epoch 730, Training-Loss 2.1245e+04, Data-loss 1.6556e+04                  , pde-loss 1.3860e+03, initc-loss 3.0650e+04                    bc_loss 4.3681e+05\n",
      "Epoch 740, Training-Loss 1.9255e+04, Data-loss 1.6701e+04                  , pde-loss 1.2127e+03, initc-loss 3.9085e+04                    bc_loss 2.1511e+05\n",
      "Epoch 750, Training-Loss 1.4293e+04, Data-loss 1.3211e+04                  , pde-loss 1.4093e+03, initc-loss 3.7879e+04                    bc_loss 6.8971e+04\n",
      "Epoch 760, Training-Loss 2.2336e+04, Data-loss 1.8047e+04                  , pde-loss 9.5721e+02, initc-loss 3.4847e+04                    bc_loss 3.9311e+05\n",
      "Epoch 770, Training-Loss 1.9435e+04, Data-loss 1.7217e+04                  , pde-loss 9.6944e+02, initc-loss 4.1568e+04                    bc_loss 1.7926e+05\n",
      "Epoch 780, Training-Loss 2.0146e+04, Data-loss 1.5608e+04                  , pde-loss 1.3203e+03, initc-loss 4.1532e+04                    bc_loss 4.1091e+05\n",
      "Epoch 790, Training-Loss 2.2369e+04, Data-loss 1.9820e+04                  , pde-loss 1.3224e+03, initc-loss 4.1627e+04                    bc_loss 2.1194e+05\n",
      "Epoch 800, Training-Loss 1.8217e+04, Data-loss 1.4316e+04                  , pde-loss 1.6171e+03, initc-loss 3.3291e+04                    bc_loss 3.5523e+05\n",
      "Epoch 810, Training-Loss 2.0320e+04, Data-loss 1.6872e+04                  , pde-loss 1.2741e+03, initc-loss 4.1423e+04                    bc_loss 3.0209e+05\n",
      "Epoch 820, Training-Loss 2.2386e+04, Data-loss 1.9232e+04                  , pde-loss 9.3095e+02, initc-loss 3.5796e+04                    bc_loss 2.7862e+05\n",
      "Epoch 830, Training-Loss 2.2692e+04, Data-loss 2.0347e+04                  , pde-loss 1.3398e+03, initc-loss 4.3264e+04                    bc_loss 1.8996e+05\n",
      "Epoch 840, Training-Loss 1.8556e+04, Data-loss 1.5802e+04                  , pde-loss 1.0496e+03, initc-loss 3.9839e+04                    bc_loss 2.3452e+05\n",
      "Epoch 850, Training-Loss 2.0956e+04, Data-loss 1.8492e+04                  , pde-loss 1.5488e+03, initc-loss 3.4468e+04                    bc_loss 2.1043e+05\n",
      "Epoch 860, Training-Loss 1.5571e+04, Data-loss 1.1645e+04                  , pde-loss 1.3600e+03, initc-loss 2.2056e+04                    bc_loss 3.6916e+05\n",
      "Epoch 870, Training-Loss 1.9540e+04, Data-loss 1.6572e+04                  , pde-loss 1.0061e+03, initc-loss 4.1733e+04                    bc_loss 2.5410e+05\n",
      "Epoch 880, Training-Loss 1.5132e+04, Data-loss 1.2954e+04                  , pde-loss 1.9973e+03, initc-loss 3.9403e+04                    bc_loss 1.7641e+05\n",
      "Epoch 890, Training-Loss 1.7434e+04, Data-loss 1.4439e+04                  , pde-loss 1.2565e+03, initc-loss 3.1664e+04                    bc_loss 2.6665e+05\n",
      "Epoch 900, Training-Loss 1.6417e+04, Data-loss 1.4779e+04                  , pde-loss 1.1485e+03, initc-loss 3.1443e+04                    bc_loss 1.3120e+05\n",
      "Epoch 910, Training-Loss 1.7189e+04, Data-loss 1.5118e+04                  , pde-loss 1.7930e+03, initc-loss 3.6228e+04                    bc_loss 1.6905e+05\n",
      "Epoch 920, Training-Loss 1.6103e+04, Data-loss 1.4247e+04                  , pde-loss 1.8783e+03, initc-loss 3.0954e+04                    bc_loss 1.5278e+05\n",
      "Epoch 930, Training-Loss 1.6306e+04, Data-loss 1.2997e+04                  , pde-loss 1.4009e+03, initc-loss 3.2032e+04                    bc_loss 2.9751e+05\n",
      "Epoch 940, Training-Loss 1.6698e+04, Data-loss 1.5051e+04                  , pde-loss 1.8186e+03, initc-loss 4.3737e+04                    bc_loss 1.1912e+05\n",
      "Epoch 950, Training-Loss 1.6082e+04, Data-loss 1.1629e+04                  , pde-loss 2.2644e+03, initc-loss 2.8232e+04                    bc_loss 4.1478e+05\n",
      "Epoch 960, Training-Loss 1.5592e+04, Data-loss 1.4725e+04                  , pde-loss 7.9694e+02, initc-loss 3.1466e+04                    bc_loss 5.4369e+04\n",
      "Epoch 970, Training-Loss 1.2754e+04, Data-loss 9.0130e+03                  , pde-loss 2.0411e+03, initc-loss 2.6788e+04                    bc_loss 3.4531e+05\n",
      "Epoch 980, Training-Loss 1.5879e+04, Data-loss 1.4282e+04                  , pde-loss 2.6430e+03, initc-loss 3.3325e+04                    bc_loss 1.2375e+05\n",
      "Epoch 990, Training-Loss 1.2525e+04, Data-loss 1.1901e+04                  , pde-loss 1.2288e+03, initc-loss 3.1411e+04                    bc_loss 2.9775e+04\n",
      "Epoch 1000, Training-Loss 9.9107e+03, Data-loss 7.2756e+03                  , pde-loss 1.6398e+03, initc-loss 2.5273e+04                    bc_loss 2.3660e+05\n",
      "Epoch 1010, Training-Loss 1.0540e+04, Data-loss 8.1282e+03                  , pde-loss 1.7567e+03, initc-loss 2.9679e+04                    bc_loss 2.0973e+05\n",
      "Epoch 1020, Training-Loss 2.1973e+04, Data-loss 1.8239e+04                  , pde-loss 1.7015e+03, initc-loss 3.9582e+04                    bc_loss 3.3214e+05\n",
      "Epoch 1030, Training-Loss 1.2635e+04, Data-loss 1.0740e+04                  , pde-loss 3.4070e+03, initc-loss 2.3283e+04                    bc_loss 1.6280e+05\n",
      "Epoch 1040, Training-Loss 1.2285e+04, Data-loss 9.7047e+03                  , pde-loss 1.3946e+03, initc-loss 3.1790e+04                    bc_loss 2.2483e+05\n",
      "Epoch 1050, Training-Loss 1.4375e+04, Data-loss 1.1155e+04                  , pde-loss 1.1221e+03, initc-loss 2.5538e+04                    bc_loss 2.9527e+05\n",
      "Epoch 1060, Training-Loss 9.6538e+03, Data-loss 8.8320e+03                  , pde-loss 3.2528e+03, initc-loss 2.9133e+04                    bc_loss 4.9789e+04\n",
      "Epoch 1070, Training-Loss 9.8728e+03, Data-loss 7.2507e+03                  , pde-loss 2.8117e+03, initc-loss 2.7502e+04                    bc_loss 2.3189e+05\n",
      "Epoch 1080, Training-Loss 1.1616e+04, Data-loss 1.0288e+04                  , pde-loss 1.3791e+03, initc-loss 2.5474e+04                    bc_loss 1.0593e+05\n",
      "Epoch 1090, Training-Loss 1.0550e+04, Data-loss 7.5391e+03                  , pde-loss 3.8153e+03, initc-loss 3.0615e+04                    bc_loss 2.6667e+05\n",
      "Epoch 1100, Training-Loss 1.0658e+04, Data-loss 8.9679e+03                  , pde-loss 3.8685e+03, initc-loss 2.3521e+04                    bc_loss 1.4158e+05\n",
      "Epoch 1110, Training-Loss 1.3129e+04, Data-loss 1.2377e+04                  , pde-loss 1.3201e+03, initc-loss 2.2807e+04                    bc_loss 5.1021e+04\n",
      "Epoch 1120, Training-Loss 8.2809e+03, Data-loss 7.6179e+03                  , pde-loss 2.5369e+03, initc-loss 2.4004e+04                    bc_loss 3.9758e+04\n",
      "Epoch 1130, Training-Loss 1.6569e+04, Data-loss 1.0809e+04                  , pde-loss 3.9241e+03, initc-loss 3.3396e+04                    bc_loss 5.3862e+05\n",
      "Epoch 1140, Training-Loss 1.1246e+04, Data-loss 8.2794e+03                  , pde-loss 2.9202e+03, initc-loss 2.6334e+04                    bc_loss 2.6740e+05\n",
      "Epoch 1150, Training-Loss 8.1847e+03, Data-loss 7.3089e+03                  , pde-loss 2.0763e+03, initc-loss 2.1918e+04                    bc_loss 6.3594e+04\n",
      "Epoch 1160, Training-Loss 8.5184e+03, Data-loss 7.4432e+03                  , pde-loss 3.5779e+03, initc-loss 2.7998e+04                    bc_loss 7.5946e+04\n",
      "Epoch 1170, Training-Loss 1.8833e+04, Data-loss 7.7803e+03                  , pde-loss 3.4229e+03, initc-loss 2.6596e+04                    bc_loss 1.0753e+06\n",
      "Epoch 1180, Training-Loss 1.2626e+04, Data-loss 1.1249e+04                  , pde-loss 2.1095e+03, initc-loss 2.6594e+04                    bc_loss 1.0906e+05\n",
      "Epoch 1190, Training-Loss 1.2743e+04, Data-loss 1.0558e+04                  , pde-loss 2.4935e+03, initc-loss 2.5503e+04                    bc_loss 1.9050e+05\n",
      "Epoch 1200, Training-Loss 1.1473e+04, Data-loss 8.7826e+03                  , pde-loss 3.9750e+03, initc-loss 1.6640e+04                    bc_loss 2.4846e+05\n",
      "Epoch 1210, Training-Loss 5.5347e+03, Data-loss 4.4092e+03                  , pde-loss 1.8835e+03, initc-loss 2.3681e+04                    bc_loss 8.6986e+04\n",
      "Epoch 1220, Training-Loss 1.6169e+04, Data-loss 8.0036e+03                  , pde-loss 2.2806e+03, initc-loss 2.5031e+04                    bc_loss 7.8921e+05\n",
      "Epoch 1230, Training-Loss 1.1831e+04, Data-loss 1.0859e+04                  , pde-loss 2.5063e+03, initc-loss 3.1332e+04                    bc_loss 6.3318e+04\n",
      "Epoch 1240, Training-Loss 1.3346e+04, Data-loss 1.2331e+04                  , pde-loss 2.4787e+03, initc-loss 3.3322e+04                    bc_loss 6.5675e+04\n",
      "Epoch 1250, Training-Loss 7.5006e+03, Data-loss 5.8227e+03                  , pde-loss 3.3884e+03, initc-loss 2.1644e+04                    bc_loss 1.4276e+05\n",
      "Epoch 1260, Training-Loss 1.0506e+04, Data-loss 8.6800e+03                  , pde-loss 1.6485e+03, initc-loss 2.7499e+04                    bc_loss 1.5349e+05\n",
      "Epoch 1270, Training-Loss 1.5262e+04, Data-loss 7.6769e+03                  , pde-loss 2.6039e+03, initc-loss 2.7272e+04                    bc_loss 7.2867e+05\n",
      "Epoch 1280, Training-Loss 1.3874e+04, Data-loss 1.1240e+04                  , pde-loss 4.2913e+03, initc-loss 2.7661e+04                    bc_loss 2.3152e+05\n",
      "Epoch 1290, Training-Loss 5.8585e+03, Data-loss 4.5924e+03                  , pde-loss 2.5067e+03, initc-loss 1.8654e+04                    bc_loss 1.0545e+05\n",
      "Epoch 1300, Training-Loss 6.4702e+03, Data-loss 5.7219e+03                  , pde-loss 3.8060e+03, initc-loss 2.8975e+04                    bc_loss 4.2052e+04\n",
      "Epoch 1310, Training-Loss 1.3915e+04, Data-loss 7.3700e+03                  , pde-loss 3.0939e+03, initc-loss 3.5081e+04                    bc_loss 6.1631e+05\n",
      "Epoch 1320, Training-Loss 1.1008e+04, Data-loss 9.8169e+03                  , pde-loss 2.1872e+03, initc-loss 2.7806e+04                    bc_loss 8.9153e+04\n",
      "Epoch 1330, Training-Loss 8.8238e+03, Data-loss 7.7623e+03                  , pde-loss 2.3361e+03, initc-loss 2.4552e+04                    bc_loss 7.9272e+04\n",
      "Epoch 1340, Training-Loss 4.8491e+03, Data-loss 3.9673e+03                  , pde-loss 2.6636e+03, initc-loss 2.3860e+04                    bc_loss 6.1661e+04\n",
      "Epoch 1350, Training-Loss 7.0569e+03, Data-loss 4.3213e+03                  , pde-loss 3.6150e+03, initc-loss 2.4788e+04                    bc_loss 2.4516e+05\n",
      "Epoch 1360, Training-Loss 9.4096e+03, Data-loss 8.8528e+03                  , pde-loss 1.8708e+03, initc-loss 2.6979e+04                    bc_loss 2.6838e+04\n",
      "Epoch 1370, Training-Loss 9.6371e+03, Data-loss 7.1959e+03                  , pde-loss 2.4970e+03, initc-loss 2.8675e+04                    bc_loss 2.1295e+05\n",
      "Epoch 1380, Training-Loss 8.3280e+03, Data-loss 7.6809e+03                  , pde-loss 2.1431e+03, initc-loss 1.8750e+04                    bc_loss 4.3818e+04\n",
      "Epoch 1390, Training-Loss 5.4122e+03, Data-loss 4.2428e+03                  , pde-loss 3.7025e+03, initc-loss 2.0067e+04                    bc_loss 9.3167e+04\n",
      "Epoch 1400, Training-Loss 5.6493e+03, Data-loss 3.5578e+03                  , pde-loss 1.9660e+03, initc-loss 1.9676e+04                    bc_loss 1.8751e+05\n",
      "Epoch 1410, Training-Loss 1.1747e+04, Data-loss 8.5904e+03                  , pde-loss 1.8852e+03, initc-loss 2.7062e+04                    bc_loss 2.8669e+05\n",
      "Epoch 1420, Training-Loss 7.8525e+03, Data-loss 6.6893e+03                  , pde-loss 2.1199e+03, initc-loss 1.5769e+04                    bc_loss 9.8424e+04\n",
      "Epoch 1430, Training-Loss 6.1537e+03, Data-loss 5.1745e+03                  , pde-loss 5.0006e+03, initc-loss 2.0178e+04                    bc_loss 7.2738e+04\n",
      "Epoch 1440, Training-Loss 6.1822e+03, Data-loss 3.7423e+03                  , pde-loss 1.2157e+03, initc-loss 2.6242e+04                    bc_loss 2.1653e+05\n",
      "Epoch 1450, Training-Loss 1.2657e+04, Data-loss 8.7393e+03                  , pde-loss 1.9957e+03, initc-loss 2.6705e+04                    bc_loss 3.6305e+05\n",
      "Epoch 1460, Training-Loss 1.1016e+04, Data-loss 9.6347e+03                  , pde-loss 8.4314e+03, initc-loss 1.7951e+04                    bc_loss 1.1179e+05\n",
      "Epoch 1470, Training-Loss 5.7945e+03, Data-loss 5.0231e+03                  , pde-loss 1.5339e+03, initc-loss 1.8264e+04                    bc_loss 5.7342e+04\n",
      "Epoch 1480, Training-Loss 9.0250e+03, Data-loss 4.8089e+03                  , pde-loss 1.4333e+03, initc-loss 2.4572e+04                    bc_loss 3.9560e+05\n",
      "Epoch 1490, Training-Loss 5.1444e+03, Data-loss 4.4208e+03                  , pde-loss 2.5593e+03, initc-loss 1.6309e+04                    bc_loss 5.3488e+04\n",
      "Epoch 1500, Training-Loss 7.6472e+03, Data-loss 6.1775e+03                  , pde-loss 4.1063e+03, initc-loss 2.3100e+04                    bc_loss 1.1977e+05\n",
      "Epoch 1510, Training-Loss 1.6203e+04, Data-loss 1.2352e+04                  , pde-loss 9.6651e+02, initc-loss 3.3914e+04                    bc_loss 3.5027e+05\n",
      "Epoch 1520, Training-Loss 8.9475e+03, Data-loss 7.1315e+03                  , pde-loss 1.2574e+03, initc-loss 1.3597e+04                    bc_loss 1.6674e+05\n",
      "Epoch 1530, Training-Loss 6.0059e+03, Data-loss 4.8222e+03                  , pde-loss 5.0552e+03, initc-loss 2.1332e+04                    bc_loss 9.1987e+04\n",
      "Epoch 1540, Training-Loss 6.4519e+03, Data-loss 4.7372e+03                  , pde-loss 1.5473e+03, initc-loss 2.2853e+04                    bc_loss 1.4707e+05\n",
      "Epoch 1550, Training-Loss 9.6646e+03, Data-loss 7.9438e+03                  , pde-loss 6.4495e+02, initc-loss 2.6681e+04                    bc_loss 1.4475e+05\n",
      "Epoch 1560, Training-Loss 1.5129e+04, Data-loss 1.1287e+04                  , pde-loss 2.0542e+03, initc-loss 2.2241e+04                    bc_loss 3.5988e+05\n",
      "Epoch 1570, Training-Loss 1.1153e+04, Data-loss 5.2997e+03                  , pde-loss 5.5679e+03, initc-loss 1.3820e+04                    bc_loss 5.6594e+05\n",
      "Epoch 1580, Training-Loss 4.8511e+03, Data-loss 2.9791e+03                  , pde-loss 2.3603e+03, initc-loss 2.0985e+04                    bc_loss 1.6386e+05\n",
      "Epoch 1590, Training-Loss 3.5758e+03, Data-loss 2.2751e+03                  , pde-loss 2.9190e+03, initc-loss 1.7410e+04                    bc_loss 1.0974e+05\n",
      "Epoch 1600, Training-Loss 7.7409e+03, Data-loss 5.6787e+03                  , pde-loss 1.1966e+03, initc-loss 2.7442e+04                    bc_loss 1.7758e+05\n",
      "Epoch 1610, Training-Loss 1.3487e+04, Data-loss 9.2682e+03                  , pde-loss 1.6739e+03, initc-loss 2.8711e+04                    bc_loss 3.9148e+05\n",
      "Epoch 1620, Training-Loss 6.7864e+03, Data-loss 4.9776e+03                  , pde-loss 2.2160e+03, initc-loss 1.7994e+04                    bc_loss 1.6067e+05\n",
      "Epoch 1630, Training-Loss 6.5690e+03, Data-loss 5.5510e+03                  , pde-loss 4.8143e+03, initc-loss 1.9283e+04                    bc_loss 7.7700e+04\n",
      "Epoch 1640, Training-Loss 2.7274e+03, Data-loss 1.6723e+03                  , pde-loss 2.0447e+03, initc-loss 1.4569e+04                    bc_loss 8.8902e+04\n",
      "Epoch 1650, Training-Loss 5.5637e+03, Data-loss 4.7720e+03                  , pde-loss 3.4838e+03, initc-loss 2.1813e+04                    bc_loss 5.3866e+04\n",
      "Epoch 1660, Training-Loss 1.0820e+04, Data-loss 8.5033e+03                  , pde-loss 3.2097e+03, initc-loss 2.7210e+04                    bc_loss 2.0120e+05\n",
      "Epoch 1670, Training-Loss 7.5846e+03, Data-loss 6.0357e+03                  , pde-loss 2.1656e+03, initc-loss 2.2788e+04                    bc_loss 1.2994e+05\n",
      "Epoch 1680, Training-Loss 7.3343e+03, Data-loss 5.4107e+03                  , pde-loss 3.4030e+03, initc-loss 1.1048e+04                    bc_loss 1.7791e+05\n",
      "Epoch 1690, Training-Loss 5.2291e+03, Data-loss 4.2024e+03                  , pde-loss 8.6235e+02, initc-loss 1.9384e+04                    bc_loss 8.2427e+04\n",
      "Epoch 1700, Training-Loss 3.7952e+03, Data-loss 2.8514e+03                  , pde-loss 1.1817e+03, initc-loss 2.2114e+04                    bc_loss 7.1082e+04\n",
      "Epoch 1710, Training-Loss 6.5586e+03, Data-loss 4.3335e+03                  , pde-loss 1.7847e+03, initc-loss 2.3476e+04                    bc_loss 1.9726e+05\n",
      "Epoch 1720, Training-Loss 7.0907e+03, Data-loss 5.6261e+03                  , pde-loss 8.0381e+02, initc-loss 2.0507e+04                    bc_loss 1.2515e+05\n",
      "Epoch 1730, Training-Loss 8.5367e+03, Data-loss 7.7187e+03                  , pde-loss 3.2185e+03, initc-loss 1.7108e+04                    bc_loss 6.1467e+04\n",
      "Epoch 1740, Training-Loss 6.8963e+03, Data-loss 4.9552e+03                  , pde-loss 2.7907e+03, initc-loss 1.7968e+04                    bc_loss 1.7335e+05\n",
      "Epoch 1750, Training-Loss 7.4890e+03, Data-loss 3.7001e+03                  , pde-loss 1.6180e+03, initc-loss 1.8104e+04                    bc_loss 3.5917e+05\n",
      "Epoch 1760, Training-Loss 1.1353e+04, Data-loss 1.0028e+04                  , pde-loss 8.2827e+03, initc-loss 2.2685e+04                    bc_loss 1.0153e+05\n",
      "Epoch 1770, Training-Loss 7.7054e+03, Data-loss 4.5967e+03                  , pde-loss 1.8052e+03, initc-loss 1.4079e+04                    bc_loss 2.9498e+05\n",
      "Epoch 1780, Training-Loss 7.0923e+03, Data-loss 5.8921e+03                  , pde-loss 1.0436e+03, initc-loss 1.9148e+04                    bc_loss 9.9831e+04\n",
      "Epoch 1790, Training-Loss 3.0145e+03, Data-loss 1.7733e+03                  , pde-loss 3.3932e+03, initc-loss 1.4477e+04                    bc_loss 1.0626e+05\n",
      "Epoch 1800, Training-Loss 3.0367e+03, Data-loss 2.6297e+03                  , pde-loss 2.1491e+03, initc-loss 1.1437e+04                    bc_loss 2.7110e+04\n",
      "Epoch 1810, Training-Loss 8.8889e+03, Data-loss 5.5018e+03                  , pde-loss 8.3405e+02, initc-loss 2.4077e+04                    bc_loss 3.1380e+05\n",
      "Epoch 1820, Training-Loss 5.2492e+03, Data-loss 3.9930e+03                  , pde-loss 2.1020e+03, initc-loss 1.6404e+04                    bc_loss 1.0712e+05\n",
      "Epoch 1830, Training-Loss 8.2963e+03, Data-loss 7.0257e+03                  , pde-loss 4.7890e+03, initc-loss 2.0333e+04                    bc_loss 1.0194e+05\n",
      "Epoch 1840, Training-Loss 5.7093e+03, Data-loss 3.9986e+03                  , pde-loss 2.9313e+03, initc-loss 1.6030e+04                    bc_loss 1.5210e+05\n",
      "Epoch 1850, Training-Loss 1.1016e+04, Data-loss 7.1232e+03                  , pde-loss 1.4121e+03, initc-loss 2.0779e+04                    bc_loss 3.6707e+05\n",
      "Epoch 1860, Training-Loss 4.7348e+03, Data-loss 2.7935e+03                  , pde-loss 1.4845e+03, initc-loss 1.6727e+04                    bc_loss 1.7591e+05\n",
      "Epoch 1870, Training-Loss 4.6363e+03, Data-loss 3.7753e+03                  , pde-loss 5.6247e+03, initc-loss 1.5542e+04                    bc_loss 6.4934e+04\n",
      "Epoch 1880, Training-Loss 2.7398e+03, Data-loss 2.1141e+03                  , pde-loss 1.6756e+03, initc-loss 1.5959e+04                    bc_loss 4.4935e+04\n",
      "Epoch 1890, Training-Loss 8.5780e+03, Data-loss 1.7616e+03                  , pde-loss 2.5812e+03, initc-loss 1.8042e+04                    bc_loss 6.6102e+05\n",
      "Epoch 1900, Training-Loss 1.0768e+04, Data-loss 5.2459e+03                  , pde-loss 3.9414e+03, initc-loss 2.7219e+04                    bc_loss 5.2101e+05\n",
      "Epoch 1910, Training-Loss 1.1823e+04, Data-loss 9.1758e+03                  , pde-loss 1.0021e+03, initc-loss 2.3167e+04                    bc_loss 2.4052e+05\n",
      "Epoch 1920, Training-Loss 6.7096e+03, Data-loss 4.2443e+03                  , pde-loss 1.3808e+03, initc-loss 1.4973e+04                    bc_loss 2.3018e+05\n",
      "Epoch 1930, Training-Loss 4.8083e+03, Data-loss 3.6660e+03                  , pde-loss 3.6792e+03, initc-loss 2.6078e+04                    bc_loss 8.4478e+04\n",
      "Epoch 1940, Training-Loss 6.1330e+03, Data-loss 2.3966e+03                  , pde-loss 1.6107e+03, initc-loss 2.0085e+04                    bc_loss 3.5195e+05\n",
      "Epoch 1950, Training-Loss 8.5331e+03, Data-loss 6.8436e+03                  , pde-loss 1.7017e+03, initc-loss 2.5629e+04                    bc_loss 1.4163e+05\n",
      "Epoch 1960, Training-Loss 8.0256e+03, Data-loss 6.2682e+03                  , pde-loss 1.8406e+03, initc-loss 1.6224e+04                    bc_loss 1.5767e+05\n",
      "Epoch 1970, Training-Loss 8.6353e+03, Data-loss 7.8561e+03                  , pde-loss 2.0752e+03, initc-loss 1.9004e+04                    bc_loss 5.6834e+04\n",
      "Epoch 1980, Training-Loss 7.3730e+03, Data-loss 6.1334e+03                  , pde-loss 3.2942e+03, initc-loss 2.3276e+04                    bc_loss 9.7387e+04\n",
      "Epoch 1990, Training-Loss 9.5114e+03, Data-loss 5.2504e+03                  , pde-loss 1.5672e+03, initc-loss 2.0236e+04                    bc_loss 4.0430e+05\n",
      "Epoch 2000, Training-Loss 4.8475e+03, Data-loss 4.1740e+03                  , pde-loss 9.9692e+02, initc-loss 1.3596e+04                    bc_loss 5.2756e+04\n",
      "Epoch 2010, Training-Loss 1.0637e+04, Data-loss 4.9788e+03                  , pde-loss 4.0567e+03, initc-loss 2.3827e+04                    bc_loss 5.3793e+05\n",
      "Epoch 2020, Training-Loss 5.9606e+03, Data-loss 4.6144e+03                  , pde-loss 4.6441e+03, initc-loss 2.0217e+04                    bc_loss 1.0975e+05\n",
      "Epoch 2030, Training-Loss 1.2124e+04, Data-loss 6.2516e+03                  , pde-loss 1.4615e+03, initc-loss 1.2498e+04                    bc_loss 5.7331e+05\n",
      "Epoch 2040, Training-Loss 4.8625e+03, Data-loss 3.4872e+03                  , pde-loss 5.1628e+03, initc-loss 1.7998e+04                    bc_loss 1.1437e+05\n",
      "Epoch 2050, Training-Loss 6.4115e+03, Data-loss 5.4808e+03                  , pde-loss 4.8578e+03, initc-loss 2.1565e+04                    bc_loss 6.6648e+04\n",
      "Epoch 2060, Training-Loss 3.3037e+03, Data-loss 2.8475e+03                  , pde-loss 1.5451e+03, initc-loss 1.3747e+04                    bc_loss 3.0332e+04\n",
      "Epoch 2070, Training-Loss 3.9215e+03, Data-loss 2.8415e+03                  , pde-loss 2.6309e+03, initc-loss 1.5879e+04                    bc_loss 8.9495e+04\n",
      "Epoch 2080, Training-Loss 3.1762e+03, Data-loss 2.4755e+03                  , pde-loss 1.9944e+03, initc-loss 1.5691e+04                    bc_loss 5.2383e+04\n",
      "Epoch 2090, Training-Loss 4.7753e+03, Data-loss 2.1029e+03                  , pde-loss 2.7927e+03, initc-loss 1.7786e+04                    bc_loss 2.4665e+05\n",
      "Epoch 2100, Training-Loss 2.5054e+03, Data-loss 1.6513e+03                  , pde-loss 3.8867e+03, initc-loss 1.2093e+04                    bc_loss 6.9424e+04\n",
      "Epoch 2110, Training-Loss 5.3278e+03, Data-loss 2.3318e+03                  , pde-loss 4.4959e+03, initc-loss 1.8209e+04                    bc_loss 2.7690e+05\n",
      "Epoch 2120, Training-Loss 3.8882e+03, Data-loss 3.4809e+03                  , pde-loss 2.4270e+03, initc-loss 1.6883e+04                    bc_loss 2.1422e+04\n",
      "Epoch 2130, Training-Loss 1.3088e+04, Data-loss 8.2636e+03                  , pde-loss 7.7628e+02, initc-loss 2.7371e+04                    bc_loss 4.5429e+05\n",
      "Epoch 2140, Training-Loss 9.3587e+03, Data-loss 7.9802e+03                  , pde-loss 8.1900e+02, initc-loss 1.7729e+04                    bc_loss 1.1931e+05\n",
      "Epoch 2150, Training-Loss 4.6248e+03, Data-loss 3.7579e+03                  , pde-loss 3.7766e+03, initc-loss 1.8057e+04                    bc_loss 6.4855e+04\n",
      "Epoch 2160, Training-Loss 4.2859e+03, Data-loss 2.0455e+03                  , pde-loss 1.5463e+03, initc-loss 1.2917e+04                    bc_loss 2.0958e+05\n",
      "Epoch 2170, Training-Loss 7.9855e+03, Data-loss 5.2022e+03                  , pde-loss 1.0456e+03, initc-loss 1.9419e+04                    bc_loss 2.5787e+05\n",
      "Epoch 2180, Training-Loss 5.2062e+03, Data-loss 3.6864e+03                  , pde-loss 1.0359e+03, initc-loss 1.9502e+04                    bc_loss 1.3144e+05\n",
      "Epoch 2190, Training-Loss 4.7716e+03, Data-loss 3.9511e+03                  , pde-loss 5.3398e+03, initc-loss 2.1488e+04                    bc_loss 5.5223e+04\n",
      "Epoch 2200, Training-Loss 2.6667e+03, Data-loss 2.1925e+03                  , pde-loss 2.8749e+03, initc-loss 1.4468e+04                    bc_loss 3.0076e+04\n",
      "Epoch 2210, Training-Loss 1.0924e+04, Data-loss 3.5027e+03                  , pde-loss 2.1558e+03, initc-loss 1.6022e+04                    bc_loss 7.2395e+05\n",
      "Epoch 2220, Training-Loss 6.7827e+03, Data-loss 5.9449e+03                  , pde-loss 4.1808e+03, initc-loss 2.6647e+04                    bc_loss 5.2958e+04\n",
      "Epoch 2230, Training-Loss 6.8273e+03, Data-loss 6.0066e+03                  , pde-loss 3.0717e+03, initc-loss 2.0233e+04                    bc_loss 5.8763e+04\n",
      "Epoch 2240, Training-Loss 4.9782e+03, Data-loss 4.3519e+03                  , pde-loss 1.7235e+03, initc-loss 2.0721e+04                    bc_loss 4.0187e+04\n",
      "Epoch 2250, Training-Loss 8.1047e+03, Data-loss 3.2131e+03                  , pde-loss 1.8530e+03, initc-loss 2.3022e+04                    bc_loss 4.6429e+05\n",
      "Epoch 2260, Training-Loss 1.2381e+04, Data-loss 8.3715e+03                  , pde-loss 2.0234e+03, initc-loss 1.7641e+04                    bc_loss 3.8124e+05\n",
      "Epoch 2270, Training-Loss 7.1521e+03, Data-loss 5.0661e+03                  , pde-loss 2.6469e+03, initc-loss 1.7234e+04                    bc_loss 1.8872e+05\n",
      "Epoch 2280, Training-Loss 7.3649e+03, Data-loss 6.8197e+03                  , pde-loss 2.2538e+03, initc-loss 2.1730e+04                    bc_loss 3.0534e+04\n",
      "Epoch 2290, Training-Loss 4.8273e+03, Data-loss 2.3598e+03                  , pde-loss 2.1067e+03, initc-loss 1.9464e+04                    bc_loss 2.2518e+05\n",
      "Epoch 2300, Training-Loss 7.1128e+03, Data-loss 5.6344e+03                  , pde-loss 2.9026e+03, initc-loss 1.6296e+04                    bc_loss 1.2864e+05\n",
      "Epoch 2310, Training-Loss 7.7431e+03, Data-loss 6.0837e+03                  , pde-loss 4.5645e+03, initc-loss 2.1437e+04                    bc_loss 1.3993e+05\n",
      "Epoch 2320, Training-Loss 1.0601e+04, Data-loss 9.6321e+03                  , pde-loss 8.9780e+02, initc-loss 2.1145e+04                    bc_loss 7.4844e+04\n",
      "Epoch 2330, Training-Loss 3.6610e+03, Data-loss 2.3836e+03                  , pde-loss 2.7717e+03, initc-loss 2.1588e+04                    bc_loss 1.0338e+05\n",
      "Epoch 2340, Training-Loss 4.4452e+03, Data-loss 3.4768e+03                  , pde-loss 4.3585e+03, initc-loss 2.0196e+04                    bc_loss 7.2287e+04\n",
      "Epoch 2350, Training-Loss 8.4170e+03, Data-loss 3.9679e+03                  , pde-loss 1.0094e+03, initc-loss 2.3731e+04                    bc_loss 4.2017e+05\n",
      "Epoch 2360, Training-Loss 1.0681e+04, Data-loss 9.2370e+03                  , pde-loss 1.3934e+03, initc-loss 1.6020e+04                    bc_loss 1.2698e+05\n",
      "Epoch 2370, Training-Loss 5.7834e+03, Data-loss 4.0572e+03                  , pde-loss 4.5168e+03, initc-loss 1.7619e+04                    bc_loss 1.5048e+05\n",
      "Epoch 2380, Training-Loss 3.7103e+03, Data-loss 3.1121e+03                  , pde-loss 2.2212e+03, initc-loss 1.7844e+04                    bc_loss 3.9760e+04\n",
      "Epoch 2390, Training-Loss 3.2798e+03, Data-loss 2.4881e+03                  , pde-loss 2.2500e+03, initc-loss 1.4085e+04                    bc_loss 6.2839e+04\n",
      "Epoch 2400, Training-Loss 8.9657e+03, Data-loss 2.5699e+03                  , pde-loss 2.9511e+03, initc-loss 1.9464e+04                    bc_loss 6.1716e+05\n",
      "Epoch 2410, Training-Loss 6.6155e+03, Data-loss 5.4614e+03                  , pde-loss 3.7529e+03, initc-loss 2.1983e+04                    bc_loss 8.9673e+04\n",
      "Epoch 2420, Training-Loss 8.6149e+03, Data-loss 6.9102e+03                  , pde-loss 1.9774e+03, initc-loss 1.7238e+04                    bc_loss 1.5125e+05\n",
      "Epoch 2430, Training-Loss 7.2214e+03, Data-loss 5.0589e+03                  , pde-loss 6.2067e+03, initc-loss 1.9013e+04                    bc_loss 1.9104e+05\n",
      "Epoch 2440, Training-Loss 3.1391e+03, Data-loss 2.3678e+03                  , pde-loss 2.6699e+03, initc-loss 1.3272e+04                    bc_loss 6.1183e+04\n",
      "Epoch 2450, Training-Loss 5.2873e+03, Data-loss 3.5904e+03                  , pde-loss 9.8299e+02, initc-loss 1.5005e+04                    bc_loss 1.5371e+05\n",
      "Epoch 2460, Training-Loss 2.3006e+03, Data-loss 1.5429e+03                  , pde-loss 1.2571e+03, initc-loss 1.6957e+04                    bc_loss 5.7551e+04\n",
      "Epoch 2470, Training-Loss 2.6342e+03, Data-loss 2.0629e+03                  , pde-loss 2.7705e+03, initc-loss 1.4951e+04                    bc_loss 3.9410e+04\n",
      "Epoch 2480, Training-Loss 7.1526e+03, Data-loss 2.0047e+03                  , pde-loss 9.4739e+02, initc-loss 1.8159e+04                    bc_loss 4.9568e+05\n",
      "Epoch 2490, Training-Loss 3.7121e+03, Data-loss 3.0911e+03                  , pde-loss 3.3794e+03, initc-loss 1.1151e+04                    bc_loss 4.7569e+04\n",
      "Epoch 2500, Training-Loss 3.3572e+03, Data-loss 2.1168e+03                  , pde-loss 3.4454e+03, initc-loss 1.7430e+04                    bc_loss 1.0316e+05\n",
      "Epoch 2510, Training-Loss 4.3939e+03, Data-loss 1.8639e+03                  , pde-loss 2.5006e+03, initc-loss 1.4539e+04                    bc_loss 2.3596e+05\n",
      "Epoch 2520, Training-Loss 3.5261e+03, Data-loss 2.8146e+03                  , pde-loss 1.8924e+03, initc-loss 1.0741e+04                    bc_loss 5.8519e+04\n",
      "Epoch 2530, Training-Loss 1.7309e+03, Data-loss 1.2264e+03                  , pde-loss 1.4163e+03, initc-loss 1.4485e+04                    bc_loss 3.4552e+04\n",
      "Epoch 2540, Training-Loss 8.8164e+03, Data-loss 2.0945e+03                  , pde-loss 6.5587e+02, initc-loss 1.6494e+04                    bc_loss 6.5504e+05\n",
      "Epoch 2550, Training-Loss 4.5063e+03, Data-loss 3.4674e+03                  , pde-loss 1.5729e+03, initc-loss 1.3140e+04                    bc_loss 8.9184e+04\n",
      "Epoch 2560, Training-Loss 3.6618e+03, Data-loss 2.6027e+03                  , pde-loss 2.8077e+03, initc-loss 1.4082e+04                    bc_loss 8.9016e+04\n",
      "Epoch 2570, Training-Loss 4.3703e+03, Data-loss 3.4891e+03                  , pde-loss 1.7464e+03, initc-loss 1.7441e+04                    bc_loss 6.8934e+04\n",
      "Epoch 2580, Training-Loss 2.9576e+03, Data-loss 2.3445e+03                  , pde-loss 2.9950e+03, initc-loss 1.2864e+04                    bc_loss 4.5450e+04\n",
      "Epoch 2590, Training-Loss 2.2606e+03, Data-loss 1.7652e+03                  , pde-loss 3.3819e+03, initc-loss 1.6583e+04                    bc_loss 2.9569e+04\n",
      "Epoch 2600, Training-Loss 1.1586e+04, Data-loss 7.6470e+03                  , pde-loss 4.9815e+03, initc-loss 3.2578e+04                    bc_loss 3.5636e+05\n",
      "Epoch 2610, Training-Loss 8.1061e+03, Data-loss 4.6814e+03                  , pde-loss 3.6682e+03, initc-loss 9.0277e+03                    bc_loss 3.2978e+05\n",
      "Epoch 2620, Training-Loss 8.3976e+03, Data-loss 5.7275e+03                  , pde-loss 7.8573e+02, initc-loss 1.8869e+04                    bc_loss 2.4735e+05\n",
      "Epoch 2630, Training-Loss 4.4441e+03, Data-loss 2.1394e+03                  , pde-loss 4.1693e+03, initc-loss 1.6705e+04                    bc_loss 2.0959e+05\n",
      "Epoch 2640, Training-Loss 4.7756e+03, Data-loss 3.9259e+03                  , pde-loss 3.7336e+03, initc-loss 1.6016e+04                    bc_loss 6.5214e+04\n",
      "Epoch 2650, Training-Loss 6.6815e+03, Data-loss 5.5205e+03                  , pde-loss 2.7880e+03, initc-loss 1.7477e+04                    bc_loss 9.5835e+04\n",
      "Epoch 2660, Training-Loss 4.2769e+03, Data-loss 2.2948e+03                  , pde-loss 3.2934e+03, initc-loss 1.6511e+04                    bc_loss 1.7840e+05\n",
      "Epoch 2670, Training-Loss 3.9399e+03, Data-loss 2.2646e+03                  , pde-loss 9.0881e+02, initc-loss 1.5225e+04                    bc_loss 1.5140e+05\n",
      "Epoch 2680, Training-Loss 8.3027e+03, Data-loss 5.6652e+03                  , pde-loss 6.3715e+02, initc-loss 2.1826e+04                    bc_loss 2.4129e+05\n",
      "Epoch 2690, Training-Loss 6.2914e+03, Data-loss 4.7711e+03                  , pde-loss 2.0720e+03, initc-loss 1.8975e+04                    bc_loss 1.3098e+05\n",
      "Epoch 2700, Training-Loss 3.9093e+03, Data-loss 3.0725e+03                  , pde-loss 3.5100e+03, initc-loss 1.4755e+04                    bc_loss 6.5415e+04\n",
      "Epoch 2710, Training-Loss 3.1147e+03, Data-loss 2.2185e+03                  , pde-loss 7.5671e+02, initc-loss 1.0818e+04                    bc_loss 7.8042e+04\n",
      "Epoch 2720, Training-Loss 6.7787e+03, Data-loss 3.3720e+03                  , pde-loss 7.7228e+02, initc-loss 1.8081e+04                    bc_loss 3.2182e+05\n",
      "Epoch 2730, Training-Loss 3.4280e+03, Data-loss 2.9799e+03                  , pde-loss 1.3246e+03, initc-loss 1.1951e+04                    bc_loss 3.1535e+04\n",
      "Epoch 2740, Training-Loss 4.1007e+03, Data-loss 3.8080e+03                  , pde-loss 2.5613e+03, initc-loss 1.2595e+04                    bc_loss 1.4107e+04\n",
      "Epoch 2750, Training-Loss 3.0324e+03, Data-loss 2.4638e+03                  , pde-loss 2.3370e+03, initc-loss 1.4932e+04                    bc_loss 3.9586e+04\n",
      "Epoch 2760, Training-Loss 5.9136e+03, Data-loss 3.2127e+03                  , pde-loss 1.2402e+03, initc-loss 2.1302e+04                    bc_loss 2.4755e+05\n",
      "Epoch 2770, Training-Loss 4.9378e+03, Data-loss 3.2895e+03                  , pde-loss 1.2679e+03, initc-loss 1.5092e+04                    bc_loss 1.4847e+05\n",
      "Epoch 2780, Training-Loss 7.4961e+03, Data-loss 5.0900e+03                  , pde-loss 6.6795e+03, initc-loss 2.9349e+04                    bc_loss 2.0458e+05\n",
      "Epoch 2790, Training-Loss 5.2304e+03, Data-loss 3.0633e+03                  , pde-loss 1.7276e+03, initc-loss 1.5563e+04                    bc_loss 1.9942e+05\n",
      "Epoch 2800, Training-Loss 1.0270e+04, Data-loss 7.3969e+03                  , pde-loss 1.5028e+03, initc-loss 1.8875e+04                    bc_loss 2.6691e+05\n",
      "Epoch 2810, Training-Loss 2.9781e+03, Data-loss 1.8167e+03                  , pde-loss 2.6783e+03, initc-loss 1.5367e+04                    bc_loss 9.8096e+04\n",
      "Epoch 2820, Training-Loss 3.2162e+03, Data-loss 2.7206e+03                  , pde-loss 2.9853e+03, initc-loss 1.3323e+04                    bc_loss 3.3258e+04\n",
      "Epoch 2830, Training-Loss 1.8541e+03, Data-loss 1.2394e+03                  , pde-loss 2.6248e+03, initc-loss 1.7198e+04                    bc_loss 4.1646e+04\n",
      "Epoch 2840, Training-Loss 5.0807e+03, Data-loss 2.4018e+03                  , pde-loss 1.7618e+03, initc-loss 1.7349e+04                    bc_loss 2.4878e+05\n",
      "Epoch 2850, Training-Loss 3.0268e+03, Data-loss 2.2316e+03                  , pde-loss 2.3735e+03, initc-loss 7.0170e+03                    bc_loss 7.0126e+04\n",
      "Epoch 2860, Training-Loss 3.0390e+03, Data-loss 1.8464e+03                  , pde-loss 2.2151e+03, initc-loss 1.2682e+04                    bc_loss 1.0436e+05\n",
      "Epoch 2870, Training-Loss 2.2101e+03, Data-loss 1.6421e+03                  , pde-loss 1.8716e+03, initc-loss 1.4488e+04                    bc_loss 4.0436e+04\n",
      "Epoch 2880, Training-Loss 1.6212e+04, Data-loss 4.0624e+03                  , pde-loss 2.6094e+03, initc-loss 2.7250e+04                    bc_loss 1.1851e+06\n",
      "Epoch 2890, Training-Loss 1.0364e+04, Data-loss 8.8461e+03                  , pde-loss 5.8298e+03, initc-loss 1.6533e+04                    bc_loss 1.2947e+05\n",
      "Epoch 2900, Training-Loss 4.9303e+03, Data-loss 3.5633e+03                  , pde-loss 9.7094e+02, initc-loss 1.3609e+04                    bc_loss 1.2212e+05\n",
      "Epoch 2910, Training-Loss 5.3614e+03, Data-loss 3.0336e+03                  , pde-loss 1.5469e+03, initc-loss 1.6649e+04                    bc_loss 2.1459e+05\n",
      "Epoch 2920, Training-Loss 5.0331e+03, Data-loss 3.3997e+03                  , pde-loss 5.7681e+03, initc-loss 2.2519e+04                    bc_loss 1.3505e+05\n",
      "Epoch 2930, Training-Loss 6.9986e+03, Data-loss 4.4227e+03                  , pde-loss 5.6924e+03, initc-loss 9.9566e+03                    bc_loss 2.4194e+05\n",
      "Epoch 2940, Training-Loss 6.4975e+03, Data-loss 4.3640e+03                  , pde-loss 8.8229e+02, initc-loss 1.1322e+04                    bc_loss 2.0114e+05\n",
      "Epoch 2950, Training-Loss 1.0711e+04, Data-loss 7.3630e+03                  , pde-loss 9.1070e+02, initc-loss 2.0546e+04                    bc_loss 3.1330e+05\n",
      "Epoch 2960, Training-Loss 4.8640e+03, Data-loss 2.8958e+03                  , pde-loss 1.5421e+03, initc-loss 2.0342e+04                    bc_loss 1.7493e+05\n",
      "Epoch 2970, Training-Loss 3.6393e+03, Data-loss 2.3269e+03                  , pde-loss 1.1933e+03, initc-loss 1.4075e+04                    bc_loss 1.1598e+05\n",
      "Epoch 2980, Training-Loss 2.6427e+03, Data-loss 1.7809e+03                  , pde-loss 2.4768e+03, initc-loss 1.2580e+04                    bc_loss 7.1123e+04\n",
      "Epoch 2990, Training-Loss 8.2059e+03, Data-loss 2.1775e+03                  , pde-loss 1.2427e+03, initc-loss 1.6675e+04                    bc_loss 5.8493e+05\n",
      "Epoch 3000, Training-Loss 1.0995e+04, Data-loss 5.9780e+03                  , pde-loss 8.6405e+02, initc-loss 1.6317e+04                    bc_loss 4.8457e+05\n",
      "Epoch 3010, Training-Loss 4.9104e+03, Data-loss 2.5933e+03                  , pde-loss 4.2888e+03, initc-loss 1.7804e+04                    bc_loss 2.0962e+05\n",
      "Epoch 3020, Training-Loss 5.8222e+03, Data-loss 3.9215e+03                  , pde-loss 3.2474e+03, initc-loss 1.8191e+04                    bc_loss 1.6863e+05\n",
      "Epoch 3030, Training-Loss 4.6853e+03, Data-loss 3.9403e+03                  , pde-loss 9.4008e+02, initc-loss 1.7321e+04                    bc_loss 5.6243e+04\n",
      "Epoch 3040, Training-Loss 2.3789e+03, Data-loss 1.8309e+03                  , pde-loss 1.9216e+03, initc-loss 1.7343e+04                    bc_loss 3.5543e+04\n",
      "Epoch 3050, Training-Loss 3.0516e+03, Data-loss 2.0991e+03                  , pde-loss 2.6382e+03, initc-loss 1.2998e+04                    bc_loss 7.9614e+04\n",
      "Epoch 3060, Training-Loss 4.3309e+03, Data-loss 2.7310e+03                  , pde-loss 4.5678e+03, initc-loss 1.6062e+04                    bc_loss 1.3936e+05\n",
      "Epoch 3070, Training-Loss 1.1002e+04, Data-loss 6.1451e+03                  , pde-loss 4.7992e+03, initc-loss 2.9034e+04                    bc_loss 4.5189e+05\n",
      "Epoch 3080, Training-Loss 1.0985e+04, Data-loss 5.7328e+03                  , pde-loss 3.2305e+03, initc-loss 1.3783e+04                    bc_loss 5.0823e+05\n",
      "Epoch 3090, Training-Loss 9.3816e+03, Data-loss 6.4377e+03                  , pde-loss 7.5253e+02, initc-loss 2.0688e+04                    bc_loss 2.7294e+05\n",
      "Epoch 3100, Training-Loss 4.1923e+03, Data-loss 2.6485e+03                  , pde-loss 2.3721e+03, initc-loss 2.0724e+04                    bc_loss 1.3129e+05\n",
      "Epoch 3110, Training-Loss 5.7243e+03, Data-loss 4.6845e+03                  , pde-loss 2.9808e+03, initc-loss 1.6720e+04                    bc_loss 8.4283e+04\n",
      "Epoch 3120, Training-Loss 5.1851e+03, Data-loss 2.5336e+03                  , pde-loss 3.1689e+03, initc-loss 1.5055e+04                    bc_loss 2.4693e+05\n",
      "Epoch 3130, Training-Loss 5.2889e+03, Data-loss 2.0626e+03                  , pde-loss 1.7309e+03, initc-loss 9.8316e+03                    bc_loss 3.1106e+05\n",
      "Epoch 3140, Training-Loss 7.7933e+03, Data-loss 5.7812e+03                  , pde-loss 8.3400e+02, initc-loss 1.7273e+04                    bc_loss 1.8311e+05\n",
      "Epoch 3150, Training-Loss 6.6272e+03, Data-loss 4.9856e+03                  , pde-loss 3.9162e+02, initc-loss 1.4900e+04                    bc_loss 1.4887e+05\n",
      "Epoch 3160, Training-Loss 2.9640e+03, Data-loss 2.2363e+03                  , pde-loss 2.5851e+03, initc-loss 1.9456e+04                    bc_loss 5.0729e+04\n",
      "Epoch 3170, Training-Loss 4.2395e+03, Data-loss 2.8180e+03                  , pde-loss 1.7878e+03, initc-loss 1.9357e+04                    bc_loss 1.2100e+05\n",
      "Epoch 3180, Training-Loss 4.3949e+03, Data-loss 1.7690e+03                  , pde-loss 6.7015e+02, initc-loss 1.9018e+04                    bc_loss 2.4291e+05\n",
      "Epoch 3190, Training-Loss 5.7887e+03, Data-loss 4.6922e+03                  , pde-loss 1.1271e+03, initc-loss 1.6363e+04                    bc_loss 9.2163e+04\n",
      "Epoch 3200, Training-Loss 5.0729e+03, Data-loss 4.2858e+03                  , pde-loss 3.0546e+03, initc-loss 9.4623e+03                    bc_loss 6.6190e+04\n",
      "Epoch 3210, Training-Loss 4.7611e+03, Data-loss 3.0425e+03                  , pde-loss 3.9853e+03, initc-loss 2.1005e+04                    bc_loss 1.4687e+05\n",
      "Epoch 3220, Training-Loss 1.1422e+04, Data-loss 3.1273e+03                  , pde-loss 8.4232e+02, initc-loss 2.0490e+04                    bc_loss 8.0817e+05\n",
      "Epoch 3230, Training-Loss 8.9370e+03, Data-loss 6.9062e+03                  , pde-loss 1.0646e+03, initc-loss 1.4961e+04                    bc_loss 1.8706e+05\n",
      "Epoch 3240, Training-Loss 4.7827e+03, Data-loss 3.3676e+03                  , pde-loss 5.7069e+03, initc-loss 1.2257e+04                    bc_loss 1.2355e+05\n",
      "Epoch 3250, Training-Loss 3.4921e+03, Data-loss 2.8205e+03                  , pde-loss 4.0607e+03, initc-loss 1.6133e+04                    bc_loss 4.6965e+04\n",
      "Epoch 3260, Training-Loss 1.0257e+04, Data-loss 7.2281e+03                  , pde-loss 2.0954e+03, initc-loss 2.1436e+04                    bc_loss 2.7931e+05\n",
      "Epoch 3270, Training-Loss 7.8557e+03, Data-loss 6.3926e+03                  , pde-loss 2.1727e+03, initc-loss 2.7503e+04                    bc_loss 1.1663e+05\n",
      "Epoch 3280, Training-Loss 6.8387e+03, Data-loss 6.0724e+03                  , pde-loss 1.9138e+03, initc-loss 2.0473e+04                    bc_loss 5.4235e+04\n",
      "Epoch 3290, Training-Loss 6.1265e+03, Data-loss 3.6144e+03                  , pde-loss 1.4185e+03, initc-loss 1.2491e+04                    bc_loss 2.3730e+05\n",
      "Epoch 3300, Training-Loss 6.3262e+03, Data-loss 5.3270e+03                  , pde-loss 9.0986e+02, initc-loss 1.6668e+04                    bc_loss 8.2343e+04\n",
      "Epoch 3310, Training-Loss 1.0729e+04, Data-loss 3.6377e+03                  , pde-loss 2.0729e+03, initc-loss 1.9572e+04                    bc_loss 6.8744e+05\n",
      "Epoch 3320, Training-Loss 6.7259e+03, Data-loss 4.2989e+03                  , pde-loss 6.1958e+03, initc-loss 1.4947e+04                    bc_loss 2.2156e+05\n",
      "Epoch 3330, Training-Loss 1.2945e+04, Data-loss 3.3718e+03                  , pde-loss 6.3777e+02, initc-loss 1.5312e+04                    bc_loss 9.4136e+05\n",
      "Epoch 3340, Training-Loss 1.1489e+04, Data-loss 6.0007e+03                  , pde-loss 9.4819e+02, initc-loss 1.2985e+04                    bc_loss 5.3486e+05\n",
      "Epoch 3350, Training-Loss 6.8513e+03, Data-loss 5.0908e+03                  , pde-loss 7.6311e+03, initc-loss 1.3432e+04                    bc_loss 1.5498e+05\n",
      "Epoch 3360, Training-Loss 3.6341e+03, Data-loss 3.1274e+03                  , pde-loss 3.0827e+03, initc-loss 1.4079e+04                    bc_loss 3.3512e+04\n",
      "Epoch 3370, Training-Loss 3.3580e+03, Data-loss 2.0102e+03                  , pde-loss 2.0947e+03, initc-loss 1.4586e+04                    bc_loss 1.1810e+05\n",
      "Epoch 3380, Training-Loss 5.8251e+03, Data-loss 2.8625e+03                  , pde-loss 3.2053e+03, initc-loss 1.6688e+04                    bc_loss 2.7636e+05\n",
      "Epoch 3390, Training-Loss 4.6848e+03, Data-loss 3.1644e+03                  , pde-loss 3.2334e+03, initc-loss 1.7969e+04                    bc_loss 1.3084e+05\n",
      "Epoch 3400, Training-Loss 2.7326e+03, Data-loss 1.4872e+03                  , pde-loss 2.1289e+03, initc-loss 1.5751e+04                    bc_loss 1.0665e+05\n",
      "Epoch 3410, Training-Loss 4.6657e+03, Data-loss 3.3398e+03                  , pde-loss 4.8318e+02, initc-loss 1.5986e+04                    bc_loss 1.1611e+05\n",
      "Epoch 3420, Training-Loss 6.9274e+03, Data-loss 4.0710e+03                  , pde-loss 6.4509e+02, initc-loss 1.7311e+04                    bc_loss 2.6769e+05\n",
      "Epoch 3430, Training-Loss 5.5026e+03, Data-loss 4.6237e+03                  , pde-loss 7.5947e+02, initc-loss 1.2805e+04                    bc_loss 7.4326e+04\n",
      "Epoch 3440, Training-Loss 4.5945e+03, Data-loss 3.8696e+03                  , pde-loss 3.1277e+03, initc-loss 2.4863e+04                    bc_loss 4.4497e+04\n",
      "Epoch 3450, Training-Loss 8.1073e+03, Data-loss 6.1271e+03                  , pde-loss 3.6080e+03, initc-loss 2.3264e+04                    bc_loss 1.7115e+05\n",
      "Epoch 3460, Training-Loss 1.3323e+04, Data-loss 4.1860e+03                  , pde-loss 2.3083e+03, initc-loss 1.3179e+04                    bc_loss 8.9823e+05\n",
      "Epoch 3470, Training-Loss 1.4399e+04, Data-loss 1.2817e+04                  , pde-loss 9.6516e+02, initc-loss 8.2009e+03                    bc_loss 1.4896e+05\n",
      "Epoch 3480, Training-Loss 5.0927e+03, Data-loss 3.2769e+03                  , pde-loss 3.1249e+03, initc-loss 2.0084e+04                    bc_loss 1.5838e+05\n",
      "Epoch 3490, Training-Loss 5.8002e+03, Data-loss 4.5457e+03                  , pde-loss 5.8339e+03, initc-loss 2.3319e+04                    bc_loss 9.6296e+04\n",
      "Epoch 3500, Training-Loss 3.0512e+03, Data-loss 2.6194e+03                  , pde-loss 8.6855e+02, initc-loss 1.7246e+04                    bc_loss 2.5065e+04\n",
      "Epoch 3510, Training-Loss 2.3086e+03, Data-loss 1.4430e+03                  , pde-loss 1.2538e+03, initc-loss 1.5373e+04                    bc_loss 6.9929e+04\n",
      "Epoch 3520, Training-Loss 3.8722e+03, Data-loss 2.7840e+03                  , pde-loss 7.3611e+02, initc-loss 2.0096e+04                    bc_loss 8.7988e+04\n",
      "Epoch 3530, Training-Loss 3.3332e+03, Data-loss 2.8663e+03                  , pde-loss 1.9098e+03, initc-loss 2.5602e+04                    bc_loss 1.9182e+04\n",
      "Epoch 3540, Training-Loss 6.1629e+03, Data-loss 4.5401e+03                  , pde-loss 2.9432e+03, initc-loss 2.1108e+04                    bc_loss 1.3823e+05\n",
      "Epoch 3550, Training-Loss 8.0152e+03, Data-loss 5.7503e+03                  , pde-loss 2.6009e+03, initc-loss 2.6007e+04                    bc_loss 1.9788e+05\n",
      "Epoch 3560, Training-Loss 5.5678e+03, Data-loss 3.5143e+03                  , pde-loss 1.7252e+03, initc-loss 1.8190e+04                    bc_loss 1.8543e+05\n",
      "Epoch 3570, Training-Loss 3.5263e+03, Data-loss 2.4703e+03                  , pde-loss 2.1249e+03, initc-loss 1.9454e+04                    bc_loss 8.4026e+04\n",
      "Epoch 3580, Training-Loss 3.5210e+03, Data-loss 2.4300e+03                  , pde-loss 2.2648e+03, initc-loss 1.5007e+04                    bc_loss 9.1831e+04\n",
      "Epoch 3590, Training-Loss 7.4770e+03, Data-loss 4.7654e+03                  , pde-loss 2.5707e+03, initc-loss 1.9630e+04                    bc_loss 2.4896e+05\n",
      "Epoch 3600, Training-Loss 9.8541e+03, Data-loss 8.5193e+03                  , pde-loss 1.6040e+03, initc-loss 2.5543e+04                    bc_loss 1.0634e+05\n",
      "Epoch 3610, Training-Loss 4.8347e+03, Data-loss 3.8543e+03                  , pde-loss 1.3974e+03, initc-loss 2.3457e+04                    bc_loss 7.3187e+04\n",
      "Epoch 3620, Training-Loss 3.7978e+03, Data-loss 3.2406e+03                  , pde-loss 1.6044e+03, initc-loss 1.9199e+04                    bc_loss 3.4913e+04\n",
      "Epoch 3630, Training-Loss 5.9848e+03, Data-loss 2.3185e+03                  , pde-loss 2.9229e+03, initc-loss 2.2752e+04                    bc_loss 3.4096e+05\n",
      "Epoch 3640, Training-Loss 6.3047e+03, Data-loss 5.5141e+03                  , pde-loss 2.1201e+03, initc-loss 1.9826e+04                    bc_loss 5.7119e+04\n",
      "Epoch 3650, Training-Loss 5.3072e+03, Data-loss 4.0459e+03                  , pde-loss 2.1412e+03, initc-loss 1.9839e+04                    bc_loss 1.0416e+05\n",
      "Epoch 3660, Training-Loss 6.0784e+03, Data-loss 3.5738e+03                  , pde-loss 1.1624e+03, initc-loss 2.3839e+04                    bc_loss 2.2546e+05\n",
      "Epoch 3670, Training-Loss 7.3830e+03, Data-loss 5.8525e+03                  , pde-loss 9.3094e+02, initc-loss 1.4955e+04                    bc_loss 1.3717e+05\n",
      "Epoch 3680, Training-Loss 5.6169e+03, Data-loss 4.6655e+03                  , pde-loss 3.8383e+03, initc-loss 1.9253e+04                    bc_loss 7.2053e+04\n",
      "Epoch 3690, Training-Loss 6.8723e+03, Data-loss 4.0535e+03                  , pde-loss 3.3181e+03, initc-loss 1.4304e+04                    bc_loss 2.6425e+05\n",
      "Epoch 3700, Training-Loss 4.7737e+03, Data-loss 3.9778e+03                  , pde-loss 8.1066e+02, initc-loss 1.7881e+04                    bc_loss 6.0896e+04\n",
      "Epoch 3710, Training-Loss 4.9750e+03, Data-loss 2.0457e+03                  , pde-loss 1.0425e+03, initc-loss 1.9205e+04                    bc_loss 2.7268e+05\n",
      "Epoch 3720, Training-Loss 2.9202e+03, Data-loss 1.8375e+03                  , pde-loss 2.1234e+03, initc-loss 1.5043e+04                    bc_loss 9.1107e+04\n",
      "Epoch 3730, Training-Loss 7.3932e+03, Data-loss 5.3255e+03                  , pde-loss 5.7830e+03, initc-loss 2.3175e+04                    bc_loss 1.7782e+05\n",
      "Epoch 3740, Training-Loss 4.6302e+03, Data-loss 3.3431e+03                  , pde-loss 1.3144e+03, initc-loss 1.2771e+04                    bc_loss 1.1463e+05\n",
      "Epoch 3750, Training-Loss 1.0198e+04, Data-loss 6.8382e+03                  , pde-loss 7.2753e+02, initc-loss 2.8847e+04                    bc_loss 3.0635e+05\n",
      "Epoch 3760, Training-Loss 3.7056e+03, Data-loss 2.2484e+03                  , pde-loss 3.5609e+03, initc-loss 1.1874e+04                    bc_loss 1.3028e+05\n",
      "Epoch 3770, Training-Loss 5.6769e+03, Data-loss 4.5799e+03                  , pde-loss 4.3771e+03, initc-loss 1.4515e+04                    bc_loss 9.0801e+04\n",
      "Epoch 3780, Training-Loss 2.5424e+03, Data-loss 2.0360e+03                  , pde-loss 1.3373e+03, initc-loss 1.3613e+04                    bc_loss 3.5696e+04\n",
      "Epoch 3790, Training-Loss 1.6542e+03, Data-loss 1.1876e+03                  , pde-loss 1.0274e+03, initc-loss 1.8463e+04                    bc_loss 2.7170e+04\n",
      "Epoch 3800, Training-Loss 2.5002e+03, Data-loss 2.1595e+03                  , pde-loss 1.3699e+03, initc-loss 1.6684e+04                    bc_loss 1.6017e+04\n",
      "Epoch 3810, Training-Loss 2.1353e+03, Data-loss 1.7222e+03                  , pde-loss 1.5464e+03, initc-loss 1.3726e+04                    bc_loss 2.6036e+04\n",
      "Epoch 3820, Training-Loss 8.0925e+03, Data-loss 4.8015e+03                  , pde-loss 3.6604e+03, initc-loss 2.9614e+04                    bc_loss 2.9583e+05\n",
      "Epoch 3830, Training-Loss 1.7365e+04, Data-loss 3.9930e+03                  , pde-loss 3.2189e+03, initc-loss 1.4933e+04                    bc_loss 1.3190e+06\n",
      "Epoch 3840, Training-Loss 9.7452e+03, Data-loss 8.7099e+03                  , pde-loss 6.0016e+02, initc-loss 1.8089e+04                    bc_loss 8.4849e+04\n",
      "Epoch 3850, Training-Loss 5.5790e+03, Data-loss 2.9218e+03                  , pde-loss 2.1295e+03, initc-loss 1.6263e+04                    bc_loss 2.4732e+05\n",
      "Epoch 3860, Training-Loss 7.5004e+03, Data-loss 5.8351e+03                  , pde-loss 5.4725e+03, initc-loss 2.3034e+04                    bc_loss 1.3803e+05\n",
      "Epoch 3870, Training-Loss 3.5969e+03, Data-loss 2.9971e+03                  , pde-loss 1.1412e+03, initc-loss 1.8279e+04                    bc_loss 4.0554e+04\n",
      "Epoch 3880, Training-Loss 4.9577e+03, Data-loss 2.4519e+03                  , pde-loss 5.2881e+02, initc-loss 1.5508e+04                    bc_loss 2.3454e+05\n",
      "Epoch 3890, Training-Loss 4.1493e+03, Data-loss 2.9463e+03                  , pde-loss 6.7843e+02, initc-loss 1.3193e+04                    bc_loss 1.0643e+05\n",
      "Epoch 3900, Training-Loss 3.4877e+03, Data-loss 2.7896e+03                  , pde-loss 2.7731e+03, initc-loss 1.9576e+04                    bc_loss 4.7461e+04\n",
      "Epoch 3910, Training-Loss 4.2729e+03, Data-loss 3.1876e+03                  , pde-loss 2.0435e+03, initc-loss 2.1255e+04                    bc_loss 8.5238e+04\n",
      "Epoch 3920, Training-Loss 6.8526e+03, Data-loss 3.4055e+03                  , pde-loss 9.2206e+02, initc-loss 1.9528e+04                    bc_loss 3.2425e+05\n",
      "Epoch 3930, Training-Loss 7.8976e+03, Data-loss 4.2364e+03                  , pde-loss 1.5699e+03, initc-loss 1.1051e+04                    bc_loss 3.5351e+05\n",
      "Epoch 3940, Training-Loss 8.9128e+03, Data-loss 4.5478e+03                  , pde-loss 6.5821e+03, initc-loss 2.2939e+04                    bc_loss 4.0698e+05\n",
      "Epoch 3950, Training-Loss 6.0269e+03, Data-loss 4.4590e+03                  , pde-loss 4.2719e+03, initc-loss 1.7820e+04                    bc_loss 1.3470e+05\n",
      "Epoch 3960, Training-Loss 3.4244e+03, Data-loss 2.7939e+03                  , pde-loss 7.4220e+02, initc-loss 1.9088e+04                    bc_loss 4.3219e+04\n",
      "Epoch 3970, Training-Loss 3.3083e+03, Data-loss 2.7276e+03                  , pde-loss 6.3600e+02, initc-loss 2.2196e+04                    bc_loss 3.5233e+04\n",
      "Epoch 3980, Training-Loss 4.2196e+03, Data-loss 2.6358e+03                  , pde-loss 1.8601e+03, initc-loss 1.5355e+04                    bc_loss 1.4117e+05\n",
      "Epoch 3990, Training-Loss 4.9409e+03, Data-loss 3.7898e+03                  , pde-loss 1.0368e+03, initc-loss 2.0591e+04                    bc_loss 9.3484e+04\n",
      "Epoch 4000, Training-Loss 3.0391e+03, Data-loss 2.3557e+03                  , pde-loss 1.8176e+03, initc-loss 1.7549e+04                    bc_loss 4.8971e+04\n",
      "Epoch 4010, Training-Loss 9.8945e+03, Data-loss 3.6791e+03                  , pde-loss 3.2088e+03, initc-loss 2.1713e+04                    bc_loss 5.9661e+05\n",
      "Epoch 4020, Training-Loss 8.7006e+03, Data-loss 6.7117e+03                  , pde-loss 5.1556e+03, initc-loss 1.5173e+04                    bc_loss 1.7856e+05\n",
      "Epoch 4030, Training-Loss 1.0578e+04, Data-loss 5.0478e+03                  , pde-loss 1.1934e+03, initc-loss 1.0034e+04                    bc_loss 5.4178e+05\n",
      "Epoch 4040, Training-Loss 9.9587e+03, Data-loss 6.0844e+03                  , pde-loss 7.7573e+02, initc-loss 1.4742e+04                    bc_loss 3.7191e+05\n",
      "Epoch 4050, Training-Loss 3.9305e+03, Data-loss 2.9289e+03                  , pde-loss 3.7143e+03, initc-loss 2.2038e+04                    bc_loss 7.4402e+04\n",
      "Epoch 4060, Training-Loss 3.5140e+03, Data-loss 2.5910e+03                  , pde-loss 3.3718e+03, initc-loss 1.7103e+04                    bc_loss 7.1831e+04\n",
      "Epoch 4070, Training-Loss 2.3958e+03, Data-loss 1.8685e+03                  , pde-loss 1.7928e+03, initc-loss 1.5333e+04                    bc_loss 3.5608e+04\n",
      "Epoch 4080, Training-Loss 3.7178e+03, Data-loss 2.3761e+03                  , pde-loss 1.8513e+03, initc-loss 1.4080e+04                    bc_loss 1.1824e+05\n",
      "Epoch 4090, Training-Loss 3.8661e+03, Data-loss 2.8606e+03                  , pde-loss 1.5477e+03, initc-loss 2.0014e+04                    bc_loss 7.8990e+04\n",
      "Epoch 4100, Training-Loss 1.0500e+04, Data-loss 9.1970e+03                  , pde-loss 1.5051e+03, initc-loss 3.8263e+04                    bc_loss 9.0480e+04\n",
      "Epoch 4110, Training-Loss 9.4454e+03, Data-loss 7.9857e+03                  , pde-loss 3.0030e+03, initc-loss 1.5939e+04                    bc_loss 1.2704e+05\n",
      "Epoch 4120, Training-Loss 4.9081e+03, Data-loss 4.1253e+03                  , pde-loss 8.0491e+02, initc-loss 1.4949e+04                    bc_loss 6.2526e+04\n",
      "Epoch 4130, Training-Loss 5.2531e+03, Data-loss 4.2424e+03                  , pde-loss 1.1890e+03, initc-loss 2.3170e+04                    bc_loss 7.6713e+04\n",
      "Epoch 4140, Training-Loss 3.6064e+03, Data-loss 2.6649e+03                  , pde-loss 2.9152e+03, initc-loss 2.3868e+04                    bc_loss 6.7366e+04\n",
      "Epoch 4150, Training-Loss 1.3494e+04, Data-loss 8.3892e+03                  , pde-loss 2.6501e+03, initc-loss 3.1469e+04                    bc_loss 4.7636e+05\n",
      "Epoch 4160, Training-Loss 9.7249e+03, Data-loss 8.0036e+03                  , pde-loss 1.6193e+03, initc-loss 1.1276e+04                    bc_loss 1.5924e+05\n",
      "Epoch 4170, Training-Loss 8.4388e+03, Data-loss 6.7537e+03                  , pde-loss 7.5719e+02, initc-loss 1.5998e+04                    bc_loss 1.5175e+05\n",
      "Epoch 4180, Training-Loss 5.4339e+03, Data-loss 4.0287e+03                  , pde-loss 1.8027e+03, initc-loss 2.5139e+04                    bc_loss 1.1358e+05\n",
      "Epoch 4190, Training-Loss 4.9909e+03, Data-loss 3.6913e+03                  , pde-loss 3.8093e+03, initc-loss 1.8142e+04                    bc_loss 1.0801e+05\n",
      "Epoch 4200, Training-Loss 4.3291e+03, Data-loss 2.1251e+03                  , pde-loss 2.7286e+03, initc-loss 1.0514e+04                    bc_loss 2.0716e+05\n",
      "Epoch 4210, Training-Loss 1.0039e+04, Data-loss 7.9699e+03                  , pde-loss 3.4868e+02, initc-loss 2.3638e+04                    bc_loss 1.8297e+05\n",
      "Epoch 4220, Training-Loss 4.7781e+03, Data-loss 3.3583e+03                  , pde-loss 7.3981e+02, initc-loss 1.7965e+04                    bc_loss 1.2327e+05\n",
      "Epoch 4230, Training-Loss 5.3772e+03, Data-loss 4.3157e+03                  , pde-loss 3.4154e+03, initc-loss 1.9818e+04                    bc_loss 8.2914e+04\n",
      "Epoch 4240, Training-Loss 2.3037e+03, Data-loss 1.6447e+03                  , pde-loss 2.0290e+03, initc-loss 1.7667e+04                    bc_loss 4.6207e+04\n",
      "Epoch 4250, Training-Loss 2.9278e+03, Data-loss 1.4037e+03                  , pde-loss 1.3449e+03, initc-loss 1.4908e+04                    bc_loss 1.3617e+05\n",
      "Epoch 4260, Training-Loss 4.4222e+03, Data-loss 3.0114e+03                  , pde-loss 7.6815e+02, initc-loss 1.7362e+04                    bc_loss 1.2294e+05\n",
      "Epoch 4270, Training-Loss 4.7508e+03, Data-loss 4.3624e+03                  , pde-loss 1.5361e+03, initc-loss 1.8329e+04                    bc_loss 1.8981e+04\n",
      "Epoch 4280, Training-Loss 3.2884e+03, Data-loss 2.6323e+03                  , pde-loss 3.4579e+03, initc-loss 1.5509e+04                    bc_loss 4.6641e+04\n",
      "Epoch 4290, Training-Loss 7.4074e+03, Data-loss 1.8192e+03                  , pde-loss 3.3806e+03, initc-loss 1.6982e+04                    bc_loss 5.3845e+05\n",
      "Epoch 4300, Training-Loss 6.1314e+03, Data-loss 3.8308e+03                  , pde-loss 4.9935e+03, initc-loss 1.3230e+04                    bc_loss 2.1184e+05\n",
      "Epoch 4310, Training-Loss 1.5160e+04, Data-loss 8.7094e+03                  , pde-loss 6.5467e+02, initc-loss 2.7065e+04                    bc_loss 6.1734e+05\n",
      "Epoch 4320, Training-Loss 7.7668e+03, Data-loss 4.1407e+03                  , pde-loss 3.9949e+02, initc-loss 1.5654e+04                    bc_loss 3.4656e+05\n",
      "Epoch 4330, Training-Loss 4.4116e+03, Data-loss 3.3320e+03                  , pde-loss 5.6040e+03, initc-loss 2.0820e+04                    bc_loss 8.1540e+04\n",
      "Epoch 4340, Training-Loss 3.3815e+03, Data-loss 2.9049e+03                  , pde-loss 2.5438e+03, initc-loss 1.5292e+04                    bc_loss 2.9831e+04\n",
      "Epoch 4350, Training-Loss 4.4806e+03, Data-loss 3.2372e+03                  , pde-loss 1.6308e+03, initc-loss 1.7976e+04                    bc_loss 1.0473e+05\n",
      "Epoch 4360, Training-Loss 7.7830e+03, Data-loss 4.5694e+03                  , pde-loss 2.9629e+03, initc-loss 1.8117e+04                    bc_loss 3.0028e+05\n",
      "Epoch 4370, Training-Loss 2.8359e+03, Data-loss 2.2725e+03                  , pde-loss 1.0822e+03, initc-loss 1.2694e+04                    bc_loss 4.2570e+04\n",
      "Epoch 4380, Training-Loss 6.8450e+03, Data-loss 3.4340e+03                  , pde-loss 3.3307e+02, initc-loss 2.1867e+04                    bc_loss 3.1889e+05\n",
      "Epoch 4390, Training-Loss 2.3932e+03, Data-loss 1.3897e+03                  , pde-loss 1.4355e+03, initc-loss 1.2961e+04                    bc_loss 8.5954e+04\n",
      "Epoch 4400, Training-Loss 2.9404e+03, Data-loss 2.1569e+03                  , pde-loss 2.6199e+03, initc-loss 1.2865e+04                    bc_loss 6.2862e+04\n",
      "Epoch 4410, Training-Loss 6.1099e+03, Data-loss 2.1268e+03                  , pde-loss 1.2492e+03, initc-loss 1.6427e+04                    bc_loss 3.8063e+05\n",
      "Epoch 4420, Training-Loss 6.0505e+03, Data-loss 3.6932e+03                  , pde-loss 3.8653e+02, initc-loss 1.7367e+04                    bc_loss 2.1798e+05\n",
      "Epoch 4430, Training-Loss 4.0648e+03, Data-loss 3.5006e+03                  , pde-loss 3.1939e+03, initc-loss 2.0836e+04                    bc_loss 3.2391e+04\n",
      "Epoch 4440, Training-Loss 3.6285e+03, Data-loss 3.1947e+03                  , pde-loss 2.8905e+03, initc-loss 1.9652e+04                    bc_loss 2.0842e+04\n",
      "Epoch 4450, Training-Loss 2.9092e+03, Data-loss 2.1579e+03                  , pde-loss 1.3193e+03, initc-loss 1.3377e+04                    bc_loss 6.0432e+04\n",
      "Epoch 4460, Training-Loss 1.1535e+04, Data-loss 6.8731e+03                  , pde-loss 2.5475e+03, initc-loss 2.9110e+04                    bc_loss 4.3453e+05\n",
      "Epoch 4470, Training-Loss 6.9642e+03, Data-loss 3.9266e+03                  , pde-loss 4.4701e+03, initc-loss 1.3259e+04                    bc_loss 2.8604e+05\n",
      "Epoch 4480, Training-Loss 9.6821e+03, Data-loss 4.1722e+03                  , pde-loss 4.1141e+02, initc-loss 1.6660e+04                    bc_loss 5.3392e+05\n",
      "Epoch 4490, Training-Loss 8.1720e+03, Data-loss 5.7971e+03                  , pde-loss 5.7193e+02, initc-loss 2.1703e+04                    bc_loss 2.1522e+05\n",
      "Epoch 4500, Training-Loss 5.9583e+03, Data-loss 4.3752e+03                  , pde-loss 4.3191e+03, initc-loss 1.9942e+04                    bc_loss 1.3404e+05\n",
      "Epoch 4510, Training-Loss 4.8372e+03, Data-loss 4.1296e+03                  , pde-loss 5.1308e+03, initc-loss 2.1025e+04                    bc_loss 4.4612e+04\n",
      "Epoch 4520, Training-Loss 3.2047e+03, Data-loss 2.4583e+03                  , pde-loss 1.8020e+03, initc-loss 1.3697e+04                    bc_loss 5.9134e+04\n",
      "Epoch 4530, Training-Loss 3.5744e+03, Data-loss 2.9429e+03                  , pde-loss 4.9873e+02, initc-loss 1.7971e+04                    bc_loss 4.4677e+04\n",
      "Epoch 4540, Training-Loss 4.6567e+03, Data-loss 2.6960e+03                  , pde-loss 1.0511e+03, initc-loss 1.6623e+04                    bc_loss 1.7839e+05\n",
      "Epoch 4550, Training-Loss 6.5429e+03, Data-loss 2.7447e+03                  , pde-loss 2.8310e+03, initc-loss 2.1091e+04                    bc_loss 3.5590e+05\n",
      "Epoch 4560, Training-Loss 3.0647e+03, Data-loss 2.0377e+03                  , pde-loss 3.7540e+03, initc-loss 1.6287e+04                    bc_loss 8.2654e+04\n",
      "Epoch 4570, Training-Loss 9.9506e+03, Data-loss 6.9399e+03                  , pde-loss 4.1965e+02, initc-loss 2.5223e+04                    bc_loss 2.7544e+05\n",
      "Epoch 4580, Training-Loss 7.6235e+03, Data-loss 4.6507e+03                  , pde-loss 5.3351e+02, initc-loss 2.0515e+04                    bc_loss 2.7623e+05\n",
      "Epoch 4590, Training-Loss 5.3870e+03, Data-loss 4.3238e+03                  , pde-loss 4.8227e+03, initc-loss 2.0988e+04                    bc_loss 8.0510e+04\n",
      "Epoch 4600, Training-Loss 5.4759e+03, Data-loss 2.6807e+03                  , pde-loss 3.6746e+03, initc-loss 2.0070e+04                    bc_loss 2.5578e+05\n",
      "Epoch 4610, Training-Loss 3.4972e+03, Data-loss 2.5045e+03                  , pde-loss 2.0422e+03, initc-loss 1.4776e+04                    bc_loss 8.2454e+04\n",
      "Epoch 4620, Training-Loss 7.4346e+03, Data-loss 3.0686e+03                  , pde-loss 8.4823e+02, initc-loss 1.0314e+04                    bc_loss 4.2544e+05\n",
      "Epoch 4630, Training-Loss 6.0196e+03, Data-loss 4.3924e+03                  , pde-loss 7.1622e+02, initc-loss 2.0597e+04                    bc_loss 1.4140e+05\n",
      "Epoch 4640, Training-Loss 2.9127e+03, Data-loss 2.2521e+03                  , pde-loss 2.3475e+03, initc-loss 2.0074e+04                    bc_loss 4.3638e+04\n",
      "Epoch 4650, Training-Loss 2.0290e+03, Data-loss 1.1006e+03                  , pde-loss 2.2270e+03, initc-loss 1.6326e+04                    bc_loss 7.4289e+04\n",
      "Epoch 4660, Training-Loss 8.1167e+03, Data-loss 4.0314e+03                  , pde-loss 9.8452e+02, initc-loss 1.9138e+04                    bc_loss 3.8841e+05\n",
      "Epoch 4670, Training-Loss 7.9825e+03, Data-loss 3.7782e+03                  , pde-loss 1.3199e+03, initc-loss 9.7282e+03                    bc_loss 4.0938e+05\n",
      "Epoch 4680, Training-Loss 3.8173e+03, Data-loss 2.3259e+03                  , pde-loss 6.8589e+03, initc-loss 1.9009e+04                    bc_loss 1.2328e+05\n",
      "Epoch 4690, Training-Loss 3.0170e+03, Data-loss 2.4171e+03                  , pde-loss 3.4026e+03, initc-loss 1.3065e+04                    bc_loss 4.3523e+04\n",
      "Epoch 4700, Training-Loss 3.9990e+03, Data-loss 3.4087e+03                  , pde-loss 9.9232e+02, initc-loss 1.7210e+04                    bc_loss 4.0828e+04\n",
      "Epoch 4710, Training-Loss 4.9014e+03, Data-loss 2.9116e+03                  , pde-loss 7.7149e+02, initc-loss 1.9639e+04                    bc_loss 1.7858e+05\n",
      "Epoch 4720, Training-Loss 1.9142e+03, Data-loss 1.5101e+03                  , pde-loss 1.0109e+03, initc-loss 1.3704e+04                    bc_loss 2.5697e+04\n",
      "Epoch 4730, Training-Loss 2.6637e+03, Data-loss 2.2522e+03                  , pde-loss 2.1815e+03, initc-loss 1.4337e+04                    bc_loss 2.4630e+04\n",
      "Epoch 4740, Training-Loss 2.9733e+03, Data-loss 1.7017e+03                  , pde-loss 1.8616e+03, initc-loss 1.4440e+04                    bc_loss 1.1086e+05\n",
      "Epoch 4750, Training-Loss 7.2124e+03, Data-loss 4.5358e+03                  , pde-loss 1.2761e+03, initc-loss 2.5534e+04                    bc_loss 2.4084e+05\n",
      "Epoch 4760, Training-Loss 3.3322e+03, Data-loss 2.0852e+03                  , pde-loss 1.7970e+03, initc-loss 1.7912e+04                    bc_loss 1.0499e+05\n",
      "Epoch 4770, Training-Loss 9.5325e+03, Data-loss 5.5504e+03                  , pde-loss 6.4378e+03, initc-loss 3.0284e+04                    bc_loss 3.6149e+05\n",
      "Epoch 4780, Training-Loss 5.5487e+03, Data-loss 3.2684e+03                  , pde-loss 4.5804e+03, initc-loss 1.3694e+04                    bc_loss 2.0976e+05\n",
      "Epoch 4790, Training-Loss 7.1293e+03, Data-loss 6.2337e+03                  , pde-loss 4.8784e+02, initc-loss 1.9639e+04                    bc_loss 6.9438e+04\n",
      "Epoch 4800, Training-Loss 5.6213e+03, Data-loss 3.1639e+03                  , pde-loss 7.2011e+02, initc-loss 1.4680e+04                    bc_loss 2.3034e+05\n",
      "Epoch 4810, Training-Loss 5.3245e+03, Data-loss 3.2790e+03                  , pde-loss 2.9399e+03, initc-loss 9.2842e+03                    bc_loss 1.9232e+05\n",
      "Epoch 4820, Training-Loss 3.7510e+03, Data-loss 2.2181e+03                  , pde-loss 4.0925e+03, initc-loss 1.2334e+04                    bc_loss 1.3687e+05\n",
      "Epoch 4830, Training-Loss 2.5085e+03, Data-loss 1.8942e+03                  , pde-loss 6.5285e+02, initc-loss 1.8274e+04                    bc_loss 4.2500e+04\n",
      "Epoch 4840, Training-Loss 5.1959e+03, Data-loss 1.3826e+03                  , pde-loss 7.7892e+02, initc-loss 1.7330e+04                    bc_loss 3.6322e+05\n",
      "Epoch 4850, Training-Loss 2.1319e+03, Data-loss 1.4813e+03                  , pde-loss 1.2533e+03, initc-loss 1.4862e+04                    bc_loss 4.8943e+04\n",
      "Epoch 4860, Training-Loss 1.6782e+03, Data-loss 1.3073e+03                  , pde-loss 2.0567e+03, initc-loss 1.5246e+04                    bc_loss 1.9782e+04\n",
      "Epoch 4870, Training-Loss 2.5329e+03, Data-loss 1.4060e+03                  , pde-loss 2.2729e+03, initc-loss 1.8249e+04                    bc_loss 9.2174e+04\n",
      "Epoch 4880, Training-Loss 1.2799e+04, Data-loss 8.8947e+03                  , pde-loss 4.6245e+03, initc-loss 3.4443e+04                    bc_loss 3.5140e+05\n",
      "Epoch 4890, Training-Loss 6.8082e+03, Data-loss 4.8870e+03                  , pde-loss 3.4731e+03, initc-loss 1.4165e+04                    bc_loss 1.7448e+05\n",
      "Epoch 4900, Training-Loss 8.1039e+03, Data-loss 3.4953e+03                  , pde-loss 4.2756e+02, initc-loss 1.5980e+04                    bc_loss 4.4445e+05\n",
      "Epoch 4910, Training-Loss 5.9486e+03, Data-loss 3.5101e+03                  , pde-loss 1.8393e+02, initc-loss 1.1364e+04                    bc_loss 2.3230e+05\n",
      "Epoch 4920, Training-Loss 2.4734e+03, Data-loss 1.4887e+03                  , pde-loss 2.6780e+03, initc-loss 1.8952e+04                    bc_loss 7.6835e+04\n",
      "Epoch 4930, Training-Loss 4.0931e+03, Data-loss 3.6113e+03                  , pde-loss 3.2393e+03, initc-loss 1.4984e+04                    bc_loss 2.9959e+04\n",
      "Epoch 4940, Training-Loss 2.1143e+03, Data-loss 1.5918e+03                  , pde-loss 1.5167e+03, initc-loss 1.5168e+04                    bc_loss 3.5561e+04\n",
      "Epoch 4950, Training-Loss 2.0581e+03, Data-loss 1.6200e+03                  , pde-loss 1.0881e+03, initc-loss 1.7670e+04                    bc_loss 2.5055e+04\n",
      "Epoch 4960, Training-Loss 8.1842e+03, Data-loss 3.2636e+03                  , pde-loss 5.1259e+02, initc-loss 2.2552e+04                    bc_loss 4.6900e+05\n",
      "Epoch 4970, Training-Loss 4.1577e+03, Data-loss 2.7683e+03                  , pde-loss 1.3752e+03, initc-loss 6.6006e+03                    bc_loss 1.3096e+05\n",
      "Epoch 4980, Training-Loss 7.9667e+03, Data-loss 3.6165e+03                  , pde-loss 6.0128e+03, initc-loss 1.7847e+04                    bc_loss 4.1116e+05\n",
      "Epoch 4990, Training-Loss 5.9888e+03, Data-loss 2.4842e+03                  , pde-loss 3.5088e+03, initc-loss 1.3516e+04                    bc_loss 3.3343e+05\n",
      "Epoch 5000, Training-Loss 4.0605e+03, Data-loss 2.7252e+03                  , pde-loss 6.3811e+02, initc-loss 1.9041e+04                    bc_loss 1.1385e+05\n",
      "Epoch 5010, Training-Loss 2.2341e+03, Data-loss 9.8477e+02                  , pde-loss 1.7974e+03, initc-loss 1.7271e+04                    bc_loss 1.0587e+05\n",
      "Epoch 5020, Training-Loss 2.6269e+03, Data-loss 1.7357e+03                  , pde-loss 1.2490e+03, initc-loss 1.7190e+04                    bc_loss 7.0686e+04\n",
      "Epoch 5030, Training-Loss 1.2055e+04, Data-loss 4.9552e+03                  , pde-loss 3.9235e+02, initc-loss 2.3359e+04                    bc_loss 6.8622e+05\n",
      "Epoch 5040, Training-Loss 5.7141e+03, Data-loss 3.8181e+03                  , pde-loss 8.1819e+02, initc-loss 1.3131e+04                    bc_loss 1.7565e+05\n",
      "Epoch 5050, Training-Loss 3.4232e+03, Data-loss 1.7785e+03                  , pde-loss 4.0130e+03, initc-loss 1.5483e+04                    bc_loss 1.4497e+05\n",
      "Epoch 5060, Training-Loss 2.6582e+03, Data-loss 1.8419e+03                  , pde-loss 1.4974e+03, initc-loss 1.4527e+04                    bc_loss 6.5604e+04\n",
      "Epoch 5070, Training-Loss 2.3633e+03, Data-loss 1.8079e+03                  , pde-loss 1.3559e+03, initc-loss 1.2864e+04                    bc_loss 4.1319e+04\n",
      "Epoch 5080, Training-Loss 1.3462e+04, Data-loss 2.1368e+03                  , pde-loss 2.2553e+03, initc-loss 2.0371e+04                    bc_loss 1.1099e+06\n",
      "Epoch 5090, Training-Loss 9.8229e+03, Data-loss 4.9570e+03                  , pde-loss 5.6725e+03, initc-loss 1.2580e+04                    bc_loss 4.6835e+05\n",
      "Epoch 5100, Training-Loss 5.7582e+03, Data-loss 2.9467e+03                  , pde-loss 7.7183e+02, initc-loss 1.9482e+04                    bc_loss 2.6090e+05\n",
      "Epoch 5110, Training-Loss 8.3035e+03, Data-loss 4.4147e+03                  , pde-loss 3.7240e+02, initc-loss 2.2132e+04                    bc_loss 3.6638e+05\n",
      "Epoch 5120, Training-Loss 5.4700e+03, Data-loss 4.3449e+03                  , pde-loss 2.0388e+03, initc-loss 1.6369e+04                    bc_loss 9.4103e+04\n",
      "Epoch 5130, Training-Loss 3.2237e+03, Data-loss 2.5395e+03                  , pde-loss 2.1534e+03, initc-loss 1.3682e+04                    bc_loss 5.2581e+04\n",
      "Epoch 5140, Training-Loss 2.8557e+03, Data-loss 2.0246e+03                  , pde-loss 1.1910e+03, initc-loss 1.5006e+04                    bc_loss 6.6917e+04\n",
      "Epoch 5150, Training-Loss 2.7343e+03, Data-loss 2.3065e+03                  , pde-loss 2.0638e+03, initc-loss 1.4340e+04                    bc_loss 2.6374e+04\n",
      "Epoch 5160, Training-Loss 1.1295e+04, Data-loss 3.1954e+03                  , pde-loss 1.2438e+03, initc-loss 2.3585e+04                    bc_loss 7.8514e+05\n",
      "Epoch 5170, Training-Loss 8.0817e+03, Data-loss 5.7330e+03                  , pde-loss 4.5912e+02, initc-loss 1.0830e+04                    bc_loss 2.2358e+05\n",
      "Epoch 5180, Training-Loss 3.6292e+03, Data-loss 2.5205e+03                  , pde-loss 1.9546e+03, initc-loss 1.2593e+04                    bc_loss 9.6318e+04\n",
      "Epoch 5190, Training-Loss 3.7822e+03, Data-loss 3.2810e+03                  , pde-loss 3.3867e+03, initc-loss 1.7986e+04                    bc_loss 2.8743e+04\n",
      "Epoch 5200, Training-Loss 1.7725e+03, Data-loss 1.3273e+03                  , pde-loss 1.5198e+03, initc-loss 1.5153e+04                    bc_loss 2.7851e+04\n",
      "Epoch 5210, Training-Loss 2.5173e+03, Data-loss 9.7208e+02                  , pde-loss 1.2227e+03, initc-loss 1.3675e+04                    bc_loss 1.3962e+05\n",
      "Epoch 5220, Training-Loss 2.6400e+04, Data-loss 7.6597e+03                  , pde-loss 9.5190e+02, initc-loss 3.2050e+04                    bc_loss 1.8410e+06\n",
      "Epoch 5230, Training-Loss 1.6379e+04, Data-loss 1.4827e+04                  , pde-loss 1.0453e+03, initc-loss 1.3643e+04                    bc_loss 1.4055e+05\n",
      "Epoch 5240, Training-Loss 9.0231e+03, Data-loss 6.8717e+03                  , pde-loss 2.1606e+03, initc-loss 2.0789e+04                    bc_loss 1.9219e+05\n",
      "Epoch 5250, Training-Loss 5.3371e+03, Data-loss 3.4596e+03                  , pde-loss 1.2107e+03, initc-loss 2.1255e+04                    bc_loss 1.6528e+05\n",
      "Epoch 5260, Training-Loss 3.1787e+03, Data-loss 2.6572e+03                  , pde-loss 8.6603e+02, initc-loss 1.9898e+04                    bc_loss 3.1389e+04\n",
      "Epoch 5270, Training-Loss 3.3410e+03, Data-loss 2.5460e+03                  , pde-loss 2.1073e+03, initc-loss 2.2522e+04                    bc_loss 5.4871e+04\n",
      "Epoch 5280, Training-Loss 2.0088e+04, Data-loss 7.9649e+03                  , pde-loss 1.9654e+03, initc-loss 3.6273e+04                    bc_loss 1.1740e+06\n",
      "Epoch 5290, Training-Loss 1.4278e+04, Data-loss 1.2878e+04                  , pde-loss 2.2710e+03, initc-loss 6.4593e+03                    bc_loss 1.3132e+05\n",
      "Epoch 5300, Training-Loss 7.4694e+03, Data-loss 4.4965e+03                  , pde-loss 1.4193e+03, initc-loss 1.4223e+04                    bc_loss 2.8165e+05\n",
      "Epoch 5310, Training-Loss 4.3893e+03, Data-loss 4.0594e+03                  , pde-loss 1.4682e+03, initc-loss 2.1834e+04                    bc_loss 9.6837e+03\n",
      "Epoch 5320, Training-Loss 6.4244e+03, Data-loss 3.9080e+03                  , pde-loss 1.4637e+03, initc-loss 2.5570e+04                    bc_loss 2.2461e+05\n",
      "Epoch 5330, Training-Loss 5.2482e+03, Data-loss 4.0702e+03                  , pde-loss 7.8624e+02, initc-loss 1.9087e+04                    bc_loss 9.7930e+04\n",
      "Epoch 5340, Training-Loss 6.8769e+03, Data-loss 4.3413e+03                  , pde-loss 2.0451e+03, initc-loss 1.3643e+04                    bc_loss 2.3787e+05\n",
      "Epoch 5350, Training-Loss 6.6334e+03, Data-loss 4.5838e+03                  , pde-loss 5.3982e+03, initc-loss 2.5088e+04                    bc_loss 1.7448e+05\n",
      "Epoch 5360, Training-Loss 4.4265e+03, Data-loss 2.0090e+03                  , pde-loss 2.3791e+03, initc-loss 1.7053e+04                    bc_loss 2.2232e+05\n",
      "Epoch 5370, Training-Loss 6.0502e+03, Data-loss 3.6835e+03                  , pde-loss 5.5133e+02, initc-loss 1.6729e+04                    bc_loss 2.1938e+05\n",
      "Epoch 5380, Training-Loss 6.0407e+03, Data-loss 3.1791e+03                  , pde-loss 9.4760e+02, initc-loss 1.3270e+04                    bc_loss 2.7194e+05\n",
      "Epoch 5390, Training-Loss 4.9781e+03, Data-loss 3.2342e+03                  , pde-loss 4.4140e+03, initc-loss 2.0712e+04                    bc_loss 1.4926e+05\n",
      "Epoch 5400, Training-Loss 1.9337e+03, Data-loss 1.6756e+03                  , pde-loss 1.9314e+03, initc-loss 1.2645e+04                    bc_loss 1.1233e+04\n",
      "Epoch 5410, Training-Loss 2.1147e+03, Data-loss 1.5964e+03                  , pde-loss 1.0421e+03, initc-loss 1.6832e+04                    bc_loss 3.3958e+04\n",
      "Epoch 5420, Training-Loss 2.3067e+03, Data-loss 2.1277e+03                  , pde-loss 1.3906e+03, initc-loss 1.3267e+04                    bc_loss 3.2385e+03\n",
      "Epoch 5430, Training-Loss 2.8039e+03, Data-loss 1.9931e+03                  , pde-loss 1.5832e+03, initc-loss 1.4941e+04                    bc_loss 6.4552e+04\n",
      "Epoch 5440, Training-Loss 9.3676e+03, Data-loss 1.3599e+03                  , pde-loss 8.6354e+02, initc-loss 2.1284e+04                    bc_loss 7.7862e+05\n",
      "Epoch 5450, Training-Loss 6.4433e+03, Data-loss 4.9782e+03                  , pde-loss 5.4813e+02, initc-loss 1.5182e+04                    bc_loss 1.3078e+05\n",
      "Epoch 5460, Training-Loss 5.0135e+03, Data-loss 4.0092e+03                  , pde-loss 2.6290e+03, initc-loss 1.4127e+04                    bc_loss 8.3672e+04\n",
      "Epoch 5470, Training-Loss 9.2748e+03, Data-loss 4.6803e+03                  , pde-loss 3.9208e+03, initc-loss 2.0401e+04                    bc_loss 4.3513e+05\n",
      "Epoch 5480, Training-Loss 3.7527e+03, Data-loss 2.2834e+03                  , pde-loss 1.6105e+03, initc-loss 1.3516e+04                    bc_loss 1.3181e+05\n",
      "Epoch 5490, Training-Loss 8.7980e+03, Data-loss 4.2911e+03                  , pde-loss 4.8800e+02, initc-loss 1.8320e+04                    bc_loss 4.3188e+05\n",
      "Epoch 5500, Training-Loss 7.2586e+03, Data-loss 2.8542e+03                  , pde-loss 8.0293e+02, initc-loss 9.2131e+03                    bc_loss 4.3043e+05\n",
      "Epoch 5510, Training-Loss 4.2875e+03, Data-loss 3.3767e+03                  , pde-loss 4.9779e+03, initc-loss 2.2280e+04                    bc_loss 6.3824e+04\n",
      "Epoch 5520, Training-Loss 2.8280e+03, Data-loss 2.3114e+03                  , pde-loss 2.5041e+03, initc-loss 1.5900e+04                    bc_loss 3.3262e+04\n",
      "Epoch 5530, Training-Loss 2.4352e+03, Data-loss 2.1974e+03                  , pde-loss 1.2849e+03, initc-loss 1.1635e+04                    bc_loss 1.0855e+04\n",
      "Epoch 5540, Training-Loss 2.0880e+03, Data-loss 1.1986e+03                  , pde-loss 1.3561e+03, initc-loss 1.4309e+04                    bc_loss 7.3276e+04\n",
      "Epoch 5550, Training-Loss 5.5760e+03, Data-loss 9.7178e+02                  , pde-loss 9.3361e+02, initc-loss 1.5037e+04                    bc_loss 4.4445e+05\n",
      "Epoch 5560, Training-Loss 1.8813e+04, Data-loss 1.7166e+04                  , pde-loss 6.5810e+02, initc-loss 5.6002e+04                    bc_loss 1.0810e+05\n",
      "Epoch 5570, Training-Loss 9.0232e+03, Data-loss 8.0426e+03                  , pde-loss 1.2984e+03, initc-loss 1.4088e+04                    bc_loss 8.2673e+04\n",
      "Epoch 5580, Training-Loss 3.8478e+03, Data-loss 2.3855e+03                  , pde-loss 1.5629e+03, initc-loss 1.1747e+04                    bc_loss 1.3292e+05\n",
      "Epoch 5590, Training-Loss 3.7018e+03, Data-loss 2.7642e+03                  , pde-loss 2.3178e+03, initc-loss 1.9432e+04                    bc_loss 7.2011e+04\n",
      "Epoch 5600, Training-Loss 2.8873e+03, Data-loss 1.8892e+03                  , pde-loss 2.3743e+03, initc-loss 1.8514e+04                    bc_loss 7.8924e+04\n",
      "Epoch 5610, Training-Loss 8.8379e+03, Data-loss 7.2274e+03                  , pde-loss 1.2669e+03, initc-loss 3.2292e+04                    bc_loss 1.2749e+05\n",
      "Epoch 5620, Training-Loss 1.2723e+04, Data-loss 7.3560e+03                  , pde-loss 1.1579e+03, initc-loss 2.2168e+04                    bc_loss 5.1341e+05\n",
      "Epoch 5630, Training-Loss 1.0679e+04, Data-loss 9.5629e+03                  , pde-loss 2.5531e+03, initc-loss 1.8279e+04                    bc_loss 9.0794e+04\n",
      "Epoch 5640, Training-Loss 6.2795e+03, Data-loss 4.5851e+03                  , pde-loss 1.7018e+03, initc-loss 1.0415e+04                    bc_loss 1.5733e+05\n",
      "Epoch 5650, Training-Loss 7.4324e+03, Data-loss 3.9922e+03                  , pde-loss 3.9824e+02, initc-loss 2.4888e+04                    bc_loss 3.1873e+05\n",
      "Epoch 5660, Training-Loss 4.3665e+03, Data-loss 3.1679e+03                  , pde-loss 1.2106e+03, initc-loss 1.6233e+04                    bc_loss 1.0242e+05\n",
      "Epoch 5670, Training-Loss 3.8062e+03, Data-loss 3.1386e+03                  , pde-loss 4.3132e+03, initc-loss 2.1026e+04                    bc_loss 4.1416e+04\n",
      "Epoch 5680, Training-Loss 3.5715e+03, Data-loss 3.1409e+03                  , pde-loss 2.1999e+03, initc-loss 1.3990e+04                    bc_loss 2.6872e+04\n",
      "Epoch 5690, Training-Loss 7.5322e+03, Data-loss 2.5196e+03                  , pde-loss 7.7948e+02, initc-loss 2.0482e+04                    bc_loss 4.8000e+05\n",
      "Epoch 5700, Training-Loss 9.0975e+03, Data-loss 6.3942e+03                  , pde-loss 5.0640e+02, initc-loss 1.2639e+04                    bc_loss 2.5718e+05\n",
      "Epoch 5710, Training-Loss 5.8823e+03, Data-loss 3.9765e+03                  , pde-loss 3.2162e+03, initc-loss 2.1605e+04                    bc_loss 1.6576e+05\n",
      "Epoch 5720, Training-Loss 8.0353e+03, Data-loss 5.2663e+03                  , pde-loss 5.8550e+03, initc-loss 2.1307e+04                    bc_loss 2.4973e+05\n",
      "Epoch 5730, Training-Loss 5.5676e+03, Data-loss 4.2993e+03                  , pde-loss 1.0838e+03, initc-loss 9.2153e+03                    bc_loss 1.1653e+05\n",
      "Epoch 5740, Training-Loss 5.9691e+03, Data-loss 3.9324e+03                  , pde-loss 4.7470e+02, initc-loss 2.0698e+04                    bc_loss 1.8250e+05\n",
      "Epoch 5750, Training-Loss 2.2011e+03, Data-loss 1.6320e+03                  , pde-loss 1.4324e+03, initc-loss 1.8733e+04                    bc_loss 3.6751e+04\n",
      "Epoch 5760, Training-Loss 2.6816e+03, Data-loss 2.0615e+03                  , pde-loss 2.1884e+03, initc-loss 1.6564e+04                    bc_loss 4.3255e+04\n",
      "Epoch 5770, Training-Loss 6.1249e+03, Data-loss 2.6785e+03                  , pde-loss 7.3448e+02, initc-loss 1.7272e+04                    bc_loss 3.2663e+05\n",
      "Epoch 5780, Training-Loss 4.3569e+03, Data-loss 3.3808e+03                  , pde-loss 8.2542e+02, initc-loss 1.4302e+04                    bc_loss 8.2482e+04\n",
      "Epoch 5790, Training-Loss 3.3507e+03, Data-loss 2.8100e+03                  , pde-loss 2.3978e+03, initc-loss 1.7442e+04                    bc_loss 3.4232e+04\n",
      "Epoch 5800, Training-Loss 2.5152e+03, Data-loss 1.8158e+03                  , pde-loss 2.3624e+03, initc-loss 1.6753e+04                    bc_loss 5.0828e+04\n",
      "Epoch 5810, Training-Loss 7.7093e+03, Data-loss 2.1333e+03                  , pde-loss 1.1150e+03, initc-loss 1.8563e+04                    bc_loss 5.3792e+05\n",
      "Epoch 5820, Training-Loss 9.2344e+03, Data-loss 6.5032e+03                  , pde-loss 5.6866e+02, initc-loss 2.0748e+04                    bc_loss 2.5181e+05\n",
      "Epoch 5830, Training-Loss 6.5277e+03, Data-loss 4.0324e+03                  , pde-loss 3.2353e+03, initc-loss 1.3463e+04                    bc_loss 2.3283e+05\n",
      "Epoch 5840, Training-Loss 1.0081e+04, Data-loss 5.3071e+03                  , pde-loss 5.9837e+03, initc-loss 2.2978e+04                    bc_loss 4.4841e+05\n",
      "Epoch 5850, Training-Loss 3.7258e+03, Data-loss 2.8456e+03                  , pde-loss 9.3815e+02, initc-loss 1.2305e+04                    bc_loss 7.4776e+04\n",
      "Epoch 5860, Training-Loss 1.9844e+03, Data-loss 1.3664e+03                  , pde-loss 1.3296e+03, initc-loss 1.2032e+04                    bc_loss 4.8433e+04\n",
      "Epoch 5870, Training-Loss 1.9823e+03, Data-loss 1.4606e+03                  , pde-loss 1.5394e+03, initc-loss 1.3118e+04                    bc_loss 3.7521e+04\n",
      "Epoch 5880, Training-Loss 2.6952e+03, Data-loss 1.2209e+03                  , pde-loss 9.3024e+02, initc-loss 1.6448e+04                    bc_loss 1.3005e+05\n",
      "Epoch 5890, Training-Loss 2.0968e+03, Data-loss 1.5688e+03                  , pde-loss 1.1902e+03, initc-loss 1.5422e+04                    bc_loss 3.6189e+04\n",
      "Epoch 5900, Training-Loss 2.8933e+03, Data-loss 1.2684e+03                  , pde-loss 1.8319e+03, initc-loss 1.8121e+04                    bc_loss 1.4254e+05\n",
      "Epoch 5910, Training-Loss 2.7863e+03, Data-loss 1.9126e+03                  , pde-loss 1.2296e+03, initc-loss 1.3259e+04                    bc_loss 7.2878e+04\n",
      "Epoch 5920, Training-Loss 2.4920e+03, Data-loss 2.1705e+03                  , pde-loss 2.1391e+03, initc-loss 1.5130e+04                    bc_loss 1.4884e+04\n",
      "Epoch 5930, Training-Loss 6.2744e+03, Data-loss 1.8345e+03                  , pde-loss 1.4762e+03, initc-loss 1.8800e+04                    bc_loss 4.2371e+05\n",
      "Epoch 5940, Training-Loss 6.1632e+03, Data-loss 3.3223e+03                  , pde-loss 6.1831e+02, initc-loss 2.1367e+04                    bc_loss 2.6210e+05\n",
      "Epoch 5950, Training-Loss 3.5325e+03, Data-loss 2.8450e+03                  , pde-loss 1.5402e+03, initc-loss 1.6930e+04                    bc_loss 5.0279e+04\n",
      "Epoch 5960, Training-Loss 4.7518e+03, Data-loss 3.4627e+03                  , pde-loss 2.9981e+03, initc-loss 1.6032e+04                    bc_loss 1.0988e+05\n",
      "Epoch 5970, Training-Loss 2.3959e+03, Data-loss 1.3055e+03                  , pde-loss 1.5163e+03, initc-loss 1.1618e+04                    bc_loss 9.5901e+04\n",
      "Epoch 5980, Training-Loss 1.0093e+04, Data-loss 4.5459e+03                  , pde-loss 6.6365e+02, initc-loss 2.2651e+04                    bc_loss 5.3140e+05\n",
      "Epoch 5990, Training-Loss 5.5700e+03, Data-loss 2.0560e+03                  , pde-loss 1.3546e+03, initc-loss 6.7792e+03                    bc_loss 3.4327e+05\n",
      "Epoch 6000, Training-Loss 5.1365e+03, Data-loss 3.4677e+03                  , pde-loss 6.6458e+03, initc-loss 1.8706e+04                    bc_loss 1.4153e+05\n",
      "Epoch 6010, Training-Loss 3.3127e+03, Data-loss 2.2842e+03                  , pde-loss 3.1089e+03, initc-loss 1.4962e+04                    bc_loss 8.4782e+04\n",
      "Epoch 6020, Training-Loss 3.0295e+03, Data-loss 2.6319e+03                  , pde-loss 1.1784e+03, initc-loss 1.7694e+04                    bc_loss 2.0882e+04\n",
      "Epoch 6030, Training-Loss 2.2143e+03, Data-loss 1.6932e+03                  , pde-loss 1.2997e+03, initc-loss 1.6157e+04                    bc_loss 3.4652e+04\n",
      "Epoch 6040, Training-Loss 1.7371e+03, Data-loss 1.1403e+03                  , pde-loss 1.2803e+03, initc-loss 1.3159e+04                    bc_loss 4.5236e+04\n",
      "Epoch 6050, Training-Loss 1.8285e+03, Data-loss 1.2943e+03                  , pde-loss 1.6505e+03, initc-loss 1.4796e+04                    bc_loss 3.6970e+04\n",
      "Epoch 6060, Training-Loss 2.3480e+03, Data-loss 1.5933e+03                  , pde-loss 2.3469e+03, initc-loss 1.5754e+04                    bc_loss 5.7366e+04\n",
      "Epoch 6070, Training-Loss 1.3127e+04, Data-loss 7.5664e+03                  , pde-loss 1.2661e+03, initc-loss 4.1872e+04                    bc_loss 5.1292e+05\n",
      "Epoch 6080, Training-Loss 1.0459e+04, Data-loss 9.6435e+03                  , pde-loss 1.0004e+03, initc-loss 1.8979e+04                    bc_loss 6.1566e+04\n",
      "Epoch 6090, Training-Loss 9.1968e+03, Data-loss 6.9087e+03                  , pde-loss 1.7415e+03, initc-loss 2.3048e+04                    bc_loss 2.0403e+05\n",
      "Epoch 6100, Training-Loss 3.3521e+03, Data-loss 2.4794e+03                  , pde-loss 2.0065e+03, initc-loss 1.8457e+04                    bc_loss 6.6806e+04\n",
      "Epoch 6110, Training-Loss 4.2881e+03, Data-loss 2.2647e+03                  , pde-loss 1.2210e+03, initc-loss 1.5690e+04                    bc_loss 1.8542e+05\n",
      "Epoch 6120, Training-Loss 1.2765e+04, Data-loss 4.9131e+03                  , pde-loss 1.0756e+03, initc-loss 1.5949e+04                    bc_loss 7.6817e+05\n",
      "Epoch 6130, Training-Loss 6.9690e+03, Data-loss 5.3803e+03                  , pde-loss 7.0201e+02, initc-loss 1.8449e+04                    bc_loss 1.3971e+05\n",
      "Epoch 6140, Training-Loss 4.0965e+03, Data-loss 3.5795e+03                  , pde-loss 3.1334e+03, initc-loss 1.3196e+04                    bc_loss 3.5364e+04\n",
      "Epoch 6150, Training-Loss 5.7505e+03, Data-loss 3.3921e+03                  , pde-loss 4.9354e+03, initc-loss 2.2730e+04                    bc_loss 2.0817e+05\n",
      "Epoch 6160, Training-Loss 2.0620e+03, Data-loss 1.5366e+03                  , pde-loss 9.7769e+02, initc-loss 1.5286e+04                    bc_loss 3.6275e+04\n",
      "Epoch 6170, Training-Loss 3.5280e+03, Data-loss 2.3433e+03                  , pde-loss 8.0453e+02, initc-loss 1.4718e+04                    bc_loss 1.0294e+05\n",
      "Epoch 6180, Training-Loss 3.0816e+03, Data-loss 2.4205e+03                  , pde-loss 1.4037e+03, initc-loss 1.3031e+04                    bc_loss 5.1676e+04\n",
      "Epoch 6190, Training-Loss 1.0264e+04, Data-loss 2.8137e+03                  , pde-loss 7.8981e+02, initc-loss 2.3806e+04                    bc_loss 7.2044e+05\n",
      "Epoch 6200, Training-Loss 6.8277e+03, Data-loss 3.6165e+03                  , pde-loss 8.4957e+02, initc-loss 2.0695e+04                    bc_loss 2.9957e+05\n",
      "Epoch 6210, Training-Loss 6.5076e+03, Data-loss 2.9122e+03                  , pde-loss 3.7909e+03, initc-loss 1.5673e+04                    bc_loss 3.4007e+05\n",
      "Epoch 6220, Training-Loss 6.0698e+03, Data-loss 2.8150e+03                  , pde-loss 3.8901e+03, initc-loss 1.8509e+04                    bc_loss 3.0308e+05\n",
      "Epoch 6230, Training-Loss 4.2028e+03, Data-loss 3.1866e+03                  , pde-loss 8.6819e+02, initc-loss 1.8046e+04                    bc_loss 8.2710e+04\n",
      "Epoch 6240, Training-Loss 5.2813e+03, Data-loss 2.8304e+03                  , pde-loss 6.2144e+02, initc-loss 1.8875e+04                    bc_loss 2.2559e+05\n",
      "Epoch 6250, Training-Loss 5.6177e+03, Data-loss 2.4195e+03                  , pde-loss 9.3735e+02, initc-loss 1.7523e+04                    bc_loss 3.0137e+05\n",
      "Epoch 6260, Training-Loss 3.0098e+03, Data-loss 2.3667e+03                  , pde-loss 3.2504e+03, initc-loss 1.7153e+04                    bc_loss 4.3902e+04\n",
      "Epoch 6270, Training-Loss 2.8233e+03, Data-loss 2.1491e+03                  , pde-loss 3.4489e+03, initc-loss 1.6221e+04                    bc_loss 4.7745e+04\n",
      "Epoch 6280, Training-Loss 1.1666e+03, Data-loss 5.8896e+02                  , pde-loss 1.4858e+03, initc-loss 1.4402e+04                    bc_loss 4.1875e+04\n",
      "Epoch 6290, Training-Loss 2.8058e+03, Data-loss 1.2045e+03                  , pde-loss 1.6493e+03, initc-loss 1.3834e+04                    bc_loss 1.4464e+05\n",
      "Epoch 6300, Training-Loss 2.0127e+03, Data-loss 1.6362e+03                  , pde-loss 1.6810e+03, initc-loss 1.3743e+04                    bc_loss 2.2225e+04\n",
      "Epoch 6310, Training-Loss 1.7513e+03, Data-loss 9.1073e+02                  , pde-loss 1.5209e+03, initc-loss 1.5577e+04                    bc_loss 6.6956e+04\n",
      "Epoch 6320, Training-Loss 1.0882e+04, Data-loss 4.7632e+03                  , pde-loss 7.9173e+02, initc-loss 2.3534e+04                    bc_loss 5.8754e+05\n",
      "Epoch 6330, Training-Loss 2.7587e+03, Data-loss 2.0933e+03                  , pde-loss 1.6283e+03, initc-loss 1.1742e+04                    bc_loss 5.3164e+04\n",
      "Epoch 6340, Training-Loss 8.6344e+03, Data-loss 3.3559e+03                  , pde-loss 4.3372e+03, initc-loss 2.3964e+04                    bc_loss 4.9955e+05\n",
      "Epoch 6350, Training-Loss 5.7838e+03, Data-loss 3.6075e+03                  , pde-loss 4.1285e+03, initc-loss 1.3281e+04                    bc_loss 2.0022e+05\n",
      "Epoch 6360, Training-Loss 2.8947e+03, Data-loss 2.2728e+03                  , pde-loss 1.0693e+03, initc-loss 1.5987e+04                    bc_loss 4.5136e+04\n",
      "Epoch 6370, Training-Loss 1.7357e+03, Data-loss 1.4262e+03                  , pde-loss 1.3067e+03, initc-loss 1.7007e+04                    bc_loss 1.2644e+04\n",
      "Epoch 6380, Training-Loss 3.7130e+03, Data-loss 2.7480e+03                  , pde-loss 1.5733e+03, initc-loss 2.0884e+04                    bc_loss 7.4042e+04\n",
      "Epoch 6390, Training-Loss 9.9459e+03, Data-loss 4.5732e+03                  , pde-loss 7.9015e+02, initc-loss 2.4039e+04                    bc_loss 5.1244e+05\n",
      "Epoch 6400, Training-Loss 7.1299e+03, Data-loss 2.9811e+03                  , pde-loss 7.6010e+02, initc-loss 1.1827e+04                    bc_loss 4.0229e+05\n",
      "Epoch 6410, Training-Loss 4.1105e+03, Data-loss 2.8107e+03                  , pde-loss 3.8552e+03, initc-loss 1.4393e+04                    bc_loss 1.1173e+05\n",
      "Epoch 6420, Training-Loss 1.7656e+03, Data-loss 1.3613e+03                  , pde-loss 2.0662e+03, initc-loss 1.7497e+04                    bc_loss 2.0868e+04\n",
      "Epoch 6430, Training-Loss 3.5366e+03, Data-loss 3.1336e+03                  , pde-loss 1.1657e+03, initc-loss 1.3922e+04                    bc_loss 2.5209e+04\n",
      "Epoch 6440, Training-Loss 3.7636e+03, Data-loss 3.2951e+03                  , pde-loss 1.2896e+03, initc-loss 1.5473e+04                    bc_loss 3.0085e+04\n",
      "Epoch 6450, Training-Loss 3.4321e+03, Data-loss 1.6111e+03                  , pde-loss 2.0225e+03, initc-loss 2.0401e+04                    bc_loss 1.5968e+05\n",
      "Epoch 6460, Training-Loss 1.6399e+04, Data-loss 1.2654e+04                  , pde-loss 2.3903e+03, initc-loss 4.9653e+04                    bc_loss 3.2243e+05\n",
      "Epoch 6470, Training-Loss 5.9333e+03, Data-loss 5.2176e+03                  , pde-loss 1.9558e+03, initc-loss 1.2732e+04                    bc_loss 5.6876e+04\n",
      "Epoch 6480, Training-Loss 6.2547e+03, Data-loss 4.6490e+03                  , pde-loss 1.1715e+03, initc-loss 1.0260e+04                    bc_loss 1.4914e+05\n",
      "Epoch 6490, Training-Loss 2.5637e+03, Data-loss 1.9858e+03                  , pde-loss 2.4380e+03, initc-loss 2.3344e+04                    bc_loss 3.2006e+04\n",
      "Epoch 6500, Training-Loss 2.9641e+03, Data-loss 1.6647e+03                  , pde-loss 1.9318e+03, initc-loss 1.6953e+04                    bc_loss 1.1105e+05\n",
      "Epoch 6510, Training-Loss 5.4050e+03, Data-loss 3.6535e+03                  , pde-loss 4.4758e+02, initc-loss 2.4226e+04                    bc_loss 1.5048e+05\n",
      "Epoch 6520, Training-Loss 3.6097e+03, Data-loss 1.4869e+03                  , pde-loss 1.4625e+03, initc-loss 1.0250e+04                    bc_loss 2.0057e+05\n",
      "Epoch 6530, Training-Loss 9.8767e+03, Data-loss 5.8122e+03                  , pde-loss 6.3906e+03, initc-loss 2.4060e+04                    bc_loss 3.7601e+05\n",
      "Epoch 6540, Training-Loss 6.7939e+03, Data-loss 3.8627e+03                  , pde-loss 4.6129e+03, initc-loss 1.6345e+04                    bc_loss 2.7216e+05\n",
      "Epoch 6550, Training-Loss 4.0777e+03, Data-loss 3.7382e+03                  , pde-loss 6.6684e+02, initc-loss 1.1433e+04                    bc_loss 2.1851e+04\n",
      "Epoch 6560, Training-Loss 3.4817e+03, Data-loss 2.7336e+03                  , pde-loss 6.0648e+02, initc-loss 1.3758e+04                    bc_loss 6.0450e+04\n",
      "Epoch 6570, Training-Loss 2.8057e+03, Data-loss 1.6690e+03                  , pde-loss 2.8092e+03, initc-loss 1.6923e+04                    bc_loss 9.3936e+04\n",
      "Epoch 6580, Training-Loss 4.8627e+03, Data-loss 2.2163e+03                  , pde-loss 1.3796e+03, initc-loss 1.4154e+04                    bc_loss 2.4910e+05\n",
      "Epoch 6590, Training-Loss 6.8281e+03, Data-loss 3.3479e+03                  , pde-loss 4.2420e+02, initc-loss 2.0019e+04                    bc_loss 3.2757e+05\n",
      "Epoch 6600, Training-Loss 3.1332e+03, Data-loss 2.5167e+03                  , pde-loss 1.1241e+03, initc-loss 1.5580e+04                    bc_loss 4.4945e+04\n",
      "Epoch 6610, Training-Loss 4.2935e+03, Data-loss 3.4056e+03                  , pde-loss 2.8301e+03, initc-loss 1.6700e+04                    bc_loss 6.9262e+04\n",
      "Epoch 6620, Training-Loss 3.3746e+03, Data-loss 2.9302e+03                  , pde-loss 1.7258e+03, initc-loss 1.6153e+04                    bc_loss 2.6561e+04\n",
      "Epoch 6630, Training-Loss 4.0245e+03, Data-loss 2.6948e+03                  , pde-loss 1.1415e+03, initc-loss 1.6831e+04                    bc_loss 1.1499e+05\n",
      "Epoch 6640, Training-Loss 6.0522e+03, Data-loss 2.1962e+03                  , pde-loss 9.7808e+02, initc-loss 1.5673e+04                    bc_loss 3.6895e+05\n",
      "Epoch 6650, Training-Loss 4.1198e+03, Data-loss 3.2180e+03                  , pde-loss 3.4090e+03, initc-loss 2.0521e+04                    bc_loss 6.6243e+04\n",
      "Epoch 6660, Training-Loss 7.9199e+03, Data-loss 3.8509e+03                  , pde-loss 5.4198e+03, initc-loss 1.9889e+04                    bc_loss 3.8159e+05\n",
      "Epoch 6670, Training-Loss 2.0511e+03, Data-loss 1.1098e+03                  , pde-loss 1.8870e+03, initc-loss 1.4051e+04                    bc_loss 7.8197e+04\n",
      "Epoch 6680, Training-Loss 4.2463e+03, Data-loss 3.3220e+03                  , pde-loss 6.1566e+02, initc-loss 1.4413e+04                    bc_loss 7.7411e+04\n",
      "Epoch 6690, Training-Loss 2.4936e+03, Data-loss 1.7255e+03                  , pde-loss 1.7078e+03, initc-loss 1.2624e+04                    bc_loss 6.2475e+04\n",
      "Epoch 6700, Training-Loss 1.9604e+03, Data-loss 1.3523e+03                  , pde-loss 1.4362e+03, initc-loss 1.6745e+04                    bc_loss 4.2628e+04\n",
      "Epoch 6710, Training-Loss 1.0772e+04, Data-loss 3.0475e+03                  , pde-loss 7.8400e+02, initc-loss 2.4666e+04                    bc_loss 7.4695e+05\n",
      "Epoch 6720, Training-Loss 7.9836e+03, Data-loss 6.9160e+03                  , pde-loss 3.9848e+02, initc-loss 1.2543e+04                    bc_loss 9.3826e+04\n",
      "Epoch 6730, Training-Loss 3.2358e+03, Data-loss 2.5015e+03                  , pde-loss 1.5703e+03, initc-loss 1.3261e+04                    bc_loss 5.8603e+04\n",
      "Epoch 6740, Training-Loss 4.2226e+03, Data-loss 3.3860e+03                  , pde-loss 3.5898e+03, initc-loss 1.7603e+04                    bc_loss 6.2466e+04\n",
      "Epoch 6750, Training-Loss 3.8320e+03, Data-loss 2.8856e+03                  , pde-loss 1.4767e+03, initc-loss 1.6343e+04                    bc_loss 7.6822e+04\n",
      "Epoch 6760, Training-Loss 1.2189e+04, Data-loss 9.3670e+03                  , pde-loss 7.3245e+02, initc-loss 3.6138e+04                    bc_loss 2.4537e+05\n",
      "Epoch 6770, Training-Loss 7.6613e+03, Data-loss 5.0935e+03                  , pde-loss 1.3464e+03, initc-loss 1.2013e+04                    bc_loss 2.4342e+05\n",
      "Epoch 6780, Training-Loss 1.0214e+04, Data-loss 6.9898e+03                  , pde-loss 7.2291e+03, initc-loss 2.4202e+04                    bc_loss 2.9099e+05\n",
      "Epoch 6790, Training-Loss 9.1338e+03, Data-loss 4.9639e+03                  , pde-loss 4.7846e+03, initc-loss 1.4467e+04                    bc_loss 3.9774e+05\n",
      "Epoch 6800, Training-Loss 6.6682e+03, Data-loss 6.0043e+03                  , pde-loss 5.5164e+02, initc-loss 1.9033e+04                    bc_loss 4.6813e+04\n",
      "Epoch 6810, Training-Loss 3.9856e+03, Data-loss 3.6143e+03                  , pde-loss 3.6853e+02, initc-loss 1.4053e+04                    bc_loss 2.2707e+04\n",
      "Epoch 6820, Training-Loss 3.3614e+03, Data-loss 2.3245e+03                  , pde-loss 2.2237e+03, initc-loss 1.6551e+04                    bc_loss 8.4913e+04\n",
      "Epoch 6830, Training-Loss 7.9767e+03, Data-loss 4.4570e+03                  , pde-loss 1.5784e+03, initc-loss 1.8075e+04                    bc_loss 3.3231e+05\n",
      "Epoch 6840, Training-Loss 5.5691e+03, Data-loss 4.2415e+03                  , pde-loss 5.3082e+02, initc-loss 1.7480e+04                    bc_loss 1.1474e+05\n",
      "Epoch 6850, Training-Loss 3.8250e+03, Data-loss 2.8949e+03                  , pde-loss 1.2763e+03, initc-loss 1.3954e+04                    bc_loss 7.7776e+04\n",
      "Epoch 6860, Training-Loss 6.4456e+03, Data-loss 5.4448e+03                  , pde-loss 3.1980e+03, initc-loss 1.8514e+04                    bc_loss 7.8369e+04\n",
      "Epoch 6870, Training-Loss 3.9658e+03, Data-loss 2.9094e+03                  , pde-loss 1.3688e+03, initc-loss 2.0105e+04                    bc_loss 8.4167e+04\n",
      "Epoch 6880, Training-Loss 1.4911e+04, Data-loss 8.2287e+03                  , pde-loss 7.6243e+02, initc-loss 3.1534e+04                    bc_loss 6.3596e+05\n",
      "Epoch 6890, Training-Loss 8.1261e+03, Data-loss 5.5235e+03                  , pde-loss 9.0135e+02, initc-loss 8.2061e+03                    bc_loss 2.5115e+05\n",
      "Epoch 6900, Training-Loss 3.4807e+03, Data-loss 2.1605e+03                  , pde-loss 3.7257e+03, initc-loss 1.1877e+04                    bc_loss 1.1642e+05\n",
      "Epoch 6910, Training-Loss 5.2229e+03, Data-loss 3.2853e+03                  , pde-loss 2.1540e+03, initc-loss 2.6825e+04                    bc_loss 1.6479e+05\n",
      "Epoch 6920, Training-Loss 3.7920e+03, Data-loss 2.9610e+03                  , pde-loss 9.4403e+02, initc-loss 1.7482e+04                    bc_loss 6.4676e+04\n",
      "Epoch 6930, Training-Loss 4.6289e+03, Data-loss 3.0036e+03                  , pde-loss 1.2256e+03, initc-loss 1.6156e+04                    bc_loss 1.4516e+05\n",
      "Epoch 6940, Training-Loss 2.7625e+03, Data-loss 2.0285e+03                  , pde-loss 1.7108e+03, initc-loss 1.4738e+04                    bc_loss 5.6949e+04\n",
      "Epoch 6950, Training-Loss 7.0723e+03, Data-loss 2.0771e+03                  , pde-loss 3.2106e+03, initc-loss 1.9846e+04                    bc_loss 4.7646e+05\n",
      "Epoch 6960, Training-Loss 5.6767e+03, Data-loss 4.5190e+03                  , pde-loss 5.3178e+03, initc-loss 1.8140e+04                    bc_loss 9.2315e+04\n",
      "Epoch 6970, Training-Loss 4.6238e+03, Data-loss 3.2152e+03                  , pde-loss 1.9264e+03, initc-loss 1.5046e+04                    bc_loss 1.2388e+05\n",
      "Epoch 6980, Training-Loss 6.5470e+03, Data-loss 3.6438e+03                  , pde-loss 5.8512e+02, initc-loss 1.6092e+04                    bc_loss 2.7364e+05\n",
      "Epoch 6990, Training-Loss 7.7976e+03, Data-loss 6.7600e+03                  , pde-loss 3.2292e+02, initc-loss 2.3096e+04                    bc_loss 8.0346e+04\n",
      "Epoch 7000, Training-Loss 8.8233e+03, Data-loss 4.9535e+03                  , pde-loss 2.1075e+03, initc-loss 2.3872e+04                    bc_loss 3.6101e+05\n",
      "Epoch 7010, Training-Loss 5.2109e+03, Data-loss 2.6795e+03                  , pde-loss 6.0272e+03, initc-loss 1.6770e+04                    bc_loss 2.3034e+05\n",
      "Epoch 7020, Training-Loss 5.7153e+03, Data-loss 2.8901e+03                  , pde-loss 1.8546e+03, initc-loss 8.8878e+03                    bc_loss 2.7177e+05\n",
      "Epoch 7030, Training-Loss 6.2651e+03, Data-loss 4.2374e+03                  , pde-loss 5.8086e+02, initc-loss 1.6475e+04                    bc_loss 1.8571e+05\n",
      "Epoch 7040, Training-Loss 5.4389e+03, Data-loss 2.9127e+03                  , pde-loss 4.5190e+02, initc-loss 1.4843e+04                    bc_loss 2.3733e+05\n",
      "Epoch 7050, Training-Loss 4.1185e+03, Data-loss 3.4911e+03                  , pde-loss 2.4467e+03, initc-loss 1.9850e+04                    bc_loss 4.0445e+04\n",
      "Epoch 7060, Training-Loss 2.2278e+03, Data-loss 1.8630e+03                  , pde-loss 1.9859e+03, initc-loss 1.4689e+04                    bc_loss 1.9799e+04\n",
      "Epoch 7070, Training-Loss 1.3433e+03, Data-loss 8.7451e+02                  , pde-loss 1.5474e+03, initc-loss 1.4618e+04                    bc_loss 3.0709e+04\n",
      "Epoch 7080, Training-Loss 8.8454e+03, Data-loss 3.6072e+03                  , pde-loss 1.1858e+03, initc-loss 1.5924e+04                    bc_loss 5.0671e+05\n",
      "Epoch 7090, Training-Loss 4.9557e+03, Data-loss 1.9905e+03                  , pde-loss 1.5120e+03, initc-loss 9.0719e+03                    bc_loss 2.8593e+05\n",
      "Epoch 7100, Training-Loss 4.2945e+03, Data-loss 3.3804e+03                  , pde-loss 3.8356e+03, initc-loss 1.7837e+04                    bc_loss 6.9738e+04\n",
      "Epoch 7110, Training-Loss 3.2876e+03, Data-loss 1.4736e+03                  , pde-loss 2.4063e+03, initc-loss 1.6716e+04                    bc_loss 1.6227e+05\n",
      "Epoch 7120, Training-Loss 2.0526e+03, Data-loss 1.8563e+03                  , pde-loss 1.6319e+03, initc-loss 1.3740e+04                    bc_loss 4.2632e+03\n",
      "Epoch 7130, Training-Loss 1.5317e+03, Data-loss 1.2010e+03                  , pde-loss 1.3287e+03, initc-loss 1.4088e+04                    bc_loss 1.7655e+04\n",
      "Epoch 7140, Training-Loss 2.4128e+03, Data-loss 1.0908e+03                  , pde-loss 1.4117e+03, initc-loss 1.5944e+04                    bc_loss 1.1484e+05\n",
      "Epoch 7150, Training-Loss 7.2927e+03, Data-loss 4.0067e+03                  , pde-loss 2.3444e+03, initc-loss 2.3625e+04                    bc_loss 3.0263e+05\n",
      "Epoch 7160, Training-Loss 2.6581e+03, Data-loss 1.7184e+03                  , pde-loss 2.4535e+03, initc-loss 1.4208e+04                    bc_loss 7.7301e+04\n",
      "Epoch 7170, Training-Loss 6.5659e+03, Data-loss 1.9892e+03                  , pde-loss 8.5921e+02, initc-loss 1.8508e+04                    bc_loss 4.3831e+05\n",
      "Epoch 7180, Training-Loss 4.8855e+03, Data-loss 3.3622e+03                  , pde-loss 5.3000e+02, initc-loss 1.5730e+04                    bc_loss 1.3607e+05\n",
      "Epoch 7190, Training-Loss 3.3163e+03, Data-loss 2.3583e+03                  , pde-loss 2.4683e+03, initc-loss 1.5624e+04                    bc_loss 7.7712e+04\n",
      "Epoch 7200, Training-Loss 2.3102e+03, Data-loss 1.7150e+03                  , pde-loss 2.0091e+03, initc-loss 1.6722e+04                    bc_loss 4.0782e+04\n",
      "Epoch 7210, Training-Loss 2.3455e+03, Data-loss 1.5258e+03                  , pde-loss 1.1537e+03, initc-loss 1.5297e+04                    bc_loss 6.5524e+04\n",
      "Epoch 7220, Training-Loss 1.6740e+04, Data-loss 1.5893e+04                  , pde-loss 1.0808e+03, initc-loss 5.5041e+04                    bc_loss 2.8601e+04\n",
      "Epoch 7230, Training-Loss 9.2150e+03, Data-loss 6.0819e+03                  , pde-loss 1.2494e+03, initc-loss 1.4286e+04                    bc_loss 2.9777e+05\n",
      "Epoch 7240, Training-Loss 6.8905e+03, Data-loss 5.2564e+03                  , pde-loss 2.1752e+03, initc-loss 1.3884e+04                    bc_loss 1.4735e+05\n",
      "Epoch 7250, Training-Loss 5.4302e+03, Data-loss 4.6107e+03                  , pde-loss 1.0619e+03, initc-loss 2.1261e+04                    bc_loss 5.9630e+04\n",
      "Epoch 7260, Training-Loss 8.6124e+03, Data-loss 5.6536e+03                  , pde-loss 5.5097e+02, initc-loss 2.5061e+04                    bc_loss 2.7026e+05\n",
      "Epoch 7270, Training-Loss 4.6856e+03, Data-loss 2.9629e+03                  , pde-loss 9.5304e+02, initc-loss 1.4731e+04                    bc_loss 1.5659e+05\n",
      "Epoch 7280, Training-Loss 3.7679e+03, Data-loss 2.6488e+03                  , pde-loss 2.2715e+03, initc-loss 1.1525e+04                    bc_loss 9.8106e+04\n",
      "Epoch 7290, Training-Loss 2.1107e+03, Data-loss 1.1330e+03                  , pde-loss 3.4588e+03, initc-loss 1.8689e+04                    bc_loss 7.5617e+04\n",
      "Epoch 7300, Training-Loss 2.3476e+03, Data-loss 1.0795e+03                  , pde-loss 1.2297e+03, initc-loss 1.4798e+04                    bc_loss 1.1078e+05\n",
      "Epoch 7310, Training-Loss 1.1766e+04, Data-loss 4.3658e+03                  , pde-loss 4.6654e+02, initc-loss 2.2695e+04                    bc_loss 7.1690e+05\n",
      "Epoch 7320, Training-Loss 6.2931e+03, Data-loss 4.0408e+03                  , pde-loss 6.7735e+02, initc-loss 1.1301e+04                    bc_loss 2.1325e+05\n",
      "Epoch 7330, Training-Loss 3.0927e+03, Data-loss 2.5465e+03                  , pde-loss 3.5334e+03, initc-loss 1.2418e+04                    bc_loss 3.8667e+04\n",
      "Epoch 7340, Training-Loss 3.4402e+03, Data-loss 2.3777e+03                  , pde-loss 3.0505e+03, initc-loss 1.8205e+04                    bc_loss 8.4991e+04\n",
      "Epoch 7350, Training-Loss 2.7503e+03, Data-loss 1.5911e+03                  , pde-loss 7.7757e+02, initc-loss 1.7827e+04                    bc_loss 9.7312e+04\n",
      "Epoch 7360, Training-Loss 3.3296e+03, Data-loss 2.5544e+03                  , pde-loss 9.7290e+02, initc-loss 1.3487e+04                    bc_loss 6.3055e+04\n",
      "Epoch 7370, Training-Loss 3.2603e+03, Data-loss 1.3742e+03                  , pde-loss 1.3280e+03, initc-loss 1.5589e+04                    bc_loss 1.7169e+05\n",
      "Epoch 7380, Training-Loss 2.8904e+03, Data-loss 1.6709e+03                  , pde-loss 3.1068e+03, initc-loss 1.4999e+04                    bc_loss 1.0384e+05\n",
      "Epoch 7390, Training-Loss 8.1467e+03, Data-loss 5.0210e+03                  , pde-loss 4.3868e+03, initc-loss 2.4229e+04                    bc_loss 2.8395e+05\n",
      "Epoch 7400, Training-Loss 5.4925e+03, Data-loss 4.5549e+03                  , pde-loss 3.3071e+03, initc-loss 1.4630e+04                    bc_loss 7.5818e+04\n",
      "Epoch 7410, Training-Loss 6.8872e+03, Data-loss 4.2940e+03                  , pde-loss 8.0618e+02, initc-loss 1.7953e+04                    bc_loss 2.4057e+05\n",
      "Epoch 7420, Training-Loss 4.8214e+03, Data-loss 3.1651e+03                  , pde-loss 3.5055e+02, initc-loss 1.9751e+04                    bc_loss 1.4553e+05\n",
      "Epoch 7430, Training-Loss 5.7007e+03, Data-loss 2.6392e+03                  , pde-loss 1.4811e+03, initc-loss 1.6967e+04                    bc_loss 2.8770e+05\n",
      "Epoch 7440, Training-Loss 1.0294e+04, Data-loss 4.7278e+03                  , pde-loss 5.1481e+03, initc-loss 1.7698e+04                    bc_loss 5.3380e+05\n",
      "Epoch 7450, Training-Loss 1.0096e+04, Data-loss 3.5548e+03                  , pde-loss 3.5823e+03, initc-loss 9.7481e+03                    bc_loss 6.4082e+05\n",
      "Epoch 7460, Training-Loss 1.2566e+04, Data-loss 4.5709e+03                  , pde-loss 7.4123e+02, initc-loss 2.3229e+04                    bc_loss 7.7549e+05\n",
      "Epoch 7470, Training-Loss 1.2145e+04, Data-loss 5.2099e+03                  , pde-loss 4.6895e+02, initc-loss 1.4382e+04                    bc_loss 6.7867e+05\n",
      "Epoch 7480, Training-Loss 5.5033e+03, Data-loss 3.3938e+03                  , pde-loss 4.1503e+03, initc-loss 1.5797e+04                    bc_loss 1.9100e+05\n",
      "Epoch 7490, Training-Loss 2.0052e+03, Data-loss 1.5783e+03                  , pde-loss 1.9109e+03, initc-loss 1.4366e+04                    bc_loss 2.6407e+04\n",
      "Epoch 7500, Training-Loss 2.2766e+03, Data-loss 1.6318e+03                  , pde-loss 9.7891e+02, initc-loss 1.6658e+04                    bc_loss 4.6840e+04\n",
      "Epoch 7510, Training-Loss 2.2403e+03, Data-loss 1.6161e+03                  , pde-loss 1.3415e+03, initc-loss 1.6637e+04                    bc_loss 4.4449e+04\n",
      "Epoch 7520, Training-Loss 2.2330e+03, Data-loss 1.8035e+03                  , pde-loss 1.4014e+03, initc-loss 1.4294e+04                    bc_loss 2.7254e+04\n",
      "Epoch 7530, Training-Loss 4.6924e+03, Data-loss 2.1527e+03                  , pde-loss 1.2774e+03, initc-loss 1.4027e+04                    bc_loss 2.3867e+05\n",
      "Epoch 7540, Training-Loss 1.8797e+03, Data-loss 1.1250e+03                  , pde-loss 1.6916e+03, initc-loss 1.6804e+04                    bc_loss 5.6972e+04\n",
      "Epoch 7550, Training-Loss 2.1966e+03, Data-loss 1.8189e+03                  , pde-loss 2.1579e+03, initc-loss 1.2171e+04                    bc_loss 2.3439e+04\n",
      "Epoch 7560, Training-Loss 1.8794e+03, Data-loss 1.5460e+03                  , pde-loss 2.1367e+03, initc-loss 1.2371e+04                    bc_loss 1.8831e+04\n",
      "Epoch 7570, Training-Loss 1.3677e+03, Data-loss 1.0290e+03                  , pde-loss 1.5684e+03, initc-loss 1.4079e+04                    bc_loss 1.8222e+04\n",
      "Epoch 7580, Training-Loss 1.1738e+04, Data-loss 7.9581e+03                  , pde-loss 4.8234e+02, initc-loss 3.5177e+04                    bc_loss 3.4234e+05\n",
      "Epoch 7590, Training-Loss 4.2608e+03, Data-loss 3.2596e+03                  , pde-loss 7.9410e+02, initc-loss 1.5768e+04                    bc_loss 8.3560e+04\n",
      "Epoch 7600, Training-Loss 4.0663e+03, Data-loss 2.6540e+03                  , pde-loss 2.3234e+03, initc-loss 1.0992e+04                    bc_loss 1.2792e+05\n",
      "Epoch 7610, Training-Loss 5.6020e+03, Data-loss 3.6278e+03                  , pde-loss 3.9932e+03, initc-loss 1.8832e+04                    bc_loss 1.7460e+05\n",
      "Epoch 7620, Training-Loss 3.3648e+03, Data-loss 2.3050e+03                  , pde-loss 1.4715e+03, initc-loss 1.5058e+04                    bc_loss 8.9452e+04\n",
      "Epoch 7630, Training-Loss 4.0009e+03, Data-loss 1.8348e+03                  , pde-loss 5.5223e+02, initc-loss 1.6471e+04                    bc_loss 1.9959e+05\n",
      "Epoch 7640, Training-Loss 9.8137e+03, Data-loss 3.0819e+03                  , pde-loss 5.0033e+02, initc-loss 1.9592e+04                    bc_loss 6.5309e+05\n",
      "Epoch 7650, Training-Loss 4.0495e+03, Data-loss 3.1632e+03                  , pde-loss 2.4698e+03, initc-loss 1.4447e+04                    bc_loss 7.1709e+04\n",
      "Epoch 7660, Training-Loss 4.0203e+03, Data-loss 2.6497e+03                  , pde-loss 4.3456e+03, initc-loss 1.3708e+04                    bc_loss 1.1900e+05\n",
      "Epoch 7670, Training-Loss 2.9938e+03, Data-loss 2.4531e+03                  , pde-loss 1.5246e+03, initc-loss 1.4240e+04                    bc_loss 3.8306e+04\n",
      "Epoch 7680, Training-Loss 1.1131e+03, Data-loss 8.0110e+02                  , pde-loss 1.2714e+03, initc-loss 1.3611e+04                    bc_loss 1.6318e+04\n",
      "Epoch 7690, Training-Loss 2.7792e+03, Data-loss 2.2614e+03                  , pde-loss 1.6603e+03, initc-loss 1.5662e+04                    bc_loss 3.4452e+04\n",
      "Epoch 7700, Training-Loss 2.0785e+03, Data-loss 1.3225e+03                  , pde-loss 1.5408e+03, initc-loss 1.3830e+04                    bc_loss 6.0226e+04\n",
      "Epoch 7710, Training-Loss 7.2952e+03, Data-loss 4.1338e+03                  , pde-loss 7.3071e+02, initc-loss 2.2631e+04                    bc_loss 2.9277e+05\n",
      "Epoch 7720, Training-Loss 6.0423e+03, Data-loss 3.6568e+03                  , pde-loss 1.7026e+03, initc-loss 1.4722e+04                    bc_loss 2.2212e+05\n",
      "Epoch 7730, Training-Loss 9.2857e+03, Data-loss 3.9061e+03                  , pde-loss 5.5818e+03, initc-loss 2.4477e+04                    bc_loss 5.0790e+05\n",
      "Epoch 7740, Training-Loss 6.2162e+03, Data-loss 3.9060e+03                  , pde-loss 4.5328e+03, initc-loss 1.2790e+04                    bc_loss 2.1370e+05\n",
      "Epoch 7750, Training-Loss 2.8375e+03, Data-loss 1.9825e+03                  , pde-loss 1.0359e+03, initc-loss 1.2656e+04                    bc_loss 7.1807e+04\n",
      "Epoch 7760, Training-Loss 1.4447e+03, Data-loss 8.5821e+02                  , pde-loss 1.2005e+03, initc-loss 1.5950e+04                    bc_loss 4.1500e+04\n",
      "Epoch 7770, Training-Loss 3.9624e+03, Data-loss 2.6445e+03                  , pde-loss 1.9036e+03, initc-loss 1.6861e+04                    bc_loss 1.1303e+05\n",
      "Epoch 7780, Training-Loss 3.0477e+03, Data-loss 2.5436e+03                  , pde-loss 1.0816e+03, initc-loss 1.4872e+04                    bc_loss 3.4455e+04\n",
      "Epoch 7790, Training-Loss 2.6934e+03, Data-loss 1.9304e+03                  , pde-loss 8.3083e+02, initc-loss 1.7592e+04                    bc_loss 5.7868e+04\n",
      "Epoch 7800, Training-Loss 4.3233e+03, Data-loss 2.3613e+03                  , pde-loss 7.3679e+02, initc-loss 1.5928e+04                    bc_loss 1.7953e+05\n",
      "Epoch 7810, Training-Loss 2.8161e+03, Data-loss 1.3100e+03                  , pde-loss 1.2505e+03, initc-loss 9.7159e+03                    bc_loss 1.3965e+05\n",
      "Epoch 7820, Training-Loss 1.4371e+03, Data-loss 1.0284e+03                  , pde-loss 3.2050e+03, initc-loss 1.6742e+04                    bc_loss 2.0922e+04\n",
      "Epoch 7830, Training-Loss 2.8007e+03, Data-loss 1.9090e+03                  , pde-loss 1.7714e+03, initc-loss 1.8533e+04                    bc_loss 6.8859e+04\n",
      "Epoch 7840, Training-Loss 2.6456e+03, Data-loss 1.0602e+03                  , pde-loss 1.7923e+03, initc-loss 1.3788e+04                    bc_loss 1.4296e+05\n",
      "Epoch 7850, Training-Loss 6.7669e+03, Data-loss 4.4814e+03                  , pde-loss 4.2990e+03, initc-loss 2.1972e+04                    bc_loss 2.0228e+05\n",
      "Epoch 7860, Training-Loss 4.8575e+03, Data-loss 2.8891e+03                  , pde-loss 2.0659e+03, initc-loss 1.4893e+04                    bc_loss 1.7988e+05\n",
      "Epoch 7870, Training-Loss 1.0392e+04, Data-loss 4.9267e+03                  , pde-loss 2.5948e+02, initc-loss 2.5641e+04                    bc_loss 5.2059e+05\n",
      "Epoch 7880, Training-Loss 5.4123e+03, Data-loss 3.5045e+03                  , pde-loss 7.3308e+02, initc-loss 1.4675e+04                    bc_loss 1.7538e+05\n",
      "Epoch 7890, Training-Loss 4.5393e+03, Data-loss 2.8766e+03                  , pde-loss 3.3200e+03, initc-loss 1.4969e+04                    bc_loss 1.4798e+05\n",
      "Epoch 7900, Training-Loss 1.5793e+03, Data-loss 9.3063e+02                  , pde-loss 1.6804e+03, initc-loss 1.5787e+04                    bc_loss 4.7404e+04\n",
      "Epoch 7910, Training-Loss 2.9239e+03, Data-loss 2.3112e+03                  , pde-loss 1.2078e+03, initc-loss 1.7689e+04                    bc_loss 4.2372e+04\n",
      "Epoch 7920, Training-Loss 1.5501e+03, Data-loss 1.0762e+03                  , pde-loss 1.8849e+03, initc-loss 1.5719e+04                    bc_loss 2.9781e+04\n",
      "Epoch 7930, Training-Loss 1.6694e+03, Data-loss 1.2404e+03                  , pde-loss 1.4943e+03, initc-loss 1.4882e+04                    bc_loss 2.6520e+04\n",
      "Epoch 7940, Training-Loss 1.6123e+04, Data-loss 5.2202e+03                  , pde-loss 6.8466e+02, initc-loss 2.6947e+04                    bc_loss 1.0626e+06\n",
      "Epoch 7950, Training-Loss 8.7188e+03, Data-loss 6.0747e+03                  , pde-loss 3.9235e+02, initc-loss 8.9034e+03                    bc_loss 2.5511e+05\n",
      "Epoch 7960, Training-Loss 3.8481e+03, Data-loss 2.3997e+03                  , pde-loss 2.7402e+03, initc-loss 1.3182e+04                    bc_loss 1.2892e+05\n",
      "Epoch 7970, Training-Loss 1.0603e+04, Data-loss 6.0974e+03                  , pde-loss 5.1921e+03, initc-loss 2.2101e+04                    bc_loss 4.2326e+05\n",
      "Epoch 7980, Training-Loss 4.8900e+03, Data-loss 1.7789e+03                  , pde-loss 3.1380e+03, initc-loss 1.2865e+04                    bc_loss 2.9510e+05\n",
      "Epoch 7990, Training-Loss 4.6620e+03, Data-loss 3.4649e+03                  , pde-loss 4.4392e+02, initc-loss 1.7213e+04                    bc_loss 1.0205e+05\n",
      "Epoch 8000, Training-Loss 3.4854e+03, Data-loss 2.2315e+03                  , pde-loss 4.8817e+02, initc-loss 1.4585e+04                    bc_loss 1.1032e+05\n",
      "Epoch 8010, Training-Loss 1.9161e+03, Data-loss 1.3096e+03                  , pde-loss 2.2050e+03, initc-loss 1.5367e+04                    bc_loss 4.3079e+04\n",
      "Epoch 8020, Training-Loss 6.0315e+03, Data-loss 9.3698e+02                  , pde-loss 1.4726e+03, initc-loss 1.7849e+04                    bc_loss 4.9013e+05\n",
      "Epoch 8030, Training-Loss 7.7333e+03, Data-loss 3.9546e+03                  , pde-loss 4.9748e+02, initc-loss 1.3497e+04                    bc_loss 3.6388e+05\n",
      "Epoch 8040, Training-Loss 2.5917e+03, Data-loss 1.6927e+03                  , pde-loss 2.9196e+03, initc-loss 1.8438e+04                    bc_loss 6.8546e+04\n",
      "Epoch 8050, Training-Loss 1.9639e+03, Data-loss 1.4313e+03                  , pde-loss 1.8662e+03, initc-loss 1.3256e+04                    bc_loss 3.8138e+04\n",
      "Epoch 8060, Training-Loss 2.1633e+03, Data-loss 1.6173e+03                  , pde-loss 1.6052e+03, initc-loss 1.4445e+04                    bc_loss 3.8551e+04\n",
      "Epoch 8070, Training-Loss 3.3081e+03, Data-loss 2.4236e+03                  , pde-loss 1.2735e+03, initc-loss 1.4340e+04                    bc_loss 7.2841e+04\n",
      "Epoch 8080, Training-Loss 1.1307e+04, Data-loss 1.5474e+03                  , pde-loss 1.8037e+03, initc-loss 2.0177e+04                    bc_loss 9.5393e+05\n",
      "Epoch 8090, Training-Loss 6.0408e+03, Data-loss 4.6015e+03                  , pde-loss 5.6370e+03, initc-loss 1.0913e+04                    bc_loss 1.2737e+05\n",
      "Epoch 8100, Training-Loss 4.3910e+03, Data-loss 2.3051e+03                  , pde-loss 2.3373e+03, initc-loss 1.4505e+04                    bc_loss 1.9176e+05\n",
      "Epoch 8110, Training-Loss 6.3226e+03, Data-loss 4.0889e+03                  , pde-loss 2.9186e+02, initc-loss 2.1594e+04                    bc_loss 2.0148e+05\n",
      "Epoch 8120, Training-Loss 5.4186e+03, Data-loss 3.1964e+03                  , pde-loss 2.9182e+02, initc-loss 1.6740e+04                    bc_loss 2.0519e+05\n",
      "Epoch 8130, Training-Loss 3.0356e+03, Data-loss 2.4551e+03                  , pde-loss 1.7131e+03, initc-loss 1.5759e+04                    bc_loss 4.0577e+04\n",
      "Epoch 8140, Training-Loss 2.1561e+03, Data-loss 1.5685e+03                  , pde-loss 1.8305e+03, initc-loss 1.5764e+04                    bc_loss 4.1165e+04\n",
      "Epoch 8150, Training-Loss 3.5854e+03, Data-loss 2.0670e+03                  , pde-loss 1.1770e+03, initc-loss 1.4727e+04                    bc_loss 1.3594e+05\n",
      "Epoch 8160, Training-Loss 5.6702e+03, Data-loss 3.8796e+03                  , pde-loss 4.4164e+02, initc-loss 1.9227e+04                    bc_loss 1.5940e+05\n",
      "Epoch 8170, Training-Loss 3.5222e+03, Data-loss 2.3705e+03                  , pde-loss 1.0833e+03, initc-loss 1.7786e+04                    bc_loss 9.6299e+04\n",
      "Epoch 8180, Training-Loss 3.2502e+03, Data-loss 2.0690e+03                  , pde-loss 3.7141e+03, initc-loss 1.9874e+04                    bc_loss 9.4536e+04\n",
      "Epoch 8190, Training-Loss 6.2139e+03, Data-loss 2.9118e+03                  , pde-loss 3.7785e+03, initc-loss 2.1156e+04                    bc_loss 3.0527e+05\n",
      "Epoch 8200, Training-Loss 5.2310e+03, Data-loss 4.4410e+03                  , pde-loss 2.4562e+03, initc-loss 1.4967e+04                    bc_loss 6.1580e+04\n",
      "Epoch 8210, Training-Loss 5.6897e+03, Data-loss 3.7912e+03                  , pde-loss 7.2807e+02, initc-loss 2.0662e+04                    bc_loss 1.6846e+05\n",
      "Epoch 8220, Training-Loss 4.9012e+03, Data-loss 3.2109e+03                  , pde-loss 2.9396e+02, initc-loss 1.8547e+04                    bc_loss 1.5020e+05\n",
      "Epoch 8230, Training-Loss 4.0614e+03, Data-loss 3.2632e+03                  , pde-loss 2.2328e+03, initc-loss 1.4740e+04                    bc_loss 6.2841e+04\n",
      "Epoch 8240, Training-Loss 3.6237e+03, Data-loss 3.2112e+03                  , pde-loss 3.0519e+03, initc-loss 1.6496e+04                    bc_loss 2.1706e+04\n",
      "Epoch 8250, Training-Loss 2.2840e+03, Data-loss 1.7045e+03                  , pde-loss 1.9427e+03, initc-loss 1.4060e+04                    bc_loss 4.1940e+04\n",
      "Epoch 8260, Training-Loss 1.4975e+04, Data-loss 3.7124e+03                  , pde-loss 2.0417e+03, initc-loss 2.1767e+04                    bc_loss 1.1024e+06\n",
      "Epoch 8270, Training-Loss 1.0132e+04, Data-loss 5.5351e+03                  , pde-loss 5.2415e+03, initc-loss 9.7521e+03                    bc_loss 4.4472e+05\n",
      "Epoch 8280, Training-Loss 7.9505e+03, Data-loss 4.5782e+03                  , pde-loss 1.0042e+03, initc-loss 1.0375e+04                    bc_loss 3.2586e+05\n",
      "Epoch 8290, Training-Loss 1.1206e+04, Data-loss 5.2354e+03                  , pde-loss 4.8265e+02, initc-loss 2.4611e+04                    bc_loss 5.7195e+05\n",
      "Epoch 8300, Training-Loss 4.9166e+03, Data-loss 2.4641e+03                  , pde-loss 3.8989e+02, initc-loss 1.0870e+04                    bc_loss 2.3399e+05\n",
      "Epoch 8310, Training-Loss 4.5445e+03, Data-loss 3.3214e+03                  , pde-loss 3.8211e+03, initc-loss 1.5642e+04                    bc_loss 1.0284e+05\n",
      "Epoch 8320, Training-Loss 2.4003e+03, Data-loss 1.6107e+03                  , pde-loss 1.9889e+03, initc-loss 1.8056e+04                    bc_loss 5.8914e+04\n",
      "Epoch 8330, Training-Loss 1.8083e+03, Data-loss 1.0848e+03                  , pde-loss 1.4723e+03, initc-loss 1.4811e+04                    bc_loss 5.6060e+04\n",
      "Epoch 8340, Training-Loss 2.5947e+03, Data-loss 2.0829e+03                  , pde-loss 9.1150e+02, initc-loss 1.4372e+04                    bc_loss 3.5893e+04\n",
      "Epoch 8350, Training-Loss 2.5549e+03, Data-loss 1.9330e+03                  , pde-loss 1.0209e+03, initc-loss 1.4550e+04                    bc_loss 4.6623e+04\n",
      "Epoch 8360, Training-Loss 4.2866e+03, Data-loss 2.0780e+03                  , pde-loss 7.5333e+02, initc-loss 1.8211e+04                    bc_loss 2.0190e+05\n",
      "Epoch 8370, Training-Loss 4.3390e+03, Data-loss 2.9567e+03                  , pde-loss 8.1447e+02, initc-loss 1.6533e+04                    bc_loss 1.2088e+05\n",
      "Epoch 8380, Training-Loss 2.9432e+03, Data-loss 2.2778e+03                  , pde-loss 2.1904e+03, initc-loss 1.7603e+04                    bc_loss 4.6749e+04\n",
      "Epoch 8390, Training-Loss 3.8795e+03, Data-loss 2.8358e+03                  , pde-loss 2.5677e+03, initc-loss 1.8764e+04                    bc_loss 8.3044e+04\n",
      "Epoch 8400, Training-Loss 2.9870e+03, Data-loss 1.9245e+03                  , pde-loss 1.3483e+03, initc-loss 1.3198e+04                    bc_loss 9.1705e+04\n",
      "Epoch 8410, Training-Loss 1.0025e+04, Data-loss 5.1971e+03                  , pde-loss 3.6747e+02, initc-loss 2.3761e+04                    bc_loss 4.5863e+05\n",
      "Epoch 8420, Training-Loss 4.2457e+03, Data-loss 1.5501e+03                  , pde-loss 1.0970e+03, initc-loss 1.0060e+04                    bc_loss 2.5840e+05\n",
      "Epoch 8430, Training-Loss 4.5281e+03, Data-loss 3.1684e+03                  , pde-loss 5.3247e+03, initc-loss 1.6344e+04                    bc_loss 1.1431e+05\n",
      "Epoch 8440, Training-Loss 1.8666e+03, Data-loss 1.5117e+03                  , pde-loss 2.7253e+03, initc-loss 1.1788e+04                    bc_loss 2.0977e+04\n",
      "Epoch 8450, Training-Loss 2.3640e+03, Data-loss 1.5687e+03                  , pde-loss 1.3058e+03, initc-loss 1.0620e+04                    bc_loss 6.7601e+04\n",
      "Epoch 8460, Training-Loss 2.2254e+03, Data-loss 1.9093e+03                  , pde-loss 9.9120e+02, initc-loss 1.5300e+04                    bc_loss 1.5325e+04\n",
      "Epoch 8470, Training-Loss 3.2377e+03, Data-loss 2.4220e+03                  , pde-loss 1.5174e+03, initc-loss 1.9717e+04                    bc_loss 6.0327e+04\n",
      "Epoch 8480, Training-Loss 9.3885e+03, Data-loss 4.2881e+03                  , pde-loss 3.0785e+03, initc-loss 2.6933e+04                    bc_loss 4.8003e+05\n",
      "Epoch 8490, Training-Loss 5.2382e+03, Data-loss 2.5801e+03                  , pde-loss 3.2856e+03, initc-loss 1.9690e+04                    bc_loss 2.4283e+05\n",
      "Epoch 8500, Training-Loss 5.0854e+03, Data-loss 3.0337e+03                  , pde-loss 7.3806e+02, initc-loss 1.9501e+04                    bc_loss 1.8493e+05\n",
      "Epoch 8510, Training-Loss 4.6133e+03, Data-loss 2.7511e+03                  , pde-loss 5.1861e+02, initc-loss 1.8661e+04                    bc_loss 1.6704e+05\n",
      "Epoch 8520, Training-Loss 1.9261e+03, Data-loss 1.1266e+03                  , pde-loss 2.0683e+03, initc-loss 1.5195e+04                    bc_loss 6.2690e+04\n",
      "Epoch 8530, Training-Loss 2.3855e+03, Data-loss 1.8064e+03                  , pde-loss 1.9737e+03, initc-loss 1.2203e+04                    bc_loss 4.3733e+04\n",
      "Epoch 8540, Training-Loss 1.0019e+04, Data-loss 2.3472e+03                  , pde-loss 7.4310e+02, initc-loss 2.1327e+04                    bc_loss 7.4509e+05\n",
      "Epoch 8550, Training-Loss 5.2113e+03, Data-loss 3.3173e+03                  , pde-loss 2.2481e+02, initc-loss 1.9392e+04                    bc_loss 1.6978e+05\n",
      "Epoch 8560, Training-Loss 6.2675e+03, Data-loss 4.4253e+03                  , pde-loss 1.1486e+03, initc-loss 1.6165e+04                    bc_loss 1.6691e+05\n",
      "Epoch 8570, Training-Loss 4.2885e+03, Data-loss 2.8846e+03                  , pde-loss 4.2786e+03, initc-loss 2.0986e+04                    bc_loss 1.1512e+05\n",
      "Epoch 8580, Training-Loss 2.6978e+03, Data-loss 1.4756e+03                  , pde-loss 3.5779e+03, initc-loss 1.7354e+04                    bc_loss 1.0128e+05\n",
      "Epoch 8590, Training-Loss 7.1487e+03, Data-loss 3.7379e+03                  , pde-loss 7.1006e+02, initc-loss 1.8629e+04                    bc_loss 3.2174e+05\n",
      "Epoch 8600, Training-Loss 8.7326e+03, Data-loss 4.5230e+03                  , pde-loss 2.7778e+02, initc-loss 1.8757e+04                    bc_loss 4.0192e+05\n",
      "Epoch 8610, Training-Loss 3.8753e+03, Data-loss 2.8941e+03                  , pde-loss 1.4919e+03, initc-loss 1.3817e+04                    bc_loss 8.2803e+04\n",
      "Epoch 8620, Training-Loss 4.1666e+03, Data-loss 2.7815e+03                  , pde-loss 3.2356e+03, initc-loss 1.1614e+04                    bc_loss 1.2367e+05\n",
      "Epoch 8630, Training-Loss 1.7928e+03, Data-loss 1.0684e+03                  , pde-loss 1.2362e+03, initc-loss 1.4491e+04                    bc_loss 5.6711e+04\n",
      "Epoch 8640, Training-Loss 4.8563e+03, Data-loss 3.0074e+03                  , pde-loss 7.2758e+02, initc-loss 1.5360e+04                    bc_loss 1.6881e+05\n",
      "Epoch 8650, Training-Loss 3.2071e+03, Data-loss 1.3398e+03                  , pde-loss 6.2013e+02, initc-loss 1.3614e+04                    bc_loss 1.7249e+05\n",
      "Epoch 8660, Training-Loss 1.9985e+03, Data-loss 1.6131e+03                  , pde-loss 2.8925e+03, initc-loss 1.3529e+04                    bc_loss 2.2115e+04\n",
      "Epoch 8670, Training-Loss 3.0934e+03, Data-loss 2.2199e+03                  , pde-loss 3.4952e+03, initc-loss 1.6219e+04                    bc_loss 6.7641e+04\n",
      "Epoch 8680, Training-Loss 2.0844e+03, Data-loss 1.5138e+03                  , pde-loss 1.6322e+03, initc-loss 1.4548e+04                    bc_loss 4.0871e+04\n",
      "Epoch 8690, Training-Loss 1.0022e+04, Data-loss 4.6477e+03                  , pde-loss 2.1864e+03, initc-loss 2.3815e+04                    bc_loss 5.1138e+05\n",
      "Epoch 8700, Training-Loss 3.5708e+03, Data-loss 2.1913e+03                  , pde-loss 2.0759e+03, initc-loss 1.4207e+04                    bc_loss 1.2167e+05\n",
      "Epoch 8710, Training-Loss 3.6910e+03, Data-loss 2.4816e+03                  , pde-loss 5.8555e+02, initc-loss 1.7662e+04                    bc_loss 1.0269e+05\n",
      "Epoch 8720, Training-Loss 4.5696e+03, Data-loss 2.7768e+03                  , pde-loss 3.6714e+02, initc-loss 2.1215e+04                    bc_loss 1.5771e+05\n",
      "Epoch 8730, Training-Loss 3.3506e+03, Data-loss 2.7818e+03                  , pde-loss 2.4613e+03, initc-loss 1.8815e+04                    bc_loss 3.5604e+04\n",
      "Epoch 8740, Training-Loss 2.2967e+03, Data-loss 1.6650e+03                  , pde-loss 2.0558e+03, initc-loss 1.3732e+04                    bc_loss 4.7390e+04\n",
      "Epoch 8750, Training-Loss 4.0525e+03, Data-loss 2.0400e+03                  , pde-loss 9.7328e+02, initc-loss 1.9018e+04                    bc_loss 1.8126e+05\n",
      "Epoch 8760, Training-Loss 8.8100e+03, Data-loss 6.6428e+03                  , pde-loss 3.0790e+02, initc-loss 2.1925e+04                    bc_loss 1.9449e+05\n",
      "Epoch 8770, Training-Loss 6.5014e+03, Data-loss 3.4155e+03                  , pde-loss 1.5987e+03, initc-loss 2.1330e+04                    bc_loss 2.8567e+05\n",
      "Epoch 8780, Training-Loss 6.6304e+03, Data-loss 3.6313e+03                  , pde-loss 4.8893e+03, initc-loss 2.3315e+04                    bc_loss 2.7171e+05\n",
      "Epoch 8790, Training-Loss 2.8312e+03, Data-loss 2.2414e+03                  , pde-loss 2.3056e+03, initc-loss 1.4173e+04                    bc_loss 4.2506e+04\n",
      "Epoch 8800, Training-Loss 4.5455e+03, Data-loss 2.7093e+03                  , pde-loss 7.7418e+02, initc-loss 1.9959e+04                    bc_loss 1.6288e+05\n",
      "Epoch 8810, Training-Loss 3.0347e+03, Data-loss 2.1933e+03                  , pde-loss 5.1562e+02, initc-loss 1.3641e+04                    bc_loss 6.9988e+04\n",
      "Epoch 8820, Training-Loss 5.2829e+03, Data-loss 2.0943e+03                  , pde-loss 2.4565e+03, initc-loss 1.7952e+04                    bc_loss 2.9845e+05\n",
      "Epoch 8830, Training-Loss 8.0715e+03, Data-loss 4.7946e+03                  , pde-loss 5.6162e+03, initc-loss 1.9770e+04                    bc_loss 3.0230e+05\n",
      "Epoch 8840, Training-Loss 4.8167e+03, Data-loss 2.6055e+03                  , pde-loss 1.5241e+03, initc-loss 1.4627e+04                    bc_loss 2.0497e+05\n",
      "Epoch 8850, Training-Loss 1.1605e+04, Data-loss 7.0330e+03                  , pde-loss 4.9927e+02, initc-loss 2.4647e+04                    bc_loss 4.3201e+05\n",
      "Epoch 8860, Training-Loss 6.2345e+03, Data-loss 3.9674e+03                  , pde-loss 6.8616e+02, initc-loss 1.6730e+04                    bc_loss 2.0929e+05\n",
      "Epoch 8870, Training-Loss 3.8526e+03, Data-loss 2.5612e+03                  , pde-loss 3.2208e+03, initc-loss 1.4823e+04                    bc_loss 1.1110e+05\n",
      "Epoch 8880, Training-Loss 1.4884e+03, Data-loss 1.0651e+03                  , pde-loss 1.5074e+03, initc-loss 1.7067e+04                    bc_loss 2.3751e+04\n",
      "Epoch 8890, Training-Loss 4.3264e+03, Data-loss 1.9793e+03                  , pde-loss 9.1651e+02, initc-loss 1.9854e+04                    bc_loss 2.1394e+05\n",
      "Epoch 8900, Training-Loss 6.5804e+03, Data-loss 3.0797e+03                  , pde-loss 8.4674e+02, initc-loss 1.4790e+04                    bc_loss 3.3443e+05\n",
      "Epoch 8910, Training-Loss 2.0715e+03, Data-loss 1.7728e+03                  , pde-loss 2.9053e+03, initc-loss 1.8674e+04                    bc_loss 8.2893e+03\n",
      "Epoch 8920, Training-Loss 1.6107e+03, Data-loss 1.1865e+03                  , pde-loss 2.5615e+03, initc-loss 1.6569e+04                    bc_loss 2.3296e+04\n",
      "Epoch 8930, Training-Loss 2.8216e+03, Data-loss 2.4886e+03                  , pde-loss 1.2316e+03, initc-loss 1.6200e+04                    bc_loss 1.5865e+04\n",
      "Epoch 8940, Training-Loss 2.3031e+03, Data-loss 1.9411e+03                  , pde-loss 9.4050e+02, initc-loss 1.2098e+04                    bc_loss 2.3156e+04\n",
      "Epoch 8950, Training-Loss 2.7478e+03, Data-loss 1.5493e+03                  , pde-loss 1.3738e+03, initc-loss 1.4392e+04                    bc_loss 1.0408e+05\n",
      "Epoch 8960, Training-Loss 5.4502e+03, Data-loss 2.7651e+03                  , pde-loss 3.3538e+03, initc-loss 2.0244e+04                    bc_loss 2.4491e+05\n",
      "Epoch 8970, Training-Loss 5.9420e+03, Data-loss 4.2227e+03                  , pde-loss 3.1208e+03, initc-loss 1.1355e+04                    bc_loss 1.5746e+05\n",
      "Epoch 8980, Training-Loss 6.1279e+03, Data-loss 1.8601e+03                  , pde-loss 1.4947e+03, initc-loss 9.9882e+03                    bc_loss 4.1530e+05\n",
      "Epoch 8990, Training-Loss 4.3567e+03, Data-loss 3.0607e+03                  , pde-loss 2.4984e+02, initc-loss 1.8165e+04                    bc_loss 1.1118e+05\n",
      "Epoch 9000, Training-Loss 3.7141e+03, Data-loss 2.1767e+03                  , pde-loss 1.0985e+03, initc-loss 1.6772e+04                    bc_loss 1.3587e+05\n",
      "Epoch 9010, Training-Loss 2.9171e+03, Data-loss 2.4796e+03                  , pde-loss 2.2661e+03, initc-loss 1.8122e+04                    bc_loss 2.3362e+04\n",
      "Epoch 9020, Training-Loss 1.8083e+03, Data-loss 1.3011e+03                  , pde-loss 1.7603e+03, initc-loss 1.7444e+04                    bc_loss 3.1518e+04\n",
      "Epoch 9030, Training-Loss 2.8780e+03, Data-loss 1.7192e+03                  , pde-loss 1.0256e+03, initc-loss 1.6036e+04                    bc_loss 9.8822e+04\n",
      "Epoch 9040, Training-Loss 1.8262e+04, Data-loss 1.3599e+04                  , pde-loss 1.3971e+03, initc-loss 5.0873e+04                    bc_loss 4.1407e+05\n",
      "Epoch 9050, Training-Loss 8.2955e+03, Data-loss 7.6800e+03                  , pde-loss 1.0153e+03, initc-loss 1.2349e+04                    bc_loss 4.8182e+04\n",
      "Epoch 9060, Training-Loss 5.7349e+03, Data-loss 4.1940e+03                  , pde-loss 5.3845e+02, initc-loss 1.3020e+04                    bc_loss 1.4053e+05\n",
      "Epoch 9070, Training-Loss 3.7073e+03, Data-loss 2.5925e+03                  , pde-loss 2.3570e+03, initc-loss 1.2299e+04                    bc_loss 9.6824e+04\n",
      "Epoch 9080, Training-Loss 2.2981e+03, Data-loss 1.5024e+03                  , pde-loss 2.1495e+03, initc-loss 1.4414e+04                    bc_loss 6.3009e+04\n",
      "Epoch 9090, Training-Loss 3.3205e+03, Data-loss 1.6417e+03                  , pde-loss 1.2810e+03, initc-loss 1.5844e+04                    bc_loss 1.5076e+05\n",
      "Epoch 9100, Training-Loss 3.7861e+03, Data-loss 2.7181e+03                  , pde-loss 2.0815e+03, initc-loss 1.4706e+04                    bc_loss 9.0018e+04\n",
      "Epoch 9110, Training-Loss 7.5678e+03, Data-loss 3.4808e+03                  , pde-loss 3.7254e+03, initc-loss 1.6792e+04                    bc_loss 3.8818e+05\n",
      "Epoch 9120, Training-Loss 6.7700e+03, Data-loss 3.5762e+03                  , pde-loss 1.9158e+03, initc-loss 2.1617e+04                    bc_loss 2.9585e+05\n",
      "Epoch 9130, Training-Loss 1.0220e+04, Data-loss 3.4765e+03                  , pde-loss 3.5738e+02, initc-loss 1.6506e+04                    bc_loss 6.5752e+05\n",
      "Epoch 9140, Training-Loss 1.0895e+04, Data-loss 4.7237e+03                  , pde-loss 3.0746e+02, initc-loss 2.0058e+04                    bc_loss 5.9678e+05\n",
      "Epoch 9150, Training-Loss 7.8644e+03, Data-loss 3.6655e+03                  , pde-loss 6.2781e+02, initc-loss 1.3212e+04                    bc_loss 4.0605e+05\n",
      "Epoch 9160, Training-Loss 5.4639e+03, Data-loss 4.3857e+03                  , pde-loss 5.0195e+03, initc-loss 2.4162e+04                    bc_loss 7.8634e+04\n",
      "Epoch 9170, Training-Loss 2.8545e+03, Data-loss 2.5125e+03                  , pde-loss 3.1929e+03, initc-loss 1.5011e+04                    bc_loss 1.5999e+04\n",
      "Epoch 9180, Training-Loss 3.9970e+03, Data-loss 3.2454e+03                  , pde-loss 1.1690e+03, initc-loss 1.5687e+04                    bc_loss 5.8302e+04\n",
      "Epoch 9190, Training-Loss 3.0312e+03, Data-loss 2.4077e+03                  , pde-loss 9.2159e+02, initc-loss 1.4211e+04                    bc_loss 4.7216e+04\n",
      "Epoch 9200, Training-Loss 2.2015e+03, Data-loss 1.7209e+03                  , pde-loss 7.2289e+02, initc-loss 1.4914e+04                    bc_loss 3.2425e+04\n",
      "Epoch 9210, Training-Loss 1.8086e+03, Data-loss 1.2248e+03                  , pde-loss 1.2532e+03, initc-loss 1.4998e+04                    bc_loss 4.2122e+04\n",
      "Epoch 9220, Training-Loss 5.5519e+03, Data-loss 1.8656e+03                  , pde-loss 1.3735e+03, initc-loss 1.6182e+04                    bc_loss 3.5108e+05\n",
      "Epoch 9230, Training-Loss 4.4974e+03, Data-loss 2.1099e+03                  , pde-loss 6.4687e+02, initc-loss 2.0169e+04                    bc_loss 2.1794e+05\n",
      "Epoch 9240, Training-Loss 3.2322e+03, Data-loss 2.8284e+03                  , pde-loss 1.1662e+03, initc-loss 1.6404e+04                    bc_loss 2.2806e+04\n",
      "Epoch 9250, Training-Loss 2.5896e+03, Data-loss 1.3576e+03                  , pde-loss 2.1000e+03, initc-loss 1.5655e+04                    bc_loss 1.0544e+05\n",
      "Epoch 9260, Training-Loss 1.7176e+03, Data-loss 1.0403e+03                  , pde-loss 2.6312e+03, initc-loss 1.2954e+04                    bc_loss 5.2144e+04\n",
      "Epoch 9270, Training-Loss 2.1889e+03, Data-loss 1.0802e+03                  , pde-loss 2.0265e+03, initc-loss 1.5367e+04                    bc_loss 9.3482e+04\n",
      "Epoch 9280, Training-Loss 9.8881e+03, Data-loss 4.9168e+03                  , pde-loss 3.9500e+02, initc-loss 2.8682e+04                    bc_loss 4.6805e+05\n",
      "Epoch 9290, Training-Loss 8.6668e+03, Data-loss 5.0949e+03                  , pde-loss 5.5473e+02, initc-loss 1.4647e+04                    bc_loss 3.4198e+05\n",
      "Epoch 9300, Training-Loss 4.3561e+03, Data-loss 3.0146e+03                  , pde-loss 3.7916e+03, initc-loss 1.3361e+04                    bc_loss 1.1700e+05\n",
      "Epoch 9310, Training-Loss 2.7380e+03, Data-loss 1.9435e+03                  , pde-loss 2.7480e+03, initc-loss 1.4956e+04                    bc_loss 6.1744e+04\n",
      "Epoch 9320, Training-Loss 3.4198e+03, Data-loss 2.8322e+03                  , pde-loss 8.0353e+02, initc-loss 1.5648e+04                    bc_loss 4.2318e+04\n",
      "Epoch 9330, Training-Loss 3.6335e+03, Data-loss 2.6342e+03                  , pde-loss 8.0573e+02, initc-loss 1.6575e+04                    bc_loss 8.2545e+04\n",
      "Epoch 9340, Training-Loss 3.0605e+03, Data-loss 2.2874e+03                  , pde-loss 1.8713e+03, initc-loss 1.1409e+04                    bc_loss 6.4034e+04\n",
      "Epoch 9350, Training-Loss 1.0955e+04, Data-loss 4.0865e+03                  , pde-loss 4.0503e+03, initc-loss 2.5225e+04                    bc_loss 6.5761e+05\n",
      "Epoch 9360, Training-Loss 2.7304e+03, Data-loss 1.9904e+03                  , pde-loss 3.4759e+03, initc-loss 1.4795e+04                    bc_loss 5.5727e+04\n",
      "Epoch 9370, Training-Loss 2.5114e+03, Data-loss 1.6769e+03                  , pde-loss 1.3756e+03, initc-loss 1.4094e+04                    bc_loss 6.7978e+04\n",
      "Epoch 9380, Training-Loss 2.6561e+03, Data-loss 1.9896e+03                  , pde-loss 1.2395e+03, initc-loss 1.5371e+04                    bc_loss 5.0048e+04\n",
      "Epoch 9390, Training-Loss 2.1592e+03, Data-loss 1.4119e+03                  , pde-loss 2.1001e+03, initc-loss 1.6265e+04                    bc_loss 5.6363e+04\n",
      "Epoch 9400, Training-Loss 1.1677e+04, Data-loss 4.1200e+03                  , pde-loss 2.6903e+03, initc-loss 2.6370e+04                    bc_loss 7.2662e+05\n",
      "Epoch 9410, Training-Loss 9.7275e+03, Data-loss 3.8800e+03                  , pde-loss 2.6356e+03, initc-loss 6.1479e+03                    bc_loss 5.7596e+05\n",
      "Epoch 9420, Training-Loss 1.2857e+04, Data-loss 4.5046e+03                  , pde-loss 5.6529e+02, initc-loss 2.0188e+04                    bc_loss 8.1451e+05\n",
      "Epoch 9430, Training-Loss 5.2730e+03, Data-loss 2.9998e+03                  , pde-loss 1.6877e+02, initc-loss 1.6622e+04                    bc_loss 2.1053e+05\n",
      "Epoch 9440, Training-Loss 4.1130e+03, Data-loss 2.6358e+03                  , pde-loss 2.7062e+03, initc-loss 1.8090e+04                    bc_loss 1.2693e+05\n",
      "Epoch 9450, Training-Loss 2.9277e+03, Data-loss 2.3612e+03                  , pde-loss 1.4453e+03, initc-loss 1.9278e+04                    bc_loss 3.5923e+04\n",
      "Epoch 9460, Training-Loss 4.0242e+03, Data-loss 3.2629e+03                  , pde-loss 6.9485e+02, initc-loss 1.7974e+04                    bc_loss 5.7457e+04\n",
      "Epoch 9470, Training-Loss 4.2413e+03, Data-loss 1.5587e+03                  , pde-loss 1.1638e+03, initc-loss 1.4491e+04                    bc_loss 2.5260e+05\n",
      "Epoch 9480, Training-Loss 1.2152e+03, Data-loss 8.4437e+02                  , pde-loss 1.8339e+03, initc-loss 1.5587e+04                    bc_loss 1.9667e+04\n",
      "Epoch 9490, Training-Loss 1.4522e+03, Data-loss 1.0316e+03                  , pde-loss 2.1548e+03, initc-loss 1.5035e+04                    bc_loss 2.4872e+04\n",
      "Epoch 9500, Training-Loss 2.2131e+03, Data-loss 1.7702e+03                  , pde-loss 1.6988e+03, initc-loss 1.7155e+04                    bc_loss 2.5429e+04\n",
      "Epoch 9510, Training-Loss 8.5026e+03, Data-loss 1.9226e+03                  , pde-loss 1.1116e+03, initc-loss 1.7850e+04                    bc_loss 6.3903e+05\n",
      "Epoch 9520, Training-Loss 2.9644e+03, Data-loss 1.9188e+03                  , pde-loss 8.3064e+02, initc-loss 1.0383e+04                    bc_loss 9.3349e+04\n",
      "Epoch 9530, Training-Loss 2.3537e+03, Data-loss 1.5676e+03                  , pde-loss 1.8622e+03, initc-loss 1.4191e+04                    bc_loss 6.2553e+04\n",
      "Epoch 9540, Training-Loss 1.3527e+03, Data-loss 9.6991e+02                  , pde-loss 1.4002e+03, initc-loss 1.3630e+04                    bc_loss 2.3250e+04\n",
      "Epoch 9550, Training-Loss 6.8322e+02, Data-loss 3.9871e+02                  , pde-loss 1.4040e+03, initc-loss 1.5577e+04                    bc_loss 1.1470e+04\n",
      "Epoch 9560, Training-Loss 1.7425e+03, Data-loss 1.0053e+03                  , pde-loss 1.4913e+03, initc-loss 1.4405e+04                    bc_loss 5.7829e+04\n",
      "Epoch 9570, Training-Loss 9.7647e+02, Data-loss 3.3473e+02                  , pde-loss 1.2419e+03, initc-loss 1.2028e+04                    bc_loss 5.0904e+04\n",
      "Epoch 9580, Training-Loss 1.9283e+03, Data-loss 1.2365e+03                  , pde-loss 2.0218e+03, initc-loss 1.4825e+04                    bc_loss 5.2331e+04\n",
      "Epoch 9590, Training-Loss 2.9234e+03, Data-loss 1.8797e+03                  , pde-loss 2.5117e+03, initc-loss 1.6992e+04                    bc_loss 8.4862e+04\n",
      "Epoch 9600, Training-Loss 3.7281e+03, Data-loss 1.8320e+03                  , pde-loss 2.8398e+03, initc-loss 1.9043e+04                    bc_loss 1.6773e+05\n",
      "Epoch 9610, Training-Loss 2.3943e+03, Data-loss 1.4927e+03                  , pde-loss 1.4440e+03, initc-loss 1.2533e+04                    bc_loss 7.6184e+04\n",
      "Epoch 9620, Training-Loss 7.6423e+03, Data-loss 1.3344e+03                  , pde-loss 7.1053e+02, initc-loss 1.4869e+04                    bc_loss 6.1520e+05\n",
      "Epoch 9630, Training-Loss 2.5201e+03, Data-loss 1.1301e+03                  , pde-loss 4.9839e+02, initc-loss 1.2765e+04                    bc_loss 1.2574e+05\n",
      "Epoch 9640, Training-Loss 1.5289e+03, Data-loss 1.0064e+03                  , pde-loss 1.2702e+03, initc-loss 1.1999e+04                    bc_loss 3.8975e+04\n",
      "Epoch 9650, Training-Loss 2.5883e+03, Data-loss 1.9170e+03                  , pde-loss 1.5257e+03, initc-loss 1.4537e+04                    bc_loss 5.1066e+04\n",
      "Epoch 9660, Training-Loss 4.1614e+03, Data-loss 9.3511e+02                  , pde-loss 1.1220e+03, initc-loss 1.6617e+04                    bc_loss 3.0489e+05\n",
      "Epoch 9670, Training-Loss 5.7642e+03, Data-loss 4.0693e+03                  , pde-loss 4.7776e+02, initc-loss 2.2152e+04                    bc_loss 1.4686e+05\n",
      "Epoch 9680, Training-Loss 3.1577e+03, Data-loss 2.5107e+03                  , pde-loss 1.1731e+03, initc-loss 1.3020e+04                    bc_loss 5.0508e+04\n",
      "Epoch 9690, Training-Loss 2.1375e+03, Data-loss 1.2893e+03                  , pde-loss 1.8514e+03, initc-loss 1.3429e+04                    bc_loss 6.9542e+04\n",
      "Epoch 9700, Training-Loss 3.8161e+03, Data-loss 3.2661e+03                  , pde-loss 1.2321e+03, initc-loss 1.5739e+04                    bc_loss 3.8035e+04\n",
      "Epoch 9710, Training-Loss 2.9168e+03, Data-loss 2.2119e+03                  , pde-loss 1.0286e+03, initc-loss 1.4213e+04                    bc_loss 5.5248e+04\n",
      "Epoch 9720, Training-Loss 8.9294e+03, Data-loss 5.5224e+03                  , pde-loss 7.2870e+02, initc-loss 3.2896e+04                    bc_loss 3.0708e+05\n",
      "Epoch 9730, Training-Loss 6.8172e+03, Data-loss 3.2040e+03                  , pde-loss 1.6996e+03, initc-loss 1.5824e+04                    bc_loss 3.4380e+05\n",
      "Epoch 9740, Training-Loss 1.0350e+04, Data-loss 5.3935e+03                  , pde-loss 7.7703e+03, initc-loss 2.3491e+04                    bc_loss 4.6441e+05\n",
      "Epoch 9750, Training-Loss 6.6057e+03, Data-loss 3.2499e+03                  , pde-loss 3.7586e+03, initc-loss 1.0406e+04                    bc_loss 3.2142e+05\n",
      "Epoch 9760, Training-Loss 4.0419e+03, Data-loss 2.7908e+03                  , pde-loss 5.2887e+02, initc-loss 1.5237e+04                    bc_loss 1.0935e+05\n",
      "Epoch 9770, Training-Loss 1.8950e+03, Data-loss 1.2069e+03                  , pde-loss 1.2057e+03, initc-loss 1.3640e+04                    bc_loss 5.3964e+04\n",
      "Epoch 9780, Training-Loss 2.7246e+03, Data-loss 1.5356e+03                  , pde-loss 1.1003e+03, initc-loss 1.5164e+04                    bc_loss 1.0264e+05\n",
      "Epoch 9790, Training-Loss 1.8559e+03, Data-loss 1.4630e+03                  , pde-loss 1.0159e+03, initc-loss 1.5079e+04                    bc_loss 2.3203e+04\n",
      "Epoch 9800, Training-Loss 1.5205e+03, Data-loss 9.9337e+02                  , pde-loss 1.6136e+03, initc-loss 1.1381e+04                    bc_loss 3.9719e+04\n",
      "Epoch 9810, Training-Loss 2.7125e+03, Data-loss 1.1496e+03                  , pde-loss 1.2704e+03, initc-loss 1.4358e+04                    bc_loss 1.4067e+05\n",
      "Epoch 9820, Training-Loss 4.8325e+03, Data-loss 1.7362e+03                  , pde-loss 1.1329e+03, initc-loss 1.6087e+04                    bc_loss 2.9242e+05\n",
      "Epoch 9830, Training-Loss 2.2237e+03, Data-loss 1.0019e+03                  , pde-loss 1.3395e+03, initc-loss 1.3708e+04                    bc_loss 1.0714e+05\n",
      "Epoch 9840, Training-Loss 2.0746e+03, Data-loss 1.4563e+03                  , pde-loss 2.1440e+03, initc-loss 1.5012e+04                    bc_loss 4.4671e+04\n",
      "Epoch 9850, Training-Loss 1.9946e+03, Data-loss 1.5512e+03                  , pde-loss 2.1254e+03, initc-loss 1.4593e+04                    bc_loss 2.7620e+04\n",
      "Epoch 9860, Training-Loss 2.0251e+03, Data-loss 1.3900e+03                  , pde-loss 1.8089e+03, initc-loss 1.5552e+04                    bc_loss 4.6146e+04\n",
      "Epoch 9870, Training-Loss 1.0679e+03, Data-loss 5.9693e+02                  , pde-loss 1.6671e+03, initc-loss 1.4502e+04                    bc_loss 3.0924e+04\n",
      "Epoch 9880, Training-Loss 1.8341e+03, Data-loss 8.8896e+02                  , pde-loss 1.7800e+03, initc-loss 1.5171e+04                    bc_loss 7.7560e+04\n",
      "Epoch 9890, Training-Loss 3.7856e+03, Data-loss 1.3425e+03                  , pde-loss 2.7395e+03, initc-loss 1.6889e+04                    bc_loss 2.2468e+05\n",
      "Epoch 9900, Training-Loss 2.2650e+03, Data-loss 1.4906e+03                  , pde-loss 1.6317e+03, initc-loss 1.9328e+04                    bc_loss 5.6479e+04\n",
      "Epoch 9910, Training-Loss 1.4075e+03, Data-loss 1.0152e+03                  , pde-loss 1.4050e+03, initc-loss 1.4900e+04                    bc_loss 2.2931e+04\n",
      "Epoch 9920, Training-Loss 1.3679e+03, Data-loss 9.5695e+02                  , pde-loss 1.4939e+03, initc-loss 1.5524e+04                    bc_loss 2.4074e+04\n",
      "Epoch 9930, Training-Loss 8.5497e+03, Data-loss 3.9551e+03                  , pde-loss 4.2399e+02, initc-loss 2.3704e+04                    bc_loss 4.3533e+05\n",
      "Epoch 9940, Training-Loss 3.4781e+03, Data-loss 3.0085e+03                  , pde-loss 5.4576e+02, initc-loss 2.0526e+04                    bc_loss 2.5886e+04\n",
      "Epoch 9950, Training-Loss 1.2374e+03, Data-loss 5.8916e+02                  , pde-loss 1.2561e+03, initc-loss 1.5247e+04                    bc_loss 4.8324e+04\n",
      "Epoch 9960, Training-Loss 3.5286e+03, Data-loss 2.4010e+03                  , pde-loss 8.7337e+02, initc-loss 1.3060e+04                    bc_loss 9.8832e+04\n",
      "Epoch 9970, Training-Loss 2.2824e+03, Data-loss 1.0934e+03                  , pde-loss 1.1570e+03, initc-loss 1.3841e+04                    bc_loss 1.0390e+05\n",
      "Epoch 9980, Training-Loss 2.8877e+03, Data-loss 2.4964e+03                  , pde-loss 2.6312e+03, initc-loss 1.7285e+04                    bc_loss 1.9214e+04\n",
      "Epoch 9990, Training-Loss 2.0127e+03, Data-loss 1.5592e+03                  , pde-loss 2.2034e+03, initc-loss 1.5253e+04                    bc_loss 2.7887e+04\n",
      "Epoch 10000, Training-Loss 1.5005e+03, Data-loss 7.0408e+02                  , pde-loss 1.3755e+03, initc-loss 1.4473e+04                    bc_loss 6.3794e+04\n",
      "Epoch 10010, Training-Loss 8.2421e+03, Data-loss 5.1234e+03                  , pde-loss 2.7486e+03, initc-loss 3.2771e+04                    bc_loss 2.7635e+05\n",
      "Epoch 10020, Training-Loss 5.3201e+03, Data-loss 3.3723e+03                  , pde-loss 2.7880e+03, initc-loss 1.5003e+04                    bc_loss 1.7699e+05\n",
      "Epoch 10030, Training-Loss 5.7861e+03, Data-loss 2.3528e+03                  , pde-loss 6.1091e+02, initc-loss 1.2857e+04                    bc_loss 3.2986e+05\n",
      "Epoch 10040, Training-Loss 4.3405e+03, Data-loss 2.5959e+03                  , pde-loss 8.6096e+02, initc-loss 1.1291e+04                    bc_loss 1.6231e+05\n",
      "Epoch 10050, Training-Loss 3.2009e+03, Data-loss 2.2735e+03                  , pde-loss 2.8566e+03, initc-loss 1.7597e+04                    bc_loss 7.2277e+04\n",
      "Epoch 10060, Training-Loss 4.4537e+03, Data-loss 2.0464e+03                  , pde-loss 2.9704e+03, initc-loss 1.6911e+04                    bc_loss 2.2085e+05\n",
      "Epoch 10070, Training-Loss 7.5685e+03, Data-loss 3.8408e+03                  , pde-loss 3.7937e+03, initc-loss 1.3979e+04                    bc_loss 3.5500e+05\n",
      "Epoch 10080, Training-Loss 4.3145e+03, Data-loss 2.9698e+03                  , pde-loss 6.8283e+02, initc-loss 1.8711e+04                    bc_loss 1.1508e+05\n",
      "Epoch 10090, Training-Loss 1.0732e+04, Data-loss 4.1930e+03                  , pde-loss 2.7977e+02, initc-loss 2.4540e+04                    bc_loss 6.2912e+05\n",
      "Epoch 10100, Training-Loss 2.9961e+03, Data-loss 2.2702e+03                  , pde-loss 7.9390e+02, initc-loss 1.3170e+04                    bc_loss 5.8625e+04\n",
      "Epoch 10110, Training-Loss 3.6046e+03, Data-loss 2.6518e+03                  , pde-loss 2.7780e+03, initc-loss 1.3542e+04                    bc_loss 7.8967e+04\n",
      "Epoch 10120, Training-Loss 2.5132e+03, Data-loss 1.9251e+03                  , pde-loss 1.3897e+03, initc-loss 1.7710e+04                    bc_loss 3.9707e+04\n",
      "Epoch 10130, Training-Loss 2.0481e+03, Data-loss 1.4659e+03                  , pde-loss 1.2353e+03, initc-loss 1.3980e+04                    bc_loss 4.3001e+04\n",
      "Epoch 10140, Training-Loss 2.8319e+03, Data-loss 1.9390e+03                  , pde-loss 1.7183e+03, initc-loss 1.5964e+04                    bc_loss 7.1608e+04\n",
      "Epoch 10150, Training-Loss 2.5662e+03, Data-loss 1.7785e+03                  , pde-loss 2.3541e+03, initc-loss 1.6076e+04                    bc_loss 6.0341e+04\n",
      "Epoch 10160, Training-Loss 1.6269e+04, Data-loss 7.0638e+03                  , pde-loss 1.0211e+03, initc-loss 3.6459e+04                    bc_loss 8.8304e+05\n",
      "Epoch 10170, Training-Loss 5.3490e+03, Data-loss 4.1866e+03                  , pde-loss 1.7659e+02, initc-loss 9.1076e+03                    bc_loss 1.0696e+05\n",
      "Epoch 10180, Training-Loss 6.7217e+03, Data-loss 3.9674e+03                  , pde-loss 6.9805e+02, initc-loss 9.0144e+03                    bc_loss 2.6572e+05\n",
      "Epoch 10190, Training-Loss 4.0930e+03, Data-loss 3.1185e+03                  , pde-loss 4.4459e+03, initc-loss 1.7472e+04                    bc_loss 7.5527e+04\n",
      "Epoch 10200, Training-Loss 3.9727e+03, Data-loss 3.0852e+03                  , pde-loss 4.1712e+03, initc-loss 1.5741e+04                    bc_loss 6.8837e+04\n",
      "Epoch 10210, Training-Loss 1.7100e+03, Data-loss 1.3785e+03                  , pde-loss 1.5403e+03, initc-loss 1.0501e+04                    bc_loss 2.1110e+04\n",
      "Epoch 10220, Training-Loss 2.4115e+03, Data-loss 1.7992e+03                  , pde-loss 1.1744e+03, initc-loss 1.3236e+04                    bc_loss 4.6821e+04\n",
      "Epoch 10230, Training-Loss 1.2194e+03, Data-loss 8.3403e+02                  , pde-loss 1.0622e+03, initc-loss 1.4848e+04                    bc_loss 2.2628e+04\n",
      "Epoch 10240, Training-Loss 6.6840e+03, Data-loss 2.0158e+03                  , pde-loss 8.1115e+02, initc-loss 2.0531e+04                    bc_loss 4.4547e+05\n",
      "Epoch 10250, Training-Loss 5.1924e+03, Data-loss 3.9582e+03                  , pde-loss 4.6075e+02, initc-loss 1.6926e+04                    bc_loss 1.0603e+05\n",
      "Epoch 10260, Training-Loss 3.9659e+03, Data-loss 3.3886e+03                  , pde-loss 9.7161e+02, initc-loss 1.0885e+04                    bc_loss 4.5876e+04\n",
      "Epoch 10270, Training-Loss 2.2554e+03, Data-loss 1.7401e+03                  , pde-loss 2.4186e+03, initc-loss 1.7975e+04                    bc_loss 3.1135e+04\n",
      "Epoch 10280, Training-Loss 1.7397e+03, Data-loss 1.2861e+03                  , pde-loss 2.0570e+03, initc-loss 1.6002e+04                    bc_loss 2.7306e+04\n",
      "Epoch 10290, Training-Loss 1.8263e+04, Data-loss 3.5851e+03                  , pde-loss 9.8476e+02, initc-loss 1.9610e+04                    bc_loss 1.4472e+06\n",
      "Epoch 10300, Training-Loss 1.1622e+04, Data-loss 7.7963e+03                  , pde-loss 7.1201e+02, initc-loss 1.0873e+04                    bc_loss 3.7104e+05\n",
      "Epoch 10310, Training-Loss 4.9241e+03, Data-loss 4.0225e+03                  , pde-loss 3.0139e+03, initc-loss 1.9530e+04                    bc_loss 6.7619e+04\n",
      "Epoch 10320, Training-Loss 1.2735e+04, Data-loss 8.3387e+03                  , pde-loss 4.5242e+03, initc-loss 3.6518e+04                    bc_loss 3.9855e+05\n",
      "Epoch 10330, Training-Loss 6.7166e+03, Data-loss 3.2623e+03                  , pde-loss 3.9597e+03, initc-loss 1.7303e+04                    bc_loss 3.2416e+05\n",
      "Epoch 10340, Training-Loss 3.4000e+03, Data-loss 2.6250e+03                  , pde-loss 7.9998e+02, initc-loss 1.7093e+04                    bc_loss 5.9610e+04\n",
      "Epoch 10350, Training-Loss 2.3600e+03, Data-loss 1.9430e+03                  , pde-loss 7.8773e+02, initc-loss 1.1568e+04                    bc_loss 2.9346e+04\n",
      "Epoch 10360, Training-Loss 2.9252e+03, Data-loss 1.0231e+03                  , pde-loss 1.6066e+03, initc-loss 1.3643e+04                    bc_loss 1.7496e+05\n",
      "Epoch 10370, Training-Loss 3.0860e+03, Data-loss 2.6413e+03                  , pde-loss 1.8217e+03, initc-loss 1.0565e+04                    bc_loss 3.2078e+04\n",
      "Epoch 10380, Training-Loss 1.6213e+03, Data-loss 1.1568e+03                  , pde-loss 1.6063e+03, initc-loss 1.5194e+04                    bc_loss 2.9656e+04\n",
      "Epoch 10390, Training-Loss 3.9381e+03, Data-loss 2.0238e+03                  , pde-loss 9.3019e+02, initc-loss 1.9717e+04                    bc_loss 1.7078e+05\n",
      "Epoch 10400, Training-Loss 5.0299e+03, Data-loss 3.4307e+03                  , pde-loss 5.6616e+02, initc-loss 1.8809e+04                    bc_loss 1.4054e+05\n",
      "Epoch 10410, Training-Loss 3.3269e+03, Data-loss 2.0646e+03                  , pde-loss 1.0375e+03, initc-loss 1.0291e+04                    bc_loss 1.1490e+05\n",
      "Epoch 10420, Training-Loss 3.0488e+03, Data-loss 2.5323e+03                  , pde-loss 2.3098e+03, initc-loss 1.5748e+04                    bc_loss 3.3592e+04\n",
      "Epoch 10430, Training-Loss 1.9436e+03, Data-loss 1.1366e+03                  , pde-loss 2.0427e+03, initc-loss 1.6237e+04                    bc_loss 6.2418e+04\n",
      "Epoch 10440, Training-Loss 1.5869e+04, Data-loss 4.6580e+03                  , pde-loss 1.1546e+03, initc-loss 2.0691e+04                    bc_loss 1.0992e+06\n",
      "Epoch 10450, Training-Loss 8.6723e+03, Data-loss 7.6588e+03                  , pde-loss 3.9922e+02, initc-loss 8.1265e+03                    bc_loss 9.2826e+04\n",
      "Epoch 10460, Training-Loss 3.9553e+03, Data-loss 2.0365e+03                  , pde-loss 8.6677e+02, initc-loss 1.8466e+04                    bc_loss 1.7255e+05\n",
      "Epoch 10470, Training-Loss 3.8647e+03, Data-loss 2.6455e+03                  , pde-loss 3.1511e+03, initc-loss 2.0788e+04                    bc_loss 9.7978e+04\n",
      "Epoch 10480, Training-Loss 3.5906e+03, Data-loss 2.7854e+03                  , pde-loss 2.3101e+03, initc-loss 1.7670e+04                    bc_loss 6.0536e+04\n",
      "Epoch 10490, Training-Loss 6.5307e+03, Data-loss 2.9141e+03                  , pde-loss 9.5004e+02, initc-loss 1.6011e+04                    bc_loss 3.4470e+05\n",
      "Epoch 10500, Training-Loss 1.5109e+04, Data-loss 1.1750e+04                  , pde-loss 6.1632e+02, initc-loss 3.0687e+04                    bc_loss 3.0460e+05\n",
      "Epoch 10510, Training-Loss 7.2395e+03, Data-loss 3.9970e+03                  , pde-loss 1.8711e+03, initc-loss 1.8163e+04                    bc_loss 3.0421e+05\n",
      "Epoch 10520, Training-Loss 7.9449e+03, Data-loss 3.5998e+03                  , pde-loss 5.2547e+03, initc-loss 1.7148e+04                    bc_loss 4.1211e+05\n",
      "Epoch 10530, Training-Loss 3.3990e+03, Data-loss 2.3730e+03                  , pde-loss 1.8022e+03, initc-loss 1.2923e+04                    bc_loss 8.7873e+04\n",
      "Epoch 10540, Training-Loss 2.4313e+03, Data-loss 2.0750e+03                  , pde-loss 4.9227e+02, initc-loss 1.6971e+04                    bc_loss 1.8162e+04\n",
      "Epoch 10550, Training-Loss 3.4717e+03, Data-loss 2.3126e+03                  , pde-loss 7.2692e+02, initc-loss 1.9237e+04                    bc_loss 9.5947e+04\n",
      "Epoch 10560, Training-Loss 1.8839e+03, Data-loss 1.1444e+03                  , pde-loss 1.2920e+03, initc-loss 1.3139e+04                    bc_loss 5.9521e+04\n",
      "Epoch 10570, Training-Loss 1.7046e+03, Data-loss 1.2235e+03                  , pde-loss 1.9607e+03, initc-loss 1.3239e+04                    bc_loss 3.2911e+04\n",
      "Epoch 10580, Training-Loss 2.7838e+03, Data-loss 1.8453e+03                  , pde-loss 1.4639e+03, initc-loss 1.7418e+04                    bc_loss 7.4969e+04\n",
      "Epoch 10590, Training-Loss 5.7772e+03, Data-loss 1.9333e+03                  , pde-loss 1.0459e+03, initc-loss 1.3023e+04                    bc_loss 3.7032e+05\n",
      "Epoch 10600, Training-Loss 1.8828e+03, Data-loss 8.7328e+02                  , pde-loss 1.1958e+03, initc-loss 1.2721e+04                    bc_loss 8.7030e+04\n",
      "Epoch 10610, Training-Loss 2.0673e+03, Data-loss 1.2545e+03                  , pde-loss 1.9072e+03, initc-loss 1.6695e+04                    bc_loss 6.2677e+04\n",
      "Epoch 10620, Training-Loss 5.8438e+03, Data-loss 2.1427e+03                  , pde-loss 2.2724e+03, initc-loss 1.6376e+04                    bc_loss 3.5147e+05\n",
      "Epoch 10630, Training-Loss 4.9284e+03, Data-loss 3.3133e+03                  , pde-loss 3.8943e+03, initc-loss 1.7368e+04                    bc_loss 1.4025e+05\n",
      "Epoch 10640, Training-Loss 4.1010e+03, Data-loss 3.0265e+03                  , pde-loss 2.9093e+03, initc-loss 1.8901e+04                    bc_loss 8.5643e+04\n",
      "Epoch 10650, Training-Loss 4.9105e+03, Data-loss 2.5548e+03                  , pde-loss 8.4459e+02, initc-loss 1.6553e+04                    bc_loss 2.1817e+05\n",
      "Epoch 10660, Training-Loss 9.9107e+03, Data-loss 5.0989e+03                  , pde-loss 2.3883e+02, initc-loss 2.9244e+04                    bc_loss 4.5170e+05\n",
      "Epoch 10670, Training-Loss 7.6428e+03, Data-loss 4.8339e+03                  , pde-loss 2.2327e+02, initc-loss 1.7191e+04                    bc_loss 2.6348e+05\n",
      "Epoch 10680, Training-Loss 4.5355e+03, Data-loss 2.9005e+03                  , pde-loss 2.3937e+03, initc-loss 1.1184e+04                    bc_loss 1.4992e+05\n",
      "Epoch 10690, Training-Loss 7.3441e+03, Data-loss 4.1263e+03                  , pde-loss 4.9665e+03, initc-loss 2.4001e+04                    bc_loss 2.9281e+05\n",
      "Epoch 10700, Training-Loss 4.9695e+03, Data-loss 2.5283e+03                  , pde-loss 2.9947e+03, initc-loss 1.7293e+04                    bc_loss 2.2383e+05\n",
      "Epoch 10710, Training-Loss 4.3661e+03, Data-loss 3.9400e+03                  , pde-loss 5.6033e+02, initc-loss 1.6720e+04                    bc_loss 2.5334e+04\n",
      "Epoch 10720, Training-Loss 3.7090e+03, Data-loss 3.0246e+03                  , pde-loss 5.1187e+02, initc-loss 1.5058e+04                    bc_loss 5.2872e+04\n",
      "Epoch 10730, Training-Loss 3.0537e+03, Data-loss 2.1058e+03                  , pde-loss 1.5926e+03, initc-loss 1.3611e+04                    bc_loss 7.9583e+04\n",
      "Epoch 10740, Training-Loss 2.5057e+03, Data-loss 1.6713e+03                  , pde-loss 1.7369e+03, initc-loss 1.5962e+04                    bc_loss 6.5734e+04\n",
      "Epoch 10750, Training-Loss 3.5796e+03, Data-loss 1.4508e+03                  , pde-loss 1.4759e+03, initc-loss 1.7171e+04                    bc_loss 1.9423e+05\n",
      "Epoch 10760, Training-Loss 7.1004e+03, Data-loss 3.0493e+03                  , pde-loss 4.2757e+02, initc-loss 2.2741e+04                    bc_loss 3.8194e+05\n",
      "Epoch 10770, Training-Loss 4.3107e+03, Data-loss 2.7631e+03                  , pde-loss 3.7695e+02, initc-loss 1.5104e+04                    bc_loss 1.3928e+05\n",
      "Epoch 10780, Training-Loss 4.8897e+03, Data-loss 4.4865e+03                  , pde-loss 2.5907e+03, initc-loss 1.6679e+04                    bc_loss 2.1051e+04\n",
      "Epoch 10790, Training-Loss 3.2290e+03, Data-loss 2.6930e+03                  , pde-loss 2.3501e+03, initc-loss 1.5838e+04                    bc_loss 3.5410e+04\n",
      "Epoch 10800, Training-Loss 3.7439e+03, Data-loss 3.3541e+03                  , pde-loss 1.3505e+03, initc-loss 1.6062e+04                    bc_loss 2.1560e+04\n",
      "Epoch 10810, Training-Loss 1.5489e+04, Data-loss 5.7098e+03                  , pde-loss 1.6260e+03, initc-loss 2.7158e+04                    bc_loss 9.4913e+05\n",
      "Epoch 10820, Training-Loss 9.3122e+03, Data-loss 7.6643e+03                  , pde-loss 4.3842e+03, initc-loss 9.5238e+03                    bc_loss 1.5088e+05\n",
      "Epoch 10830, Training-Loss 6.2522e+03, Data-loss 3.5550e+03                  , pde-loss 1.9547e+03, initc-loss 1.3487e+04                    bc_loss 2.5428e+05\n",
      "Epoch 10840, Training-Loss 6.4186e+03, Data-loss 3.3118e+03                  , pde-loss 3.3798e+02, initc-loss 1.8155e+04                    bc_loss 2.9219e+05\n",
      "Epoch 10850, Training-Loss 8.2258e+03, Data-loss 4.3628e+03                  , pde-loss 2.1457e+02, initc-loss 2.5103e+04                    bc_loss 3.6098e+05\n",
      "Epoch 10860, Training-Loss 5.5826e+03, Data-loss 2.8745e+03                  , pde-loss 1.9266e+03, initc-loss 1.6282e+04                    bc_loss 2.5260e+05\n",
      "Epoch 10870, Training-Loss 4.7193e+03, Data-loss 3.5377e+03                  , pde-loss 4.5350e+03, initc-loss 1.3691e+04                    bc_loss 9.9931e+04\n",
      "Epoch 10880, Training-Loss 4.0118e+03, Data-loss 2.5108e+03                  , pde-loss 2.5843e+03, initc-loss 1.6741e+04                    bc_loss 1.3077e+05\n",
      "Epoch 10890, Training-Loss 2.7103e+03, Data-loss 2.1063e+03                  , pde-loss 4.3299e+02, initc-loss 1.9878e+04                    bc_loss 4.0090e+04\n",
      "Epoch 10900, Training-Loss 1.8741e+03, Data-loss 1.4061e+03                  , pde-loss 8.8396e+02, initc-loss 1.5091e+04                    bc_loss 3.0819e+04\n",
      "Epoch 10910, Training-Loss 3.5251e+03, Data-loss 1.8374e+03                  , pde-loss 1.4462e+03, initc-loss 1.4556e+04                    bc_loss 1.5276e+05\n",
      "Epoch 10920, Training-Loss 1.3526e+04, Data-loss 5.9451e+03                  , pde-loss 3.6744e+02, initc-loss 2.7843e+04                    bc_loss 7.2985e+05\n",
      "Epoch 10930, Training-Loss 1.0292e+04, Data-loss 2.7621e+03                  , pde-loss 8.0462e+02, initc-loss 1.4180e+04                    bc_loss 7.3798e+05\n",
      "Epoch 10940, Training-Loss 5.0962e+03, Data-loss 3.2039e+03                  , pde-loss 5.6546e+03, initc-loss 1.6822e+04                    bc_loss 1.6675e+05\n",
      "Epoch 10950, Training-Loss 5.2200e+03, Data-loss 3.7691e+03                  , pde-loss 3.7812e+03, initc-loss 1.3198e+04                    bc_loss 1.2811e+05\n",
      "Epoch 10960, Training-Loss 2.7779e+03, Data-loss 2.5123e+03                  , pde-loss 1.0606e+03, initc-loss 1.6840e+04                    bc_loss 8.6587e+03\n",
      "Epoch 10970, Training-Loss 2.1588e+03, Data-loss 1.9370e+03                  , pde-loss 1.1360e+03, initc-loss 1.4524e+04                    bc_loss 6.5209e+03\n",
      "Epoch 10980, Training-Loss 1.9289e+03, Data-loss 1.2725e+03                  , pde-loss 1.3091e+03, initc-loss 1.7353e+04                    bc_loss 4.6975e+04\n",
      "Epoch 10990, Training-Loss 2.9223e+03, Data-loss 1.3008e+03                  , pde-loss 1.7338e+03, initc-loss 1.8382e+04                    bc_loss 1.4204e+05\n",
      "Epoch 11000, Training-Loss 6.6726e+03, Data-loss 3.8355e+03                  , pde-loss 2.5798e+03, initc-loss 2.4667e+04                    bc_loss 2.5646e+05\n",
      "Epoch 11010, Training-Loss 5.5108e+03, Data-loss 3.3365e+03                  , pde-loss 2.4468e+03, initc-loss 1.9505e+04                    bc_loss 1.9548e+05\n",
      "Epoch 11020, Training-Loss 4.3808e+03, Data-loss 3.4902e+03                  , pde-loss 5.5000e+02, initc-loss 1.1234e+04                    bc_loss 7.7280e+04\n",
      "Epoch 11030, Training-Loss 5.2599e+03, Data-loss 1.9393e+03                  , pde-loss 3.5900e+02, initc-loss 1.5914e+04                    bc_loss 3.1578e+05\n",
      "Epoch 11040, Training-Loss 2.5909e+03, Data-loss 1.2340e+03                  , pde-loss 2.0083e+03, initc-loss 1.6560e+04                    bc_loss 1.1713e+05\n",
      "Epoch 11050, Training-Loss 2.5042e+03, Data-loss 1.8076e+03                  , pde-loss 3.4831e+03, initc-loss 1.3655e+04                    bc_loss 5.2516e+04\n",
      "Epoch 11060, Training-Loss 2.5870e+03, Data-loss 1.6171e+03                  , pde-loss 2.3986e+03, initc-loss 1.3055e+04                    bc_loss 8.1530e+04\n",
      "Epoch 11070, Training-Loss 3.5688e+03, Data-loss 7.9224e+02                  , pde-loss 2.1465e+03, initc-loss 1.5979e+04                    bc_loss 2.5953e+05\n",
      "Epoch 11080, Training-Loss 2.1690e+03, Data-loss 1.2912e+03                  , pde-loss 7.8341e+02, initc-loss 1.8239e+04                    bc_loss 6.8754e+04\n",
      "Epoch 11090, Training-Loss 1.0923e+04, Data-loss 2.7792e+03                  , pde-loss 1.8905e+02, initc-loss 2.3761e+04                    bc_loss 7.9048e+05\n",
      "Epoch 11100, Training-Loss 3.9647e+03, Data-loss 2.5499e+03                  , pde-loss 3.5199e+02, initc-loss 1.0549e+04                    bc_loss 1.3058e+05\n",
      "Epoch 11110, Training-Loss 3.1495e+03, Data-loss 2.1328e+03                  , pde-loss 2.1785e+03, initc-loss 1.3565e+04                    bc_loss 8.5924e+04\n",
      "Epoch 11120, Training-Loss 1.8928e+03, Data-loss 1.5597e+03                  , pde-loss 1.7504e+03, initc-loss 1.4197e+04                    bc_loss 1.7363e+04\n",
      "Epoch 11130, Training-Loss 1.8941e+03, Data-loss 1.3809e+03                  , pde-loss 1.0447e+03, initc-loss 1.4981e+04                    bc_loss 3.5296e+04\n",
      "Epoch 11140, Training-Loss 1.9531e+03, Data-loss 1.3946e+03                  , pde-loss 1.2946e+03, initc-loss 1.3942e+04                    bc_loss 4.0613e+04\n",
      "Epoch 11150, Training-Loss 1.2517e+03, Data-loss 8.9638e+02                  , pde-loss 1.8135e+03, initc-loss 1.4374e+04                    bc_loss 1.9341e+04\n",
      "Epoch 11160, Training-Loss 2.0548e+03, Data-loss 1.2577e+03                  , pde-loss 2.0532e+03, initc-loss 1.5036e+04                    bc_loss 6.2617e+04\n",
      "Epoch 11170, Training-Loss 3.7258e+03, Data-loss 1.3953e+03                  , pde-loss 1.4879e+03, initc-loss 1.3337e+04                    bc_loss 2.1822e+05\n",
      "Epoch 11180, Training-Loss 8.1060e+03, Data-loss 4.5892e+03                  , pde-loss 4.1126e+02, initc-loss 2.8326e+04                    bc_loss 3.2294e+05\n",
      "Epoch 11190, Training-Loss 2.1011e+03, Data-loss 1.5616e+03                  , pde-loss 7.4181e+02, initc-loss 1.7266e+04                    bc_loss 3.5936e+04\n",
      "Epoch 11200, Training-Loss 1.5165e+03, Data-loss 9.5357e+02                  , pde-loss 1.9022e+03, initc-loss 1.3477e+04                    bc_loss 4.0917e+04\n",
      "Epoch 11210, Training-Loss 3.2527e+03, Data-loss 2.6628e+03                  , pde-loss 1.8389e+03, initc-loss 1.5696e+04                    bc_loss 4.1461e+04\n",
      "Epoch 11220, Training-Loss 3.3097e+03, Data-loss 2.2826e+03                  , pde-loss 8.8135e+02, initc-loss 2.0752e+04                    bc_loss 8.1075e+04\n",
      "Epoch 11230, Training-Loss 5.6431e+03, Data-loss 2.9311e+03                  , pde-loss 4.1836e+02, initc-loss 2.1097e+04                    bc_loss 2.4968e+05\n",
      "Epoch 11240, Training-Loss 4.9254e+03, Data-loss 2.8261e+03                  , pde-loss 1.3446e+03, initc-loss 1.4668e+04                    bc_loss 1.9391e+05\n",
      "Epoch 11250, Training-Loss 1.0464e+04, Data-loss 3.9904e+03                  , pde-loss 5.3435e+03, initc-loss 2.5997e+04                    bc_loss 6.1606e+05\n",
      "Epoch 11260, Training-Loss 4.1071e+03, Data-loss 2.1144e+03                  , pde-loss 3.8666e+03, initc-loss 1.1361e+04                    bc_loss 1.8405e+05\n",
      "Epoch 11270, Training-Loss 3.1482e+03, Data-loss 1.0016e+03                  , pde-loss 9.7416e+02, initc-loss 1.4834e+04                    bc_loss 1.9885e+05\n",
      "Epoch 11280, Training-Loss 2.7408e+03, Data-loss 2.2177e+03                  , pde-loss 9.6213e+02, initc-loss 1.5467e+04                    bc_loss 3.5880e+04\n",
      "Epoch 11290, Training-Loss 1.4322e+03, Data-loss 1.0003e+03                  , pde-loss 1.4970e+03, initc-loss 1.3956e+04                    bc_loss 2.7741e+04\n",
      "Epoch 11300, Training-Loss 2.1123e+03, Data-loss 1.3898e+03                  , pde-loss 1.3758e+03, initc-loss 1.5697e+04                    bc_loss 5.5175e+04\n",
      "Epoch 11310, Training-Loss 5.5048e+03, Data-loss 2.5153e+03                  , pde-loss 8.7253e+02, initc-loss 1.6620e+04                    bc_loss 2.8146e+05\n",
      "Epoch 11320, Training-Loss 4.5674e+03, Data-loss 2.9019e+03                  , pde-loss 5.2004e+02, initc-loss 1.6845e+04                    bc_loss 1.4919e+05\n",
      "Epoch 11330, Training-Loss 2.6571e+03, Data-loss 1.4428e+03                  , pde-loss 1.0483e+03, initc-loss 1.5926e+04                    bc_loss 1.0446e+05\n",
      "Epoch 11340, Training-Loss 1.9015e+03, Data-loss 1.5402e+03                  , pde-loss 2.1479e+03, initc-loss 1.4789e+04                    bc_loss 1.9184e+04\n",
      "Epoch 11350, Training-Loss 2.7508e+03, Data-loss 2.3749e+03                  , pde-loss 2.1350e+03, initc-loss 1.5914e+04                    bc_loss 1.9541e+04\n",
      "Epoch 11360, Training-Loss 1.5332e+03, Data-loss 7.8947e+02                  , pde-loss 1.3620e+03, initc-loss 1.4919e+04                    bc_loss 5.8096e+04\n",
      "Epoch 11370, Training-Loss 1.5931e+04, Data-loss 2.5706e+03                  , pde-loss 7.1749e+02, initc-loss 1.9173e+04                    bc_loss 1.3161e+06\n",
      "Epoch 11380, Training-Loss 8.0605e+03, Data-loss 6.7977e+03                  , pde-loss 3.7169e+02, initc-loss 1.8898e+04                    bc_loss 1.0702e+05\n",
      "Epoch 11390, Training-Loss 6.4454e+03, Data-loss 3.9613e+03                  , pde-loss 1.6040e+03, initc-loss 1.3622e+04                    bc_loss 2.3318e+05\n",
      "Epoch 11400, Training-Loss 1.1581e+04, Data-loss 6.3728e+03                  , pde-loss 5.3803e+03, initc-loss 2.9608e+04                    bc_loss 4.8580e+05\n",
      "Epoch 11410, Training-Loss 6.5426e+03, Data-loss 1.8763e+03                  , pde-loss 3.5918e+03, initc-loss 1.4677e+04                    bc_loss 4.4836e+05\n",
      "Epoch 11420, Training-Loss 4.3188e+03, Data-loss 2.3935e+03                  , pde-loss 6.1124e+02, initc-loss 1.7410e+04                    bc_loss 1.7451e+05\n",
      "Epoch 11430, Training-Loss 8.3407e+03, Data-loss 4.4591e+03                  , pde-loss 4.7714e+02, initc-loss 1.7897e+04                    bc_loss 3.6979e+05\n",
      "Epoch 11440, Training-Loss 3.3724e+03, Data-loss 2.5109e+03                  , pde-loss 1.8355e+03, initc-loss 1.5402e+04                    bc_loss 6.8913e+04\n",
      "Epoch 11450, Training-Loss 2.5528e+03, Data-loss 1.8109e+03                  , pde-loss 2.0494e+03, initc-loss 1.4617e+04                    bc_loss 5.7524e+04\n",
      "Epoch 11460, Training-Loss 1.2423e+03, Data-loss 7.8131e+02                  , pde-loss 1.1205e+03, initc-loss 1.4958e+04                    bc_loss 3.0025e+04\n",
      "Epoch 11470, Training-Loss 8.3435e+03, Data-loss 2.8377e+03                  , pde-loss 1.1960e+03, initc-loss 1.4610e+04                    bc_loss 5.3477e+05\n",
      "Epoch 11480, Training-Loss 2.3490e+03, Data-loss 1.1421e+03                  , pde-loss 6.4292e+02, initc-loss 1.2253e+04                    bc_loss 1.0780e+05\n",
      "Epoch 11490, Training-Loss 2.1949e+03, Data-loss 1.7887e+03                  , pde-loss 1.7524e+03, initc-loss 1.4979e+04                    bc_loss 2.3887e+04\n",
      "Epoch 11500, Training-Loss 1.8509e+03, Data-loss 1.3801e+03                  , pde-loss 2.1986e+03, initc-loss 1.3876e+04                    bc_loss 3.1006e+04\n",
      "Epoch 11510, Training-Loss 1.7877e+03, Data-loss 1.3730e+03                  , pde-loss 1.3452e+03, initc-loss 1.6533e+04                    bc_loss 2.3592e+04\n",
      "Epoch 11520, Training-Loss 3.7422e+03, Data-loss 2.6968e+03                  , pde-loss 1.2720e+03, initc-loss 1.4195e+04                    bc_loss 8.9068e+04\n",
      "Epoch 11530, Training-Loss 9.0338e+03, Data-loss 4.5891e+03                  , pde-loss 4.9786e+02, initc-loss 2.0257e+04                    bc_loss 4.2372e+05\n",
      "Epoch 11540, Training-Loss 7.0540e+03, Data-loss 5.6390e+03                  , pde-loss 2.3428e+03, initc-loss 2.2512e+04                    bc_loss 1.1664e+05\n",
      "Epoch 11550, Training-Loss 1.1409e+04, Data-loss 6.4364e+03                  , pde-loss 5.2164e+03, initc-loss 2.6193e+04                    bc_loss 4.6586e+05\n",
      "Epoch 11560, Training-Loss 4.7550e+03, Data-loss 2.5073e+03                  , pde-loss 2.9954e+03, initc-loss 1.2419e+04                    bc_loss 2.0936e+05\n",
      "Epoch 11570, Training-Loss 3.0671e+03, Data-loss 1.8942e+03                  , pde-loss 6.0170e+02, initc-loss 1.4605e+04                    bc_loss 1.0208e+05\n",
      "Epoch 11580, Training-Loss 2.4356e+03, Data-loss 1.9854e+03                  , pde-loss 1.1754e+03, initc-loss 1.3677e+04                    bc_loss 3.0176e+04\n",
      "Epoch 11590, Training-Loss 3.2749e+03, Data-loss 2.0277e+03                  , pde-loss 1.6741e+03, initc-loss 2.2568e+04                    bc_loss 1.0047e+05\n",
      "Epoch 11600, Training-Loss 1.9980e+03, Data-loss 1.4466e+03                  , pde-loss 1.2714e+03, initc-loss 1.6491e+04                    bc_loss 3.7376e+04\n",
      "Epoch 11610, Training-Loss 2.1939e+03, Data-loss 1.7668e+03                  , pde-loss 1.1908e+03, initc-loss 1.4238e+04                    bc_loss 2.7285e+04\n",
      "Epoch 11620, Training-Loss 1.6861e+03, Data-loss 1.2253e+03                  , pde-loss 1.4687e+03, initc-loss 1.5674e+04                    bc_loss 2.8943e+04\n",
      "Epoch 11630, Training-Loss 2.6495e+03, Data-loss 2.0021e+03                  , pde-loss 1.0359e+03, initc-loss 1.7902e+04                    bc_loss 4.5800e+04\n",
      "Epoch 11640, Training-Loss 3.3562e+03, Data-loss 1.0681e+03                  , pde-loss 6.0485e+02, initc-loss 1.3483e+04                    bc_loss 2.1472e+05\n",
      "Epoch 11650, Training-Loss 2.4446e+03, Data-loss 1.2393e+03                  , pde-loss 7.5081e+02, initc-loss 1.8449e+04                    bc_loss 1.0133e+05\n",
      "Epoch 11660, Training-Loss 1.3716e+03, Data-loss 1.0500e+03                  , pde-loss 2.0109e+03, initc-loss 1.7416e+04                    bc_loss 1.2730e+04\n",
      "Epoch 11670, Training-Loss 1.1656e+03, Data-loss 7.5207e+02                  , pde-loss 2.3083e+03, initc-loss 1.6349e+04                    bc_loss 2.2697e+04\n",
      "Epoch 11680, Training-Loss 1.4514e+03, Data-loss 1.1740e+03                  , pde-loss 1.3785e+03, initc-loss 1.3762e+04                    bc_loss 1.2593e+04\n",
      "Epoch 11690, Training-Loss 2.6706e+03, Data-loss 3.5348e+02                  , pde-loss 1.3960e+03, initc-loss 1.4562e+04                    bc_loss 2.1575e+05\n",
      "Epoch 11700, Training-Loss 6.9736e+03, Data-loss 4.5590e+03                  , pde-loss 3.0342e+03, initc-loss 2.7089e+04                    bc_loss 2.1133e+05\n",
      "Epoch 11710, Training-Loss 7.0809e+03, Data-loss 2.9131e+03                  , pde-loss 3.0968e+03, initc-loss 1.7964e+04                    bc_loss 3.9571e+05\n",
      "Epoch 11720, Training-Loss 9.8003e+03, Data-loss 3.7872e+03                  , pde-loss 3.5691e+02, initc-loss 2.3020e+04                    bc_loss 5.7793e+05\n",
      "Epoch 11730, Training-Loss 9.4493e+03, Data-loss 5.1015e+03                  , pde-loss 1.8677e+02, initc-loss 1.8341e+04                    bc_loss 4.1626e+05\n",
      "Epoch 11740, Training-Loss 2.6824e+03, Data-loss 1.7369e+03                  , pde-loss 2.3017e+03, initc-loss 1.4728e+04                    bc_loss 7.7522e+04\n",
      "Epoch 11750, Training-Loss 3.7843e+03, Data-loss 3.0475e+03                  , pde-loss 2.4513e+03, initc-loss 1.4563e+04                    bc_loss 5.6666e+04\n",
      "Epoch 11760, Training-Loss 2.1142e+03, Data-loss 1.1148e+03                  , pde-loss 1.1431e+03, initc-loss 1.4650e+04                    bc_loss 8.4144e+04\n",
      "Epoch 11770, Training-Loss 1.7380e+03, Data-loss 1.0853e+03                  , pde-loss 1.5611e+03, initc-loss 1.3173e+04                    bc_loss 5.0538e+04\n",
      "Epoch 11780, Training-Loss 2.2551e+03, Data-loss 1.1177e+03                  , pde-loss 1.7031e+03, initc-loss 1.4840e+04                    bc_loss 9.7201e+04\n",
      "Epoch 11790, Training-Loss 9.2570e+03, Data-loss 2.6638e+03                  , pde-loss 2.7702e+03, initc-loss 2.5718e+04                    bc_loss 6.3084e+05\n",
      "Epoch 11800, Training-Loss 1.1950e+04, Data-loss 4.1014e+03                  , pde-loss 3.7796e+03, initc-loss 7.1349e+03                    bc_loss 7.7395e+05\n",
      "Epoch 11810, Training-Loss 6.9544e+03, Data-loss 3.1172e+03                  , pde-loss 6.1834e+02, initc-loss 1.6300e+04                    bc_loss 3.6681e+05\n",
      "Epoch 11820, Training-Loss 7.4574e+03, Data-loss 3.4574e+03                  , pde-loss 2.0205e+02, initc-loss 1.8851e+04                    bc_loss 3.8095e+05\n",
      "Epoch 11830, Training-Loss 3.0674e+03, Data-loss 2.4463e+03                  , pde-loss 1.1450e+03, initc-loss 2.0204e+04                    bc_loss 4.0758e+04\n",
      "Epoch 11840, Training-Loss 3.6071e+03, Data-loss 3.1879e+03                  , pde-loss 2.2091e+03, initc-loss 1.4462e+04                    bc_loss 2.5242e+04\n",
      "Epoch 11850, Training-Loss 2.8181e+03, Data-loss 1.6058e+03                  , pde-loss 1.5595e+03, initc-loss 1.5312e+04                    bc_loss 1.0436e+05\n",
      "Epoch 11860, Training-Loss 7.2663e+03, Data-loss 3.0059e+03                  , pde-loss 6.0743e+02, initc-loss 1.5977e+04                    bc_loss 4.0945e+05\n",
      "Epoch 11870, Training-Loss 2.6856e+03, Data-loss 1.4780e+03                  , pde-loss 1.2191e+03, initc-loss 1.1394e+04                    bc_loss 1.0815e+05\n",
      "Epoch 11880, Training-Loss 3.1745e+03, Data-loss 2.4413e+03                  , pde-loss 3.0263e+03, initc-loss 1.8746e+04                    bc_loss 5.1550e+04\n",
      "Epoch 11890, Training-Loss 2.0657e+03, Data-loss 1.2165e+03                  , pde-loss 2.0877e+03, initc-loss 1.6486e+04                    bc_loss 6.6348e+04\n",
      "Epoch 11900, Training-Loss 1.4884e+03, Data-loss 1.2236e+03                  , pde-loss 1.3550e+03, initc-loss 1.4170e+04                    bc_loss 1.0955e+04\n",
      "Epoch 11910, Training-Loss 2.2659e+03, Data-loss 1.5879e+03                  , pde-loss 1.2283e+03, initc-loss 1.6497e+04                    bc_loss 5.0075e+04\n",
      "Epoch 11920, Training-Loss 2.2136e+03, Data-loss 8.6905e+02                  , pde-loss 1.3883e+03, initc-loss 1.1530e+04                    bc_loss 1.2154e+05\n",
      "Epoch 11930, Training-Loss 1.1480e+03, Data-loss 6.6401e+02                  , pde-loss 1.1475e+03, initc-loss 1.3125e+04                    bc_loss 3.4128e+04\n",
      "Epoch 11940, Training-Loss 3.3713e+03, Data-loss 1.2436e+03                  , pde-loss 1.3825e+03, initc-loss 1.4791e+04                    bc_loss 1.9661e+05\n",
      "Epoch 11950, Training-Loss 3.7612e+03, Data-loss 2.7219e+03                  , pde-loss 3.0216e+03, initc-loss 2.1662e+04                    bc_loss 7.9241e+04\n",
      "Epoch 11960, Training-Loss 4.1332e+03, Data-loss 2.9927e+03                  , pde-loss 1.9924e+03, initc-loss 1.8443e+04                    bc_loss 9.3615e+04\n",
      "Epoch 11970, Training-Loss 4.3405e+03, Data-loss 2.5955e+03                  , pde-loss 1.0170e+03, initc-loss 1.9351e+04                    bc_loss 1.5413e+05\n",
      "Epoch 11980, Training-Loss 8.3660e+03, Data-loss 3.2309e+03                  , pde-loss 4.5249e+02, initc-loss 2.3631e+04                    bc_loss 4.8943e+05\n",
      "Epoch 11990, Training-Loss 4.6589e+03, Data-loss 3.6874e+03                  , pde-loss 8.1221e+02, initc-loss 1.5460e+04                    bc_loss 8.0870e+04\n",
      "Epoch 12000, Training-Loss 3.5523e+03, Data-loss 2.0000e+03                  , pde-loss 2.3417e+03, initc-loss 1.3066e+04                    bc_loss 1.3983e+05\n",
      "Epoch 12010, Training-Loss 2.3831e+03, Data-loss 1.6572e+03                  , pde-loss 1.4146e+03, initc-loss 1.6979e+04                    bc_loss 5.4199e+04\n",
      "Epoch 12020, Training-Loss 1.8855e+03, Data-loss 1.5838e+03                  , pde-loss 8.5196e+02, initc-loss 1.3188e+04                    bc_loss 1.6134e+04\n",
      "Epoch 12030, Training-Loss 7.3053e+03, Data-loss 2.4184e+03                  , pde-loss 4.7463e+02, initc-loss 2.0080e+04                    bc_loss 4.6813e+05\n",
      "Epoch 12040, Training-Loss 3.4194e+03, Data-loss 2.4319e+03                  , pde-loss 7.6981e+02, initc-loss 1.4610e+04                    bc_loss 8.3372e+04\n",
      "Epoch 12050, Training-Loss 3.1222e+03, Data-loss 2.4830e+03                  , pde-loss 1.9260e+03, initc-loss 1.2371e+04                    bc_loss 4.9623e+04\n",
      "Epoch 12060, Training-Loss 1.4926e+03, Data-loss 7.0800e+02                  , pde-loss 1.5988e+03, initc-loss 1.2811e+04                    bc_loss 6.4051e+04\n",
      "Epoch 12070, Training-Loss 8.7106e+03, Data-loss 2.3176e+03                  , pde-loss 9.2127e+02, initc-loss 1.8099e+04                    bc_loss 6.2028e+05\n",
      "Epoch 12080, Training-Loss 5.0645e+03, Data-loss 2.4454e+03                  , pde-loss 7.2136e+02, initc-loss 1.1196e+04                    bc_loss 2.4999e+05\n",
      "Epoch 12090, Training-Loss 2.1293e+03, Data-loss 1.6131e+03                  , pde-loss 2.9553e+03, initc-loss 1.7598e+04                    bc_loss 3.1071e+04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses, val_losses = training_loop(epochs, model, loss_fn_data, optimizer,train_loader)  # Train the model\n",
    " \n",
    "test_losses = test_loop(epochs, model, loss_fn_data, optimizer, train_loader, test_loader)  # Test the model\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_nn = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).cpu().detach().numpy() # Get the predictions from the model\n",
    "\n",
    "temp_nn = temp_nn.reshape(num_steps+1, num_points) # Reshape the predictions to a 2D array\n",
    "time_ss= np.linspace(0, time_end, num_steps+1)\n",
    "plt.figure\n",
    "plt.plot(time_ss, temp_nn[:,num_points//2], label='Predicted Temperature')\n",
    "plt.plot(time_ss, temperature_history[:,num_points//2], label='Actual Temperature')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.title('Predicted vs Actual Temperature at x = 7.5mm')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
