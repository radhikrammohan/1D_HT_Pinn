{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Three Phase Simulation of Alloys and PINN model development \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the simulation of 1D Phase change of aluminium alloy. There will be three phases (solid,liquid and mushy).   \n",
    "\n",
    "The approach used is finite difference method and the physics involved in heat conduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import csv\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the constants and inital geometric domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Material :- AL 380\n",
    "\n",
    "| Sr.No | Properties  | Symbol | Value  | Unit |Range(source) |\n",
    "|:---:|:---:|:---:|:---:|:---:|:--:|\n",
    "| 1  | Liquidus Density | $\\rho_{l}$  | 2300 | $kg/m^3$  |  2200-2400 (ASM handbook) |\n",
    "|  2 |  Solidus Density  |  $\\rho_{s}$  | 2500  |  $kg/m^3$  | 2400-2600 (ASM handbook) |\n",
    "|  3 |  Mushy Desnity |  $\\rho_{m}$  |  2400 | $kg/m^3$   |Increase linearly from liquid to solid (ASM handbook) |\n",
    "|  4 |  Liquidus Thermal Conductivity| $k_l$  |  70 | $W/m-K$  |60-80 (ASM handbook) |\n",
    "|  5 |  Solidus Thermal Conductivity | $k_s$  | 180  |  $W/m-K$ | 150-210(ASM handbook) |\n",
    "|  6 | Mushy Zone Thermal Conductivity | $k_m$  | 125  |  $W/m-K$ |Decrease linearly from liquid to solid (ASM handbook) |\n",
    "|  7 | Liquidus Specific Heat | $c_{pl}$  | 1190  | $J/kg-K$  | 1100 -1200 (ASM handbook)|\n",
    "|  8 | Solidus Specific Heat | $c_{ps}$  |  1100 |  $J/kg-K$  | 1100-1200 (ASM handbook)|\n",
    "|  9 | Mushy Zone Specific Heat |  $c_{pm}$ | 1175 | $J/kg-K$   |decrease lineraly from liquid to solid (ASM handbook)|\n",
    "|  10 | Latent Heat of Fusion | $L_{fusion}$  | 450e3  | $J/kg$ | (400-500)e3 (ASM handbook) |\n",
    "| 11 | Left Boundary Temperature |$BCT_{l}$|623 |$K$| (623-723) Nissan Data |\n",
    "|12 | Right Boundary Temperature | $BCT_{r}$|623 |$K$| (623-723) Nissan Data |\n",
    "|13| Freezing time | |60 |sec|||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_l = 3.394878564540885e-05, alpha_s = 3.686205086349929e-05, m_eff = 6.296953764744878e-06\n",
      "dx is 0.0003061224489795918\n",
      "dt is  0.0012711033647622566\n",
      "num_steps is 31469\n",
      "cfl is 0.0012711033647622566\n",
      "stability criteria satisfied\n"
     ]
    }
   ],
   "source": [
    "# Geometry\n",
    "length = 15.0e-3             # Length of the rod\n",
    "\n",
    "# Material properties\n",
    "rho = 2300.0                     # Density of AL380 (kg/m^3)\n",
    "rho_l = 2460.0                   # Density of AL380 (kg/m^3)\n",
    "rho_s = 2710.0                    # Density of AL380 (kg/m^3)\n",
    "rho_m = (rho_l + rho_s )/2       # Desnity in mushy zone is taken as average of liquid and solid density\n",
    "\n",
    "k = 104.0                       # W/m-K\n",
    "k_l = k                       # W/m-K\n",
    "k_s = 96.2                    # W/m-K\n",
    "k_m =  (k_l+k_s)/2                     # W/m-K\n",
    "k_mo = 41.5\n",
    "\n",
    "\n",
    "cp = 1245.3                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_l = cp                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_s = 963.0                 # Specific heat of aluminum (J/kg-K)\n",
    "cp_m =  (cp_l+cp_s)/2                 # Specific heat of mushy zone is taken as average of liquid and solid specific heat\n",
    "# cp_m = cp\n",
    "           # Thermal diffusivity\n",
    "alpha_l = k_l / (rho_l * cp_l) \n",
    "alpha_s = k_s / (rho_s*cp_s)\n",
    "alpha_m = k_m / (rho_m * cp_m)          #`Thermal diffusivity in mushy zone is taken as average of liquid and solid thermal diffusivity`\n",
    "\n",
    "\n",
    "#L_fusion = 3.9e3                 # J/kg\n",
    "L_fusion = 389.0e3               # J/kg  # Latent heat of fusion of aluminum\n",
    "         # Thermal diffusivity\n",
    "\n",
    "\n",
    "T_L = 574.4 +273.0                       #  K -Liquidus Temperature (615 c) AL 380\n",
    "T_S = 497.3 +273.0                     # K- Solidus Temperature (550 C)\n",
    "m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "print (f\"alpha_l = {alpha_l}, alpha_s = {alpha_s}, m_eff = {m_eff}\")\n",
    "\n",
    "# htc = 10.0                   # W/m^2-K\n",
    "# q = htc*(919.0-723.0)\n",
    "# q = 10000.0\n",
    "\n",
    "\n",
    "num_points = 50                        # Number of spatial points\n",
    "dx = length / (num_points - 1)         # Distance between two spatial points\n",
    "print('dx is',dx)\n",
    "\n",
    "                                                              \n",
    "# Time Discretization  \n",
    "# \n",
    "time_end = 40        # seconds                         \n",
    "\n",
    "maxi = max(alpha_s,alpha_l,alpha_m)\n",
    "dt = abs(0.5*((dx**2) /maxi)) \n",
    "\n",
    "print('dt is ',dt)\n",
    "num_steps = round(time_end/dt)\n",
    "print('num_steps is',num_steps)\n",
    "cfl = 0.5 *(dx**2/max(alpha_l,alpha_s,alpha_m))\n",
    "print('cfl is',cfl)\n",
    "\n",
    "time_steps = np.linspace(0, time_end, num_steps + 1)\n",
    "step_coeff = dt / (dx ** 2)\n",
    "\n",
    "if dt <= cfl:\n",
    "    print('stability criteria satisfied')\n",
    "else:\n",
    "    print('stability criteria not satisfied')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial and Boundary Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initial temperature and phase fields\n",
    "temperature = np.full(num_points+2, 919.0)            # Initial temperature of the rod with ghost points at both ends\n",
    "phase = np.zeros(num_points+2)*0.0                    # Initial phase of the rod with ghost points at both ends\n",
    "\n",
    "# Set boundary conditions\n",
    "# temperature[-1] = 919.0 \n",
    "phase[-1] = 1.0\n",
    "\n",
    "# temperature[0] = 919.0 #(40 C)\n",
    "phase[0] = 1.0\n",
    "\n",
    "# Store initial state in history\n",
    "temperature_history = [temperature.copy()]    # List to store temperature at each time step\n",
    "phi_history = [phase.copy()]                    # List to store phase at each time step\n",
    "temp_init = temperature.copy()                 # Initial temperature of the rod\n",
    "# print(temperature_history,phi_history)\n",
    "# Array to store temperature at midpoint over time\n",
    "midpoint_index = num_points // 2                          # Index of the midpoint\n",
    "\n",
    "midpoint_temperature_history = [temperature[midpoint_index]]            # List to store temperature at midpoint over time\n",
    "dm = 60.0e-3                                                            # die thickness in m\n",
    "\n",
    "# r_m =  (k_mo / dm) + (1/htc)\n",
    "\n",
    "t_surr = 500.0                                        # Surrounding temperature in K\n",
    "# t_surr = h()\n",
    "\n",
    "def kramp(temp,v1,v2,T_L,T_s):                                      # Function to calculate thermal conductivity in Mushy Zone\n",
    "        slope = (v1-v2)/(T_L-T_S)\n",
    "        if temp > T_L:\n",
    "            k_m = k_l\n",
    "        elif temp < T_S:\n",
    "            k_m = k_s\n",
    "        else:\n",
    "            k_m = k_s + slope*(temp-T_S)\n",
    "        return k_m\n",
    "\n",
    "def cp_ramp(temp,v1,v2,T_L,T_s):                                    # Function to calculate specific heat capacity in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        cp_m = cp_l\n",
    "    elif temp < T_S:\n",
    "        cp_m = cp_s\n",
    "    else:\n",
    "        cp_m = cp_s + slope*(temp-T_S)\n",
    "    return cp_m\n",
    "\n",
    "def rho_ramp(temp,v1,v2,T_L,T_s):                                       # Function to calculate density in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        rho_m = rho_l\n",
    "    elif temp < T_S:\n",
    "        rho_m = rho_s\n",
    "    else:\n",
    "        rho_m = rho_s + slope*(temp-T_S)\n",
    "    return rho_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving the HT equation and phase change numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for m in range(1, num_steps+1):                                                                            # time loop\n",
    "    htc = 10.0                   # htc of Still air in W/m^2-K\n",
    "    q1 = htc*(temp_init[0]-t_surr)   # Heat flux at the left boundary\n",
    "    \n",
    "    # print(f\"q1 is {q1}\")\n",
    "    temperature[0] = temp_init[0] + alpha_l * step_coeff * ((2.0*temp_init[1]) - (2.0 * temp_init[0])-(2.0*dx*(q1)))  # Update boundary condition temperature\n",
    "    \n",
    "    q2 = htc*(temp_init[-1]-t_surr)                   # Heat flux at the right boundary\n",
    "    temperature[-1] = temp_init[-1] + alpha_l * step_coeff * ((2.0*temp_init[-2]) - (2.0 * temp_init[-1])-(2.0*dx*(q2)))  # Update boundary condition temperature\n",
    "    \n",
    "    for n in range(1,num_points+1):              # space loop, adjusted range\n",
    "       \n",
    "        if temperature[n] >= T_L:\n",
    "            temperature[n] += ((alpha_l * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            phase[n] = 0\n",
    "            \n",
    "            # print(f\" Time-Step{m},Spatial point{n},Temperature{temperature[n]}\")\n",
    "        elif T_S < temperature[n] < T_L:\n",
    "            \n",
    "            k_m = kramp(temperature[n],k_l,k_s,T_L,T_S)\n",
    "            cp_m = cp_ramp(temperature[n],cp_l,cp_s,T_L,T_S)\n",
    "            rho_m = rho_ramp(temperature[n],rho_l,rho_s,T_L,T_S)\n",
    "            m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "            \n",
    "            temperature[n] += ((m_eff * step_coeff)* (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            \n",
    "            phase[n] = (T_L - temperature[n]) / (T_L - T_S)\n",
    "            # print(m,n,temperature[n],phase[n])\n",
    "         \n",
    "        elif temperature[n]<T_S:\n",
    "            temperature[n] += ((alpha_s * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n])+ temp_init[n-1]))\n",
    "            phase[n] = 1\n",
    "                     \n",
    "        else:\n",
    "            print(\"ERROR: should not be here\")\n",
    "\n",
    "     \n",
    "          \n",
    "    temperature = temperature.copy()                                                                # Update temperature\n",
    "    phase = phase.copy()                                                                            # Update phase\n",
    "    temp_init = temperature.copy()                                                                  # Update last time step temperature\n",
    "    temperature_history.append(temperature.copy())                                                  # Append the temperature history to add ghost points\n",
    "    phi_history.append(phase.copy())                                                                # Append the phase history to add ghost points\n",
    "    midpoint_temperature_history.append(temperature[midpoint_index])                                # Store midpoint temperature\n",
    "    \n",
    "    \n",
    "    # print(f\"Step {m}, Temperature: {temperature}\")\n",
    "    \n",
    "\n",
    "\n",
    "# print(midpoint_temperature_history)\n",
    "#print(phi_history)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31470, 52)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot temperature history for debugging\n",
    "temperature_history_1 = np.array(temperature_history)\n",
    "print(temperature_history_1.shape)\n",
    "time_ss= np.linspace(0, time_end, num_steps+1)\n",
    "# print(time_ss.shape)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_ss, midpoint_temperature_history, label='Midpoint Temperature')\n",
    "plt.axhline(y=T_L, color='r', linestyle='--', label='Liquidus Temperature')\n",
    "plt.axhline(y=T_S, color='g', linestyle='--', label='Solidus Temperature')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.title('Temperature Distribution Over Time at x = 7.5mm') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot temperature history for debugging\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for i in range(0, num_steps, num_steps // 50):\n",
    "#     plt.plot(np.linspace(0, length, num_points+2), temperature_history[i], label=f't={i * dt:.2f} s')\n",
    "\n",
    "# plt.xlabel('Position (m)')\n",
    "# plt.ylabel('Temperature (K)')\n",
    "# plt.title('Temperature Distribution Over Time')\n",
    "# # plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data into Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_history = np.array(temperature_history)\n",
    "phi_history = np.array(phi_history)\n",
    "# print(temperature_history.shape)\n",
    "# print(phi_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31470, 52)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have temperature_history and phi_history as lists of arrays\n",
    "temperature_history = np.array(temperature_history)\n",
    "phi_history = np.array(phi_history)\n",
    "\n",
    "print(temperature_history.shape)\n",
    "\n",
    "# # Check the new shape after transposing\n",
    "# print(\"Transposed Temperature History Shape:\", temperature_history.shape)\n",
    "# print(\"Transposed Phi History Shape:\", phi_history.shape)\n",
    "\n",
    "# # Create a meshgrid for space and time coordinates\n",
    "# space_coord, time_coord = np.meshgrid(np.arange(temperature_history.shape[1]), np.arange(temperature_history.shape[0]))\n",
    "\n",
    "# time_coord = time_coord * dt \n",
    "# # Create a figure with two subplots\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# # Plot the temperature history on the left subplot\n",
    "# im1 = ax1.pcolormesh(space_coord, time_coord, temperature_history, cmap='viridis')\n",
    "# ax1.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_title('Temperature Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im1, ax=ax1, label='Temperature')\n",
    "\n",
    "# # Plot the phase history on the right subplot\n",
    "# im2 = ax2.pcolormesh(space_coord, time_coord, phi_history, cmap='viridis')\n",
    "# ax2.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=18)\n",
    "# ax2.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax2.set_title('Phase Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im2, ax=ax2, label='Phase')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #plot the main\n",
    "# fig, ax = plt.subplots(figsize=(14, 6))\n",
    "# im = ax.pcolormesh(space_coord, time_coord, Dim_ny, cmap='viridis')\n",
    "# ax.set_xlabel('Space Coordinate')\n",
    "# ax.set_ylabel('Time')\n",
    "# ax.set_title('Niyama Variation Over Time')\n",
    "# fig.colorbar(im, ax=ax, label='Main')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU/CPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check for gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "space = np.linspace(0, length, num_points+2)\n",
    "# print(space.shape)\n",
    "time = np.linspace(0, time_end, num_steps+1)\n",
    "# print(time.shape)\n",
    "\n",
    "scaler_space = StandardScaler()\n",
    "scaler_time = StandardScaler()\n",
    "\n",
    "space_tr = scaler_space.fit_transform(space.reshape(-1,1))\n",
    "time_tr = scaler_time.fit_transform(time.reshape(-1,1))\n",
    "\n",
    "# print(space_tr.shape)\n",
    "# print(time_tr.shape)\n",
    "\n",
    "\n",
    "# create mesh grid of space and time\n",
    "\n",
    "space_tr, time_tr = np.meshgrid(space_tr, time_tr)\n",
    "space_tr = space_tr.flatten().reshape(-1,1)\n",
    "time_tr = time_tr.flatten().reshape(-1,1)\n",
    "inputs = np.hstack([space_tr,time_tr]) # Concatenate the spatial and temporal inputs\n",
    "inputs = torch.tensor(inputs).float().to(device) # Convert the inputs to a tensor\n",
    "# print(inputs.shape)\n",
    "\n",
    "# label/temp data\n",
    "temp_tr = torch.tensor(temperature_history).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp = temp_tr.reshape(-1,1) # Reshape the temperature tensor to a column vector\n",
    "# print(temp_inp.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Data Splitting\n",
    "\n",
    "train_inputs, val_test_inputs, train_temp_inp, val_test_temp_inp = train_test_split(inputs, temp_inp, \\\n",
    "                                                                                    test_size=0.2, random_state=42)\n",
    "val_inputs, test_inputs, val_temp_inp, test_temp_inp = train_test_split(val_test_inputs, val_test_temp_inp, \\\n",
    "                                                                        test_size=0.8, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_temp_inp) # Create the training dataset\n",
    "val_dataset = TensorDataset(val_inputs, val_temp_inp) # Create the validation dataset\n",
    "test_dataset = TensorDataset(test_inputs, test_temp_inp) # Create the test dataset\n",
    "\n",
    "batch_size = 640\n",
    "random_sampler_train = RandomSampler(train_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "random_sampler_val = RandomSampler(val_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the validation dataset\n",
    "random_sampler_test = RandomSampler(test_dataset, replacement=True, num_samples=batch_size) # Create a random sampler for the test dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=random_sampler_train) # Create the training dataloader\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, sampler=random_sampler_val) # Create the validation dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, sampler=random_sampler_test) # Create the test dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural network architecture\n",
    "class Mushydata(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size): # This is the constructor\n",
    "        super(Mushydata, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):                               # This is the forward pass\n",
    "        input_features = torch.cat([x, t], dim=1)          # Concatenate the input features\n",
    "        m = self.base(input_features)                                 # Pass through the third layer\n",
    "        return m                    # Return the output of the network\n",
    "\n",
    "\n",
    "# features = torch.rand(1, 2)\n",
    "# model = HeatPINN(2, 20, 1)\n",
    "# output = model(features[:, 0:1], features[:, 1:2])\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# Loss function for data \n",
    "\n",
    "# # Apply Xavier initialization\n",
    "# def init_weights(m):\n",
    "#     if isinstance(m, nn.Linear):\n",
    "#         torch.nn.init.xavier_uniform_(m.weight)\n",
    "#         if m.bias is not None:\n",
    "#             m.bias.data.fill_(0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamters Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 40\n",
    "learning_rate = 0.004\n",
    "epochs = 20000\n",
    "# alpha = 0.01  # Adjust this value based on your problem\n",
    "# boundary_value = 313.0\n",
    "# initial_value = init_temp\n",
    "# Initialize the model\n",
    "model = Mushydata(input_size=2, hidden_size=hidden_size,output_size=1).to(device)\n",
    "\n",
    "# model.apply(init_weights)\n",
    "\n",
    "# Verify the initialization\n",
    "\n",
    "lambda_l1 = 0.04\n",
    "weight_decay = 0.09   \n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss List Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of train_loader is <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(f\"Datatype of train_loader is {type(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_data(u_pred, u_true):\n",
    "    return nn.MSELoss()(u_pred, u_true)\n",
    "\n",
    "def l1_regularization(model, lambd):\n",
    "    l1_reg = sum(param.abs().sum() for param in model.parameters())\n",
    "    return l1_reg * lambd\n",
    "def accuracy(y_pred, y_true):\n",
    "    return torch.mean(torch.abs(y_pred - y_true) / y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training, Validation and Testing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, model, loss_fn_data, optimizer, train_dataloader, val_dataloader):\n",
    "    train_losses = []  # Initialize the list to store the training losses\n",
    "    val_losses = []    # Initialize the list to store the validation losses\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                                                           # Set the model to training mode\n",
    "        train_loss = 0    \n",
    "        training_accuracy = 0                                                      # Initialize the training loss\n",
    "        for batch in train_dataloader:\n",
    "            inputs, temp_inp = batch                                                      # Get the inputs and the true values\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device)             # Move the inputs and true values to the GPU\n",
    "            optimizer.zero_grad()                                                       # Zero the gradients\n",
    "            \n",
    "            # Forward pass\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).to(device) # Get the predictions\n",
    "            \n",
    "            # Loss calculation\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp)                                  # Calculate the data loss\n",
    "            # l1_regularization_loss = l1_regularization(model, lambda_l1)                  # Calculate the L1 regularization loss\n",
    "            loss = data_loss                                                                      # Calculate the total loss\n",
    "            training_accuracy += accuracy(u_pred, temp_inp)                                # Calculate the training accuracy\n",
    "            # Backpropagation\n",
    "            loss.backward()                                                        # Backpropagate the gradients\n",
    "            \n",
    "            optimizer.step()                                                                            # Update the weights\n",
    "            \n",
    "            train_loss += loss.item()     \n",
    "                                        # Add the loss to the training set loss\n",
    "                                                         # Calculate the average training accuracy\n",
    "        train_losses.append(train_loss)                                                   # Append the training loss to the list of training losses\n",
    "        \n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0                                                                                      # Initialize the validation loss\n",
    "        val_accuracy = 0                                                                             # Initialize the validation accuracy\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:                                                            # Loop through the validation dataloader\n",
    "                inputs, temp_inp= batch                                                        # Get the inputs and the true values\n",
    "                inputs, temp_inp= inputs.to(device), temp_inp.to(device)                                     # Move the inputs and true values to the GPU\n",
    "                u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).to(device)                       # Get the predictions\n",
    "                data_loss = loss_fn_data(u_pred, temp_inp)                                          # Calculate the data loss\n",
    "                # l1_regularization_loss = l1_regularization(model, lambda_l1)                  # Calculate the L1 regularization loss\n",
    "                loss = data_loss                                                                          # Calculate the total loss\n",
    "                val_accuracy += accuracy(u_pred, temp_inp)                                            # Calculate the validation accuracy\n",
    "                val_loss += loss.item()                                                                     # Add the loss to the validation set loss\n",
    "                                                                      # Calculate the average validation accuracy\n",
    "            val_losses.append(val_loss)                                                                 # Append the validation loss to the list of validation losses\n",
    "                                                                        # Append the validation loss to the list of validation losses\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training-Loss {train_loss:.2e}, Validation-Loss {val_loss:.2e}, Training-Accuracy {training_accuracy:.2e}, Validation-Accuracy {val_accuracy:.2e}\")\n",
    "\n",
    "    return train_losses, val_losses                                                                    # Return the training and validation losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(epochs, model, loss_fn_data, optimizer, train_dataloader, test_dataloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "    with torch.no_grad():   \n",
    "        for batch in test_dataloader:\n",
    "            inputs, temp_inp= batch\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp)\n",
    "            l1_regularization_loss = l1_regularization(model, lambda_l1)                  # Calculate the L1 regularization loss\n",
    "            loss = data_loss   + l1_regularization_loss  \n",
    "            test_accuracy = accuracy(u_pred, temp_inp)\n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss)\n",
    "    if epochs % 10 == 0:\n",
    "        print(f\"Epoch {epochs}, Test-Loss {test_loss:.2e},Test-Accuracy {test_accuracy:.2e}\")    \n",
    "    return test_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Button "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training-Loss 6.55e+05, Validation-Loss 6.55e+05, Training-Accuracy 1.00e+00, Validation-Accuracy 1.00e+00\n",
      "Epoch 10, Training-Loss 6.58e+05, Validation-Loss 6.53e+05, Training-Accuracy 9.99e-01, Validation-Accuracy 9.99e-01\n",
      "Epoch 20, Training-Loss 6.51e+05, Validation-Loss 6.54e+05, Training-Accuracy 9.97e-01, Validation-Accuracy 9.97e-01\n",
      "Epoch 30, Training-Loss 6.48e+05, Validation-Loss 6.51e+05, Training-Accuracy 9.96e-01, Validation-Accuracy 9.96e-01\n",
      "Epoch 40, Training-Loss 6.50e+05, Validation-Loss 6.53e+05, Training-Accuracy 9.94e-01, Validation-Accuracy 9.94e-01\n",
      "Epoch 50, Training-Loss 6.45e+05, Validation-Loss 6.47e+05, Training-Accuracy 9.93e-01, Validation-Accuracy 9.92e-01\n",
      "Epoch 60, Training-Loss 6.44e+05, Validation-Loss 6.49e+05, Training-Accuracy 9.91e-01, Validation-Accuracy 9.91e-01\n",
      "Epoch 70, Training-Loss 6.42e+05, Validation-Loss 6.39e+05, Training-Accuracy 9.89e-01, Validation-Accuracy 9.89e-01\n",
      "Epoch 80, Training-Loss 6.38e+05, Validation-Loss 6.34e+05, Training-Accuracy 9.88e-01, Validation-Accuracy 9.88e-01\n",
      "Epoch 90, Training-Loss 6.40e+05, Validation-Loss 6.37e+05, Training-Accuracy 9.86e-01, Validation-Accuracy 9.86e-01\n",
      "Epoch 100, Training-Loss 6.35e+05, Validation-Loss 6.30e+05, Training-Accuracy 9.84e-01, Validation-Accuracy 9.84e-01\n",
      "Epoch 110, Training-Loss 6.36e+05, Validation-Loss 6.32e+05, Training-Accuracy 9.82e-01, Validation-Accuracy 9.82e-01\n",
      "Epoch 120, Training-Loss 6.29e+05, Validation-Loss 6.25e+05, Training-Accuracy 9.80e-01, Validation-Accuracy 9.80e-01\n",
      "Epoch 130, Training-Loss 6.27e+05, Validation-Loss 6.28e+05, Training-Accuracy 9.78e-01, Validation-Accuracy 9.78e-01\n",
      "Epoch 140, Training-Loss 6.24e+05, Validation-Loss 6.25e+05, Training-Accuracy 9.76e-01, Validation-Accuracy 9.76e-01\n",
      "Epoch 150, Training-Loss 6.23e+05, Validation-Loss 6.20e+05, Training-Accuracy 9.73e-01, Validation-Accuracy 9.73e-01\n",
      "Epoch 160, Training-Loss 6.17e+05, Validation-Loss 6.22e+05, Training-Accuracy 9.71e-01, Validation-Accuracy 9.71e-01\n",
      "Epoch 170, Training-Loss 6.15e+05, Validation-Loss 6.14e+05, Training-Accuracy 9.69e-01, Validation-Accuracy 9.69e-01\n",
      "Epoch 180, Training-Loss 6.15e+05, Validation-Loss 6.14e+05, Training-Accuracy 9.67e-01, Validation-Accuracy 9.66e-01\n",
      "Epoch 190, Training-Loss 6.06e+05, Validation-Loss 6.12e+05, Training-Accuracy 9.64e-01, Validation-Accuracy 9.64e-01\n",
      "Epoch 200, Training-Loss 6.07e+05, Validation-Loss 6.07e+05, Training-Accuracy 9.62e-01, Validation-Accuracy 9.62e-01\n",
      "Epoch 210, Training-Loss 6.02e+05, Validation-Loss 6.01e+05, Training-Accuracy 9.60e-01, Validation-Accuracy 9.59e-01\n",
      "Epoch 220, Training-Loss 6.01e+05, Validation-Loss 6.02e+05, Training-Accuracy 9.57e-01, Validation-Accuracy 9.57e-01\n",
      "Epoch 230, Training-Loss 5.97e+05, Validation-Loss 5.98e+05, Training-Accuracy 9.55e-01, Validation-Accuracy 9.55e-01\n",
      "Epoch 240, Training-Loss 5.97e+05, Validation-Loss 5.91e+05, Training-Accuracy 9.53e-01, Validation-Accuracy 9.53e-01\n",
      "Epoch 250, Training-Loss 5.93e+05, Validation-Loss 5.92e+05, Training-Accuracy 9.51e-01, Validation-Accuracy 9.50e-01\n",
      "Epoch 260, Training-Loss 5.94e+05, Validation-Loss 5.93e+05, Training-Accuracy 9.49e-01, Validation-Accuracy 9.48e-01\n",
      "Epoch 270, Training-Loss 5.82e+05, Validation-Loss 5.85e+05, Training-Accuracy 9.46e-01, Validation-Accuracy 9.46e-01\n",
      "Epoch 280, Training-Loss 5.86e+05, Validation-Loss 5.89e+05, Training-Accuracy 9.44e-01, Validation-Accuracy 9.44e-01\n",
      "Epoch 290, Training-Loss 5.84e+05, Validation-Loss 5.81e+05, Training-Accuracy 9.42e-01, Validation-Accuracy 9.42e-01\n",
      "Epoch 300, Training-Loss 5.79e+05, Validation-Loss 5.76e+05, Training-Accuracy 9.40e-01, Validation-Accuracy 9.39e-01\n",
      "Epoch 310, Training-Loss 5.79e+05, Validation-Loss 5.74e+05, Training-Accuracy 9.38e-01, Validation-Accuracy 9.37e-01\n",
      "Epoch 320, Training-Loss 5.69e+05, Validation-Loss 5.77e+05, Training-Accuracy 9.35e-01, Validation-Accuracy 9.35e-01\n",
      "Epoch 330, Training-Loss 5.72e+05, Validation-Loss 5.72e+05, Training-Accuracy 9.33e-01, Validation-Accuracy 9.33e-01\n",
      "Epoch 340, Training-Loss 5.71e+05, Validation-Loss 5.69e+05, Training-Accuracy 9.31e-01, Validation-Accuracy 9.31e-01\n",
      "Epoch 350, Training-Loss 5.68e+05, Validation-Loss 5.66e+05, Training-Accuracy 9.29e-01, Validation-Accuracy 9.29e-01\n",
      "Epoch 360, Training-Loss 5.64e+05, Validation-Loss 5.64e+05, Training-Accuracy 9.27e-01, Validation-Accuracy 9.27e-01\n",
      "Epoch 370, Training-Loss 5.60e+05, Validation-Loss 5.59e+05, Training-Accuracy 9.25e-01, Validation-Accuracy 9.25e-01\n",
      "Epoch 380, Training-Loss 5.59e+05, Validation-Loss 5.58e+05, Training-Accuracy 9.23e-01, Validation-Accuracy 9.23e-01\n",
      "Epoch 390, Training-Loss 5.53e+05, Validation-Loss 5.55e+05, Training-Accuracy 9.21e-01, Validation-Accuracy 9.21e-01\n",
      "Epoch 400, Training-Loss 5.51e+05, Validation-Loss 5.53e+05, Training-Accuracy 9.19e-01, Validation-Accuracy 9.19e-01\n",
      "Epoch 410, Training-Loss 5.52e+05, Validation-Loss 5.52e+05, Training-Accuracy 9.17e-01, Validation-Accuracy 9.17e-01\n",
      "Epoch 420, Training-Loss 5.45e+05, Validation-Loss 5.45e+05, Training-Accuracy 9.15e-01, Validation-Accuracy 9.15e-01\n",
      "Epoch 430, Training-Loss 5.47e+05, Validation-Loss 5.47e+05, Training-Accuracy 9.13e-01, Validation-Accuracy 9.13e-01\n",
      "Epoch 440, Training-Loss 5.44e+05, Validation-Loss 5.45e+05, Training-Accuracy 9.11e-01, Validation-Accuracy 9.11e-01\n",
      "Epoch 450, Training-Loss 5.46e+05, Validation-Loss 5.41e+05, Training-Accuracy 9.09e-01, Validation-Accuracy 9.09e-01\n",
      "Epoch 460, Training-Loss 5.41e+05, Validation-Loss 5.35e+05, Training-Accuracy 9.07e-01, Validation-Accuracy 9.07e-01\n",
      "Epoch 470, Training-Loss 5.34e+05, Validation-Loss 5.37e+05, Training-Accuracy 9.05e-01, Validation-Accuracy 9.05e-01\n",
      "Epoch 480, Training-Loss 5.34e+05, Validation-Loss 5.35e+05, Training-Accuracy 9.03e-01, Validation-Accuracy 9.03e-01\n",
      "Epoch 490, Training-Loss 5.32e+05, Validation-Loss 5.34e+05, Training-Accuracy 9.01e-01, Validation-Accuracy 9.01e-01\n",
      "Epoch 500, Training-Loss 5.30e+05, Validation-Loss 5.28e+05, Training-Accuracy 8.99e-01, Validation-Accuracy 8.99e-01\n",
      "Epoch 510, Training-Loss 5.31e+05, Validation-Loss 5.29e+05, Training-Accuracy 8.98e-01, Validation-Accuracy 8.97e-01\n",
      "Epoch 520, Training-Loss 5.25e+05, Validation-Loss 5.27e+05, Training-Accuracy 8.95e-01, Validation-Accuracy 8.95e-01\n",
      "Epoch 530, Training-Loss 5.26e+05, Validation-Loss 5.24e+05, Training-Accuracy 8.94e-01, Validation-Accuracy 8.93e-01\n",
      "Epoch 540, Training-Loss 5.22e+05, Validation-Loss 5.22e+05, Training-Accuracy 8.92e-01, Validation-Accuracy 8.92e-01\n",
      "Epoch 550, Training-Loss 5.21e+05, Validation-Loss 5.13e+05, Training-Accuracy 8.90e-01, Validation-Accuracy 8.89e-01\n",
      "Epoch 560, Training-Loss 5.14e+05, Validation-Loss 5.11e+05, Training-Accuracy 8.88e-01, Validation-Accuracy 8.87e-01\n",
      "Epoch 570, Training-Loss 5.15e+05, Validation-Loss 5.09e+05, Training-Accuracy 8.86e-01, Validation-Accuracy 8.85e-01\n",
      "Epoch 580, Training-Loss 5.11e+05, Validation-Loss 5.11e+05, Training-Accuracy 8.84e-01, Validation-Accuracy 8.84e-01\n",
      "Epoch 590, Training-Loss 5.10e+05, Validation-Loss 5.08e+05, Training-Accuracy 8.82e-01, Validation-Accuracy 8.82e-01\n",
      "Epoch 600, Training-Loss 5.08e+05, Validation-Loss 5.12e+05, Training-Accuracy 8.80e-01, Validation-Accuracy 8.80e-01\n",
      "Epoch 610, Training-Loss 5.11e+05, Validation-Loss 5.01e+05, Training-Accuracy 8.79e-01, Validation-Accuracy 8.78e-01\n",
      "Epoch 620, Training-Loss 5.06e+05, Validation-Loss 5.01e+05, Training-Accuracy 8.77e-01, Validation-Accuracy 8.76e-01\n",
      "Epoch 630, Training-Loss 5.00e+05, Validation-Loss 4.99e+05, Training-Accuracy 8.74e-01, Validation-Accuracy 8.74e-01\n",
      "Epoch 640, Training-Loss 5.00e+05, Validation-Loss 4.98e+05, Training-Accuracy 8.73e-01, Validation-Accuracy 8.72e-01\n",
      "Epoch 650, Training-Loss 4.97e+05, Validation-Loss 5.01e+05, Training-Accuracy 8.71e-01, Validation-Accuracy 8.71e-01\n",
      "Epoch 660, Training-Loss 4.97e+05, Validation-Loss 4.96e+05, Training-Accuracy 8.69e-01, Validation-Accuracy 8.69e-01\n",
      "Epoch 670, Training-Loss 4.94e+05, Validation-Loss 4.91e+05, Training-Accuracy 8.67e-01, Validation-Accuracy 8.66e-01\n",
      "Epoch 680, Training-Loss 4.93e+05, Validation-Loss 4.87e+05, Training-Accuracy 8.65e-01, Validation-Accuracy 8.64e-01\n",
      "Epoch 690, Training-Loss 4.87e+05, Validation-Loss 4.88e+05, Training-Accuracy 8.62e-01, Validation-Accuracy 8.62e-01\n",
      "Epoch 700, Training-Loss 4.89e+05, Validation-Loss 4.85e+05, Training-Accuracy 8.61e-01, Validation-Accuracy 8.60e-01\n",
      "Epoch 710, Training-Loss 4.83e+05, Validation-Loss 4.82e+05, Training-Accuracy 8.58e-01, Validation-Accuracy 8.58e-01\n",
      "Epoch 720, Training-Loss 4.84e+05, Validation-Loss 4.78e+05, Training-Accuracy 8.57e-01, Validation-Accuracy 8.56e-01\n",
      "Epoch 730, Training-Loss 4.81e+05, Validation-Loss 4.78e+05, Training-Accuracy 8.55e-01, Validation-Accuracy 8.54e-01\n",
      "Epoch 740, Training-Loss 4.78e+05, Validation-Loss 4.75e+05, Training-Accuracy 8.53e-01, Validation-Accuracy 8.52e-01\n",
      "Epoch 750, Training-Loss 4.76e+05, Validation-Loss 4.71e+05, Training-Accuracy 8.51e-01, Validation-Accuracy 8.50e-01\n",
      "Epoch 760, Training-Loss 4.76e+05, Validation-Loss 4.73e+05, Training-Accuracy 8.49e-01, Validation-Accuracy 8.48e-01\n",
      "Epoch 770, Training-Loss 4.72e+05, Validation-Loss 4.69e+05, Training-Accuracy 8.47e-01, Validation-Accuracy 8.46e-01\n",
      "Epoch 780, Training-Loss 4.70e+05, Validation-Loss 4.69e+05, Training-Accuracy 8.45e-01, Validation-Accuracy 8.44e-01\n",
      "Epoch 790, Training-Loss 4.65e+05, Validation-Loss 4.64e+05, Training-Accuracy 8.42e-01, Validation-Accuracy 8.42e-01\n",
      "Epoch 800, Training-Loss 4.62e+05, Validation-Loss 4.64e+05, Training-Accuracy 8.40e-01, Validation-Accuracy 8.40e-01\n",
      "Epoch 810, Training-Loss 4.64e+05, Validation-Loss 4.63e+05, Training-Accuracy 8.39e-01, Validation-Accuracy 8.39e-01\n",
      "Epoch 820, Training-Loss 4.56e+05, Validation-Loss 4.59e+05, Training-Accuracy 8.36e-01, Validation-Accuracy 8.36e-01\n",
      "Epoch 830, Training-Loss 4.57e+05, Validation-Loss 4.53e+05, Training-Accuracy 8.34e-01, Validation-Accuracy 8.33e-01\n",
      "Epoch 840, Training-Loss 4.52e+05, Validation-Loss 4.51e+05, Training-Accuracy 8.32e-01, Validation-Accuracy 8.31e-01\n",
      "Epoch 850, Training-Loss 4.53e+05, Validation-Loss 4.53e+05, Training-Accuracy 8.30e-01, Validation-Accuracy 8.30e-01\n",
      "Epoch 860, Training-Loss 4.45e+05, Validation-Loss 4.52e+05, Training-Accuracy 8.27e-01, Validation-Accuracy 8.28e-01\n",
      "Epoch 870, Training-Loss 4.48e+05, Validation-Loss 4.44e+05, Training-Accuracy 8.26e-01, Validation-Accuracy 8.25e-01\n",
      "Epoch 880, Training-Loss 4.44e+05, Validation-Loss 4.46e+05, Training-Accuracy 8.24e-01, Validation-Accuracy 8.24e-01\n",
      "Epoch 890, Training-Loss 4.44e+05, Validation-Loss 4.39e+05, Training-Accuracy 8.22e-01, Validation-Accuracy 8.21e-01\n",
      "Epoch 900, Training-Loss 4.35e+05, Validation-Loss 4.39e+05, Training-Accuracy 8.19e-01, Validation-Accuracy 8.19e-01\n",
      "Epoch 910, Training-Loss 4.45e+05, Validation-Loss 4.39e+05, Training-Accuracy 8.19e-01, Validation-Accuracy 8.17e-01\n",
      "Epoch 920, Training-Loss 4.37e+05, Validation-Loss 4.40e+05, Training-Accuracy 8.16e-01, Validation-Accuracy 8.16e-01\n",
      "Epoch 930, Training-Loss 4.40e+05, Validation-Loss 4.35e+05, Training-Accuracy 8.15e-01, Validation-Accuracy 8.13e-01\n",
      "Epoch 940, Training-Loss 4.33e+05, Validation-Loss 4.30e+05, Training-Accuracy 8.12e-01, Validation-Accuracy 8.11e-01\n",
      "Epoch 950, Training-Loss 4.31e+05, Validation-Loss 4.30e+05, Training-Accuracy 8.10e-01, Validation-Accuracy 8.09e-01\n",
      "Epoch 960, Training-Loss 4.27e+05, Validation-Loss 4.31e+05, Training-Accuracy 8.07e-01, Validation-Accuracy 8.08e-01\n",
      "Epoch 970, Training-Loss 4.27e+05, Validation-Loss 4.25e+05, Training-Accuracy 8.06e-01, Validation-Accuracy 8.05e-01\n",
      "Epoch 980, Training-Loss 4.22e+05, Validation-Loss 4.25e+05, Training-Accuracy 8.03e-01, Validation-Accuracy 8.04e-01\n",
      "Epoch 990, Training-Loss 4.19e+05, Validation-Loss 4.21e+05, Training-Accuracy 8.01e-01, Validation-Accuracy 8.01e-01\n",
      "Epoch 1000, Training-Loss 4.22e+05, Validation-Loss 4.19e+05, Training-Accuracy 8.00e-01, Validation-Accuracy 7.99e-01\n",
      "Epoch 1010, Training-Loss 4.16e+05, Validation-Loss 4.19e+05, Training-Accuracy 7.97e-01, Validation-Accuracy 7.98e-01\n",
      "Epoch 1020, Training-Loss 4.17e+05, Validation-Loss 4.16e+05, Training-Accuracy 7.96e-01, Validation-Accuracy 7.96e-01\n",
      "Epoch 1030, Training-Loss 4.14e+05, Validation-Loss 4.12e+05, Training-Accuracy 7.94e-01, Validation-Accuracy 7.93e-01\n",
      "Epoch 1040, Training-Loss 4.10e+05, Validation-Loss 4.12e+05, Training-Accuracy 7.91e-01, Validation-Accuracy 7.92e-01\n",
      "Epoch 1050, Training-Loss 4.11e+05, Validation-Loss 4.10e+05, Training-Accuracy 7.90e-01, Validation-Accuracy 7.90e-01\n",
      "Epoch 1060, Training-Loss 4.09e+05, Validation-Loss 4.07e+05, Training-Accuracy 7.88e-01, Validation-Accuracy 7.88e-01\n",
      "Epoch 1070, Training-Loss 4.08e+05, Validation-Loss 4.05e+05, Training-Accuracy 7.87e-01, Validation-Accuracy 7.86e-01\n",
      "Epoch 1080, Training-Loss 4.01e+05, Validation-Loss 4.03e+05, Training-Accuracy 7.84e-01, Validation-Accuracy 7.84e-01\n",
      "Epoch 1090, Training-Loss 4.05e+05, Validation-Loss 4.00e+05, Training-Accuracy 7.83e-01, Validation-Accuracy 7.82e-01\n",
      "Epoch 1100, Training-Loss 4.06e+05, Validation-Loss 4.03e+05, Training-Accuracy 7.82e-01, Validation-Accuracy 7.81e-01\n",
      "Epoch 1110, Training-Loss 4.00e+05, Validation-Loss 3.95e+05, Training-Accuracy 7.79e-01, Validation-Accuracy 7.78e-01\n",
      "Epoch 1120, Training-Loss 3.98e+05, Validation-Loss 3.98e+05, Training-Accuracy 7.77e-01, Validation-Accuracy 7.77e-01\n",
      "Epoch 1130, Training-Loss 3.97e+05, Validation-Loss 3.93e+05, Training-Accuracy 7.75e-01, Validation-Accuracy 7.74e-01\n",
      "Epoch 1140, Training-Loss 3.87e+05, Validation-Loss 3.90e+05, Training-Accuracy 7.72e-01, Validation-Accuracy 7.72e-01\n",
      "Epoch 1150, Training-Loss 3.92e+05, Validation-Loss 3.91e+05, Training-Accuracy 7.71e-01, Validation-Accuracy 7.71e-01\n",
      "Epoch 1160, Training-Loss 3.88e+05, Validation-Loss 3.88e+05, Training-Accuracy 7.69e-01, Validation-Accuracy 7.69e-01\n",
      "Epoch 1170, Training-Loss 3.89e+05, Validation-Loss 3.89e+05, Training-Accuracy 7.68e-01, Validation-Accuracy 7.67e-01\n",
      "Epoch 1180, Training-Loss 3.84e+05, Validation-Loss 3.85e+05, Training-Accuracy 7.65e-01, Validation-Accuracy 7.65e-01\n",
      "Epoch 1190, Training-Loss 3.82e+05, Validation-Loss 3.81e+05, Training-Accuracy 7.63e-01, Validation-Accuracy 7.63e-01\n",
      "Epoch 1200, Training-Loss 3.81e+05, Validation-Loss 3.81e+05, Training-Accuracy 7.61e-01, Validation-Accuracy 7.61e-01\n",
      "Epoch 1210, Training-Loss 3.81e+05, Validation-Loss 3.79e+05, Training-Accuracy 7.60e-01, Validation-Accuracy 7.59e-01\n",
      "Epoch 1220, Training-Loss 3.76e+05, Validation-Loss 3.78e+05, Training-Accuracy 7.57e-01, Validation-Accuracy 7.58e-01\n",
      "Epoch 1230, Training-Loss 3.75e+05, Validation-Loss 3.75e+05, Training-Accuracy 7.56e-01, Validation-Accuracy 7.56e-01\n",
      "Epoch 1240, Training-Loss 3.73e+05, Validation-Loss 3.72e+05, Training-Accuracy 7.54e-01, Validation-Accuracy 7.53e-01\n",
      "Epoch 1250, Training-Loss 3.77e+05, Validation-Loss 3.67e+05, Training-Accuracy 7.53e-01, Validation-Accuracy 7.51e-01\n",
      "Epoch 1260, Training-Loss 3.72e+05, Validation-Loss 3.70e+05, Training-Accuracy 7.51e-01, Validation-Accuracy 7.50e-01\n",
      "Epoch 1270, Training-Loss 3.68e+05, Validation-Loss 3.65e+05, Training-Accuracy 7.48e-01, Validation-Accuracy 7.47e-01\n",
      "Epoch 1280, Training-Loss 3.63e+05, Validation-Loss 3.68e+05, Training-Accuracy 7.46e-01, Validation-Accuracy 7.47e-01\n",
      "Epoch 1290, Training-Loss 3.63e+05, Validation-Loss 3.63e+05, Training-Accuracy 7.44e-01, Validation-Accuracy 7.44e-01\n",
      "Epoch 1300, Training-Loss 3.65e+05, Validation-Loss 3.63e+05, Training-Accuracy 7.43e-01, Validation-Accuracy 7.43e-01\n",
      "Epoch 1310, Training-Loss 3.61e+05, Validation-Loss 3.61e+05, Training-Accuracy 7.41e-01, Validation-Accuracy 7.41e-01\n",
      "Epoch 1320, Training-Loss 3.58e+05, Validation-Loss 3.58e+05, Training-Accuracy 7.39e-01, Validation-Accuracy 7.39e-01\n",
      "Epoch 1330, Training-Loss 3.54e+05, Validation-Loss 3.56e+05, Training-Accuracy 7.36e-01, Validation-Accuracy 7.37e-01\n",
      "Epoch 1340, Training-Loss 3.56e+05, Validation-Loss 3.58e+05, Training-Accuracy 7.36e-01, Validation-Accuracy 7.36e-01\n",
      "Epoch 1350, Training-Loss 3.51e+05, Validation-Loss 3.52e+05, Training-Accuracy 7.33e-01, Validation-Accuracy 7.33e-01\n",
      "Epoch 1360, Training-Loss 3.51e+05, Validation-Loss 3.48e+05, Training-Accuracy 7.31e-01, Validation-Accuracy 7.30e-01\n",
      "Epoch 1370, Training-Loss 3.52e+05, Validation-Loss 3.48e+05, Training-Accuracy 7.30e-01, Validation-Accuracy 7.29e-01\n",
      "Epoch 1380, Training-Loss 3.48e+05, Validation-Loss 3.49e+05, Training-Accuracy 7.28e-01, Validation-Accuracy 7.28e-01\n",
      "Epoch 1390, Training-Loss 3.48e+05, Validation-Loss 3.45e+05, Training-Accuracy 7.26e-01, Validation-Accuracy 7.26e-01\n",
      "Epoch 1400, Training-Loss 3.45e+05, Validation-Loss 3.44e+05, Training-Accuracy 7.24e-01, Validation-Accuracy 7.24e-01\n",
      "Epoch 1410, Training-Loss 3.43e+05, Validation-Loss 3.47e+05, Training-Accuracy 7.23e-01, Validation-Accuracy 7.23e-01\n",
      "Epoch 1420, Training-Loss 3.43e+05, Validation-Loss 3.42e+05, Training-Accuracy 7.21e-01, Validation-Accuracy 7.21e-01\n",
      "Epoch 1430, Training-Loss 3.41e+05, Validation-Loss 3.37e+05, Training-Accuracy 7.19e-01, Validation-Accuracy 7.18e-01\n",
      "Epoch 1440, Training-Loss 3.38e+05, Validation-Loss 3.43e+05, Training-Accuracy 7.17e-01, Validation-Accuracy 7.18e-01\n",
      "Epoch 1450, Training-Loss 3.36e+05, Validation-Loss 3.34e+05, Training-Accuracy 7.15e-01, Validation-Accuracy 7.15e-01\n",
      "Epoch 1460, Training-Loss 3.37e+05, Validation-Loss 3.32e+05, Training-Accuracy 7.14e-01, Validation-Accuracy 7.12e-01\n",
      "Epoch 1470, Training-Loss 3.31e+05, Validation-Loss 3.31e+05, Training-Accuracy 7.11e-01, Validation-Accuracy 7.11e-01\n",
      "Epoch 1480, Training-Loss 3.27e+05, Validation-Loss 3.32e+05, Training-Accuracy 7.08e-01, Validation-Accuracy 7.10e-01\n",
      "Epoch 1490, Training-Loss 3.31e+05, Validation-Loss 3.31e+05, Training-Accuracy 7.08e-01, Validation-Accuracy 7.08e-01\n",
      "Epoch 1500, Training-Loss 3.29e+05, Validation-Loss 3.23e+05, Training-Accuracy 7.07e-01, Validation-Accuracy 7.05e-01\n",
      "Epoch 1510, Training-Loss 3.26e+05, Validation-Loss 3.26e+05, Training-Accuracy 7.04e-01, Validation-Accuracy 7.04e-01\n",
      "Epoch 1520, Training-Loss 3.27e+05, Validation-Loss 3.26e+05, Training-Accuracy 7.03e-01, Validation-Accuracy 7.03e-01\n",
      "Epoch 1530, Training-Loss 3.24e+05, Validation-Loss 3.22e+05, Training-Accuracy 7.01e-01, Validation-Accuracy 7.01e-01\n",
      "Epoch 1540, Training-Loss 3.22e+05, Validation-Loss 3.21e+05, Training-Accuracy 6.99e-01, Validation-Accuracy 6.99e-01\n",
      "Epoch 1550, Training-Loss 3.22e+05, Validation-Loss 3.18e+05, Training-Accuracy 6.98e-01, Validation-Accuracy 6.97e-01\n",
      "Epoch 1560, Training-Loss 3.17e+05, Validation-Loss 3.18e+05, Training-Accuracy 6.95e-01, Validation-Accuracy 6.95e-01\n",
      "Epoch 1570, Training-Loss 3.17e+05, Validation-Loss 3.17e+05, Training-Accuracy 6.94e-01, Validation-Accuracy 6.94e-01\n",
      "Epoch 1580, Training-Loss 3.16e+05, Validation-Loss 3.17e+05, Training-Accuracy 6.92e-01, Validation-Accuracy 6.92e-01\n",
      "Epoch 1590, Training-Loss 3.13e+05, Validation-Loss 3.12e+05, Training-Accuracy 6.90e-01, Validation-Accuracy 6.90e-01\n",
      "Epoch 1600, Training-Loss 3.12e+05, Validation-Loss 3.12e+05, Training-Accuracy 6.88e-01, Validation-Accuracy 6.88e-01\n",
      "Epoch 1610, Training-Loss 3.08e+05, Validation-Loss 3.08e+05, Training-Accuracy 6.86e-01, Validation-Accuracy 6.85e-01\n",
      "Epoch 1620, Training-Loss 3.08e+05, Validation-Loss 3.07e+05, Training-Accuracy 6.84e-01, Validation-Accuracy 6.84e-01\n",
      "Epoch 1630, Training-Loss 3.05e+05, Validation-Loss 3.08e+05, Training-Accuracy 6.82e-01, Validation-Accuracy 6.83e-01\n",
      "Epoch 1640, Training-Loss 3.02e+05, Validation-Loss 3.06e+05, Training-Accuracy 6.80e-01, Validation-Accuracy 6.81e-01\n",
      "Epoch 1650, Training-Loss 3.05e+05, Validation-Loss 3.03e+05, Training-Accuracy 6.80e-01, Validation-Accuracy 6.79e-01\n",
      "Epoch 1660, Training-Loss 2.99e+05, Validation-Loss 3.01e+05, Training-Accuracy 6.77e-01, Validation-Accuracy 6.77e-01\n",
      "Epoch 1670, Training-Loss 3.02e+05, Validation-Loss 2.96e+05, Training-Accuracy 6.77e-01, Validation-Accuracy 6.74e-01\n",
      "Epoch 1680, Training-Loss 2.99e+05, Validation-Loss 2.99e+05, Training-Accuracy 6.74e-01, Validation-Accuracy 6.74e-01\n",
      "Epoch 1690, Training-Loss 2.99e+05, Validation-Loss 2.96e+05, Training-Accuracy 6.73e-01, Validation-Accuracy 6.72e-01\n",
      "Epoch 1700, Training-Loss 2.94e+05, Validation-Loss 2.95e+05, Training-Accuracy 6.70e-01, Validation-Accuracy 6.70e-01\n",
      "Epoch 1710, Training-Loss 2.94e+05, Validation-Loss 2.93e+05, Training-Accuracy 6.69e-01, Validation-Accuracy 6.68e-01\n",
      "Epoch 1720, Training-Loss 2.94e+05, Validation-Loss 2.94e+05, Training-Accuracy 6.67e-01, Validation-Accuracy 6.67e-01\n",
      "Epoch 1730, Training-Loss 2.92e+05, Validation-Loss 2.92e+05, Training-Accuracy 6.66e-01, Validation-Accuracy 6.65e-01\n",
      "Epoch 1740, Training-Loss 2.90e+05, Validation-Loss 2.89e+05, Training-Accuracy 6.64e-01, Validation-Accuracy 6.63e-01\n",
      "Epoch 1750, Training-Loss 2.87e+05, Validation-Loss 2.92e+05, Training-Accuracy 6.61e-01, Validation-Accuracy 6.63e-01\n",
      "Epoch 1760, Training-Loss 2.87e+05, Validation-Loss 2.88e+05, Training-Accuracy 6.60e-01, Validation-Accuracy 6.60e-01\n",
      "Epoch 1770, Training-Loss 2.87e+05, Validation-Loss 2.85e+05, Training-Accuracy 6.59e-01, Validation-Accuracy 6.58e-01\n",
      "Epoch 1780, Training-Loss 2.83e+05, Validation-Loss 2.84e+05, Training-Accuracy 6.56e-01, Validation-Accuracy 6.56e-01\n",
      "Epoch 1790, Training-Loss 2.83e+05, Validation-Loss 2.82e+05, Training-Accuracy 6.55e-01, Validation-Accuracy 6.55e-01\n",
      "Epoch 1800, Training-Loss 2.81e+05, Validation-Loss 2.78e+05, Training-Accuracy 6.53e-01, Validation-Accuracy 6.52e-01\n",
      "Epoch 1810, Training-Loss 2.79e+05, Validation-Loss 2.78e+05, Training-Accuracy 6.51e-01, Validation-Accuracy 6.51e-01\n",
      "Epoch 1820, Training-Loss 2.82e+05, Validation-Loss 2.78e+05, Training-Accuracy 6.51e-01, Validation-Accuracy 6.50e-01\n",
      "Epoch 1830, Training-Loss 2.77e+05, Validation-Loss 2.76e+05, Training-Accuracy 6.48e-01, Validation-Accuracy 6.48e-01\n",
      "Epoch 1840, Training-Loss 2.74e+05, Validation-Loss 2.73e+05, Training-Accuracy 6.46e-01, Validation-Accuracy 6.45e-01\n",
      "Epoch 1850, Training-Loss 2.76e+05, Validation-Loss 2.71e+05, Training-Accuracy 6.46e-01, Validation-Accuracy 6.43e-01\n",
      "Epoch 1860, Training-Loss 2.74e+05, Validation-Loss 2.72e+05, Training-Accuracy 6.44e-01, Validation-Accuracy 6.43e-01\n",
      "Epoch 1870, Training-Loss 2.71e+05, Validation-Loss 2.68e+05, Training-Accuracy 6.41e-01, Validation-Accuracy 6.40e-01\n",
      "Epoch 1880, Training-Loss 2.68e+05, Validation-Loss 2.68e+05, Training-Accuracy 6.39e-01, Validation-Accuracy 6.39e-01\n",
      "Epoch 1890, Training-Loss 2.70e+05, Validation-Loss 2.68e+05, Training-Accuracy 6.38e-01, Validation-Accuracy 6.38e-01\n",
      "Epoch 1900, Training-Loss 2.66e+05, Validation-Loss 2.68e+05, Training-Accuracy 6.36e-01, Validation-Accuracy 6.36e-01\n",
      "Epoch 1910, Training-Loss 2.65e+05, Validation-Loss 2.62e+05, Training-Accuracy 6.34e-01, Validation-Accuracy 6.33e-01\n",
      "Epoch 1920, Training-Loss 2.63e+05, Validation-Loss 2.66e+05, Training-Accuracy 6.32e-01, Validation-Accuracy 6.33e-01\n",
      "Epoch 1930, Training-Loss 2.62e+05, Validation-Loss 2.60e+05, Training-Accuracy 6.30e-01, Validation-Accuracy 6.30e-01\n",
      "Epoch 1940, Training-Loss 2.62e+05, Validation-Loss 2.61e+05, Training-Accuracy 6.30e-01, Validation-Accuracy 6.29e-01\n",
      "Epoch 1950, Training-Loss 2.59e+05, Validation-Loss 2.60e+05, Training-Accuracy 6.27e-01, Validation-Accuracy 6.27e-01\n",
      "Epoch 1960, Training-Loss 2.54e+05, Validation-Loss 2.60e+05, Training-Accuracy 6.24e-01, Validation-Accuracy 6.26e-01\n",
      "Epoch 1970, Training-Loss 2.58e+05, Validation-Loss 2.56e+05, Training-Accuracy 6.24e-01, Validation-Accuracy 6.24e-01\n",
      "Epoch 1980, Training-Loss 2.57e+05, Validation-Loss 2.53e+05, Training-Accuracy 6.23e-01, Validation-Accuracy 6.21e-01\n",
      "Epoch 1990, Training-Loss 2.51e+05, Validation-Loss 2.51e+05, Training-Accuracy 6.19e-01, Validation-Accuracy 6.19e-01\n",
      "Epoch 2000, Training-Loss 2.52e+05, Validation-Loss 2.51e+05, Training-Accuracy 6.18e-01, Validation-Accuracy 6.18e-01\n",
      "Epoch 2010, Training-Loss 2.50e+05, Validation-Loss 2.51e+05, Training-Accuracy 6.17e-01, Validation-Accuracy 6.17e-01\n",
      "Epoch 2020, Training-Loss 2.50e+05, Validation-Loss 2.48e+05, Training-Accuracy 6.15e-01, Validation-Accuracy 6.14e-01\n",
      "Epoch 2030, Training-Loss 2.47e+05, Validation-Loss 2.49e+05, Training-Accuracy 6.13e-01, Validation-Accuracy 6.14e-01\n",
      "Epoch 2040, Training-Loss 2.47e+05, Validation-Loss 2.46e+05, Training-Accuracy 6.12e-01, Validation-Accuracy 6.11e-01\n",
      "Epoch 2050, Training-Loss 2.47e+05, Validation-Loss 2.43e+05, Training-Accuracy 6.11e-01, Validation-Accuracy 6.09e-01\n",
      "Epoch 2060, Training-Loss 2.43e+05, Validation-Loss 2.44e+05, Training-Accuracy 6.08e-01, Validation-Accuracy 6.08e-01\n",
      "Epoch 2070, Training-Loss 2.43e+05, Validation-Loss 2.44e+05, Training-Accuracy 6.07e-01, Validation-Accuracy 6.07e-01\n",
      "Epoch 2080, Training-Loss 2.39e+05, Validation-Loss 2.41e+05, Training-Accuracy 6.04e-01, Validation-Accuracy 6.05e-01\n",
      "Epoch 2090, Training-Loss 2.40e+05, Validation-Loss 2.38e+05, Training-Accuracy 6.03e-01, Validation-Accuracy 6.02e-01\n",
      "Epoch 2100, Training-Loss 2.40e+05, Validation-Loss 2.39e+05, Training-Accuracy 6.02e-01, Validation-Accuracy 6.02e-01\n",
      "Epoch 2110, Training-Loss 2.36e+05, Validation-Loss 2.36e+05, Training-Accuracy 5.99e-01, Validation-Accuracy 5.99e-01\n",
      "Epoch 2120, Training-Loss 2.37e+05, Validation-Loss 2.36e+05, Training-Accuracy 5.99e-01, Validation-Accuracy 5.98e-01\n",
      "Epoch 2130, Training-Loss 2.36e+05, Validation-Loss 2.34e+05, Training-Accuracy 5.97e-01, Validation-Accuracy 5.96e-01\n",
      "Epoch 2140, Training-Loss 2.36e+05, Validation-Loss 2.30e+05, Training-Accuracy 5.96e-01, Validation-Accuracy 5.93e-01\n",
      "Epoch 2150, Training-Loss 2.33e+05, Validation-Loss 2.29e+05, Training-Accuracy 5.94e-01, Validation-Accuracy 5.92e-01\n",
      "Epoch 2160, Training-Loss 2.30e+05, Validation-Loss 2.28e+05, Training-Accuracy 5.91e-01, Validation-Accuracy 5.90e-01\n",
      "Epoch 2170, Training-Loss 2.28e+05, Validation-Loss 2.29e+05, Training-Accuracy 5.89e-01, Validation-Accuracy 5.89e-01\n",
      "Epoch 2180, Training-Loss 2.27e+05, Validation-Loss 2.26e+05, Training-Accuracy 5.88e-01, Validation-Accuracy 5.87e-01\n",
      "Epoch 2190, Training-Loss 2.26e+05, Validation-Loss 2.26e+05, Training-Accuracy 5.86e-01, Validation-Accuracy 5.86e-01\n",
      "Epoch 2200, Training-Loss 2.27e+05, Validation-Loss 2.24e+05, Training-Accuracy 5.85e-01, Validation-Accuracy 5.84e-01\n",
      "Epoch 2210, Training-Loss 2.25e+05, Validation-Loss 2.24e+05, Training-Accuracy 5.84e-01, Validation-Accuracy 5.83e-01\n",
      "Epoch 2220, Training-Loss 2.21e+05, Validation-Loss 2.24e+05, Training-Accuracy 5.80e-01, Validation-Accuracy 5.82e-01\n",
      "Epoch 2230, Training-Loss 2.22e+05, Validation-Loss 2.20e+05, Training-Accuracy 5.80e-01, Validation-Accuracy 5.79e-01\n",
      "Epoch 2240, Training-Loss 2.19e+05, Validation-Loss 2.20e+05, Training-Accuracy 5.77e-01, Validation-Accuracy 5.78e-01\n",
      "Epoch 2250, Training-Loss 2.16e+05, Validation-Loss 2.20e+05, Training-Accuracy 5.75e-01, Validation-Accuracy 5.76e-01\n",
      "Epoch 2260, Training-Loss 2.18e+05, Validation-Loss 2.18e+05, Training-Accuracy 5.75e-01, Validation-Accuracy 5.75e-01\n",
      "Epoch 2270, Training-Loss 2.20e+05, Validation-Loss 2.16e+05, Training-Accuracy 5.75e-01, Validation-Accuracy 5.73e-01\n",
      "Epoch 2280, Training-Loss 2.17e+05, Validation-Loss 2.15e+05, Training-Accuracy 5.72e-01, Validation-Accuracy 5.71e-01\n",
      "Epoch 2290, Training-Loss 2.14e+05, Validation-Loss 2.13e+05, Training-Accuracy 5.69e-01, Validation-Accuracy 5.69e-01\n",
      "Epoch 2300, Training-Loss 2.16e+05, Validation-Loss 2.12e+05, Training-Accuracy 5.70e-01, Validation-Accuracy 5.68e-01\n",
      "Epoch 2310, Training-Loss 2.11e+05, Validation-Loss 2.08e+05, Training-Accuracy 5.66e-01, Validation-Accuracy 5.64e-01\n",
      "Epoch 2320, Training-Loss 2.12e+05, Validation-Loss 2.10e+05, Training-Accuracy 5.65e-01, Validation-Accuracy 5.64e-01\n",
      "Epoch 2330, Training-Loss 2.10e+05, Validation-Loss 2.09e+05, Training-Accuracy 5.63e-01, Validation-Accuracy 5.63e-01\n",
      "Epoch 2340, Training-Loss 2.07e+05, Validation-Loss 2.12e+05, Training-Accuracy 5.61e-01, Validation-Accuracy 5.63e-01\n",
      "Epoch 2350, Training-Loss 2.06e+05, Validation-Loss 2.05e+05, Training-Accuracy 5.59e-01, Validation-Accuracy 5.58e-01\n",
      "Epoch 2360, Training-Loss 2.05e+05, Validation-Loss 2.02e+05, Training-Accuracy 5.58e-01, Validation-Accuracy 5.56e-01\n",
      "Epoch 2370, Training-Loss 2.07e+05, Validation-Loss 2.04e+05, Training-Accuracy 5.58e-01, Validation-Accuracy 5.56e-01\n",
      "Epoch 2380, Training-Loss 2.03e+05, Validation-Loss 2.05e+05, Training-Accuracy 5.55e-01, Validation-Accuracy 5.56e-01\n",
      "Epoch 2390, Training-Loss 2.03e+05, Validation-Loss 2.05e+05, Training-Accuracy 5.54e-01, Validation-Accuracy 5.55e-01\n",
      "Epoch 2400, Training-Loss 2.00e+05, Validation-Loss 2.00e+05, Training-Accuracy 5.51e-01, Validation-Accuracy 5.51e-01\n",
      "Epoch 2410, Training-Loss 2.02e+05, Validation-Loss 1.98e+05, Training-Accuracy 5.51e-01, Validation-Accuracy 5.49e-01\n",
      "Epoch 2420, Training-Loss 1.99e+05, Validation-Loss 1.97e+05, Training-Accuracy 5.49e-01, Validation-Accuracy 5.47e-01\n",
      "Epoch 2430, Training-Loss 1.99e+05, Validation-Loss 1.96e+05, Training-Accuracy 5.48e-01, Validation-Accuracy 5.46e-01\n",
      "Epoch 2440, Training-Loss 1.97e+05, Validation-Loss 1.96e+05, Training-Accuracy 5.45e-01, Validation-Accuracy 5.45e-01\n",
      "Epoch 2450, Training-Loss 1.96e+05, Validation-Loss 1.94e+05, Training-Accuracy 5.44e-01, Validation-Accuracy 5.43e-01\n",
      "Epoch 2460, Training-Loss 1.93e+05, Validation-Loss 1.92e+05, Training-Accuracy 5.41e-01, Validation-Accuracy 5.41e-01\n",
      "Epoch 2470, Training-Loss 1.92e+05, Validation-Loss 1.91e+05, Training-Accuracy 5.39e-01, Validation-Accuracy 5.39e-01\n",
      "Epoch 2480, Training-Loss 1.90e+05, Validation-Loss 1.91e+05, Training-Accuracy 5.37e-01, Validation-Accuracy 5.38e-01\n",
      "Epoch 2490, Training-Loss 1.90e+05, Validation-Loss 1.92e+05, Training-Accuracy 5.37e-01, Validation-Accuracy 5.38e-01\n",
      "Epoch 2500, Training-Loss 1.89e+05, Validation-Loss 1.90e+05, Training-Accuracy 5.35e-01, Validation-Accuracy 5.36e-01\n",
      "Epoch 2510, Training-Loss 1.88e+05, Validation-Loss 1.88e+05, Training-Accuracy 5.34e-01, Validation-Accuracy 5.33e-01\n",
      "Epoch 2520, Training-Loss 1.88e+05, Validation-Loss 1.87e+05, Training-Accuracy 5.32e-01, Validation-Accuracy 5.32e-01\n",
      "Epoch 2530, Training-Loss 1.88e+05, Validation-Loss 1.85e+05, Training-Accuracy 5.31e-01, Validation-Accuracy 5.30e-01\n",
      "Epoch 2540, Training-Loss 1.85e+05, Validation-Loss 1.84e+05, Training-Accuracy 5.29e-01, Validation-Accuracy 5.28e-01\n",
      "Epoch 2550, Training-Loss 1.86e+05, Validation-Loss 1.86e+05, Training-Accuracy 5.29e-01, Validation-Accuracy 5.29e-01\n",
      "Epoch 2560, Training-Loss 1.82e+05, Validation-Loss 1.82e+05, Training-Accuracy 5.25e-01, Validation-Accuracy 5.25e-01\n",
      "Epoch 2570, Training-Loss 1.83e+05, Validation-Loss 1.81e+05, Training-Accuracy 5.25e-01, Validation-Accuracy 5.24e-01\n",
      "Epoch 2580, Training-Loss 1.80e+05, Validation-Loss 1.79e+05, Training-Accuracy 5.22e-01, Validation-Accuracy 5.22e-01\n",
      "Epoch 2590, Training-Loss 1.80e+05, Validation-Loss 1.80e+05, Training-Accuracy 5.21e-01, Validation-Accuracy 5.21e-01\n",
      "Epoch 2600, Training-Loss 1.77e+05, Validation-Loss 1.78e+05, Training-Accuracy 5.18e-01, Validation-Accuracy 5.19e-01\n",
      "Epoch 2610, Training-Loss 1.76e+05, Validation-Loss 1.76e+05, Training-Accuracy 5.17e-01, Validation-Accuracy 5.17e-01\n",
      "Epoch 2620, Training-Loss 1.74e+05, Validation-Loss 1.76e+05, Training-Accuracy 5.15e-01, Validation-Accuracy 5.16e-01\n",
      "Epoch 2630, Training-Loss 1.77e+05, Validation-Loss 1.74e+05, Training-Accuracy 5.16e-01, Validation-Accuracy 5.14e-01\n",
      "Epoch 2640, Training-Loss 1.73e+05, Validation-Loss 1.73e+05, Training-Accuracy 5.12e-01, Validation-Accuracy 5.12e-01\n",
      "Epoch 2650, Training-Loss 1.74e+05, Validation-Loss 1.71e+05, Training-Accuracy 5.12e-01, Validation-Accuracy 5.10e-01\n",
      "Epoch 2660, Training-Loss 1.73e+05, Validation-Loss 1.72e+05, Training-Accuracy 5.10e-01, Validation-Accuracy 5.09e-01\n",
      "Epoch 2670, Training-Loss 1.73e+05, Validation-Loss 1.70e+05, Training-Accuracy 5.09e-01, Validation-Accuracy 5.08e-01\n",
      "Epoch 2680, Training-Loss 1.69e+05, Validation-Loss 1.69e+05, Training-Accuracy 5.06e-01, Validation-Accuracy 5.06e-01\n",
      "Epoch 2690, Training-Loss 1.70e+05, Validation-Loss 1.67e+05, Training-Accuracy 5.06e-01, Validation-Accuracy 5.04e-01\n",
      "Epoch 2700, Training-Loss 1.67e+05, Validation-Loss 1.67e+05, Training-Accuracy 5.03e-01, Validation-Accuracy 5.03e-01\n",
      "Epoch 2710, Training-Loss 1.68e+05, Validation-Loss 1.66e+05, Training-Accuracy 5.03e-01, Validation-Accuracy 5.01e-01\n",
      "Epoch 2720, Training-Loss 1.65e+05, Validation-Loss 1.63e+05, Training-Accuracy 4.99e-01, Validation-Accuracy 4.98e-01\n",
      "Epoch 2730, Training-Loss 1.66e+05, Validation-Loss 1.65e+05, Training-Accuracy 4.99e-01, Validation-Accuracy 4.99e-01\n",
      "Epoch 2740, Training-Loss 1.64e+05, Validation-Loss 1.61e+05, Training-Accuracy 4.98e-01, Validation-Accuracy 4.95e-01\n",
      "Epoch 2750, Training-Loss 1.63e+05, Validation-Loss 1.62e+05, Training-Accuracy 4.96e-01, Validation-Accuracy 4.95e-01\n",
      "Epoch 2760, Training-Loss 1.63e+05, Validation-Loss 1.62e+05, Training-Accuracy 4.94e-01, Validation-Accuracy 4.94e-01\n",
      "Epoch 2770, Training-Loss 1.59e+05, Validation-Loss 1.61e+05, Training-Accuracy 4.91e-01, Validation-Accuracy 4.93e-01\n",
      "Epoch 2780, Training-Loss 1.61e+05, Validation-Loss 1.58e+05, Training-Accuracy 4.92e-01, Validation-Accuracy 4.90e-01\n",
      "Epoch 2790, Training-Loss 1.59e+05, Validation-Loss 1.61e+05, Training-Accuracy 4.89e-01, Validation-Accuracy 4.91e-01\n",
      "Epoch 2800, Training-Loss 1.58e+05, Validation-Loss 1.57e+05, Training-Accuracy 4.88e-01, Validation-Accuracy 4.87e-01\n",
      "Epoch 2810, Training-Loss 1.57e+05, Validation-Loss 1.56e+05, Training-Accuracy 4.86e-01, Validation-Accuracy 4.85e-01\n",
      "Epoch 2820, Training-Loss 1.55e+05, Validation-Loss 1.52e+05, Training-Accuracy 4.84e-01, Validation-Accuracy 4.82e-01\n",
      "Epoch 2830, Training-Loss 1.54e+05, Validation-Loss 1.56e+05, Training-Accuracy 4.83e-01, Validation-Accuracy 4.84e-01\n",
      "Epoch 2840, Training-Loss 1.54e+05, Validation-Loss 1.51e+05, Training-Accuracy 4.81e-01, Validation-Accuracy 4.79e-01\n",
      "Epoch 2850, Training-Loss 1.53e+05, Validation-Loss 1.51e+05, Training-Accuracy 4.80e-01, Validation-Accuracy 4.78e-01\n",
      "Epoch 2860, Training-Loss 1.52e+05, Validation-Loss 1.51e+05, Training-Accuracy 4.78e-01, Validation-Accuracy 4.77e-01\n",
      "Epoch 2870, Training-Loss 1.51e+05, Validation-Loss 1.50e+05, Training-Accuracy 4.77e-01, Validation-Accuracy 4.76e-01\n",
      "Epoch 2880, Training-Loss 1.52e+05, Validation-Loss 1.50e+05, Training-Accuracy 4.77e-01, Validation-Accuracy 4.75e-01\n",
      "Epoch 2890, Training-Loss 1.48e+05, Validation-Loss 1.49e+05, Training-Accuracy 4.73e-01, Validation-Accuracy 4.74e-01\n",
      "Epoch 2900, Training-Loss 1.47e+05, Validation-Loss 1.47e+05, Training-Accuracy 4.72e-01, Validation-Accuracy 4.71e-01\n",
      "Epoch 2910, Training-Loss 1.48e+05, Validation-Loss 1.45e+05, Training-Accuracy 4.71e-01, Validation-Accuracy 4.69e-01\n",
      "Epoch 2920, Training-Loss 1.47e+05, Validation-Loss 1.45e+05, Training-Accuracy 4.70e-01, Validation-Accuracy 4.68e-01\n",
      "Epoch 2930, Training-Loss 1.47e+05, Validation-Loss 1.44e+05, Training-Accuracy 4.69e-01, Validation-Accuracy 4.67e-01\n",
      "Epoch 2940, Training-Loss 1.44e+05, Validation-Loss 1.42e+05, Training-Accuracy 4.66e-01, Validation-Accuracy 4.64e-01\n",
      "Epoch 2950, Training-Loss 1.44e+05, Validation-Loss 1.43e+05, Training-Accuracy 4.65e-01, Validation-Accuracy 4.64e-01\n",
      "Epoch 2960, Training-Loss 1.42e+05, Validation-Loss 1.40e+05, Training-Accuracy 4.63e-01, Validation-Accuracy 4.61e-01\n",
      "Epoch 2970, Training-Loss 1.42e+05, Validation-Loss 1.42e+05, Training-Accuracy 4.62e-01, Validation-Accuracy 4.62e-01\n",
      "Epoch 2980, Training-Loss 1.40e+05, Validation-Loss 1.40e+05, Training-Accuracy 4.59e-01, Validation-Accuracy 4.59e-01\n",
      "Epoch 2990, Training-Loss 1.39e+05, Validation-Loss 1.37e+05, Training-Accuracy 4.57e-01, Validation-Accuracy 4.56e-01\n",
      "Epoch 3000, Training-Loss 1.40e+05, Validation-Loss 1.38e+05, Training-Accuracy 4.58e-01, Validation-Accuracy 4.56e-01\n",
      "Epoch 3010, Training-Loss 1.35e+05, Validation-Loss 1.37e+05, Training-Accuracy 4.53e-01, Validation-Accuracy 4.55e-01\n",
      "Epoch 3020, Training-Loss 1.37e+05, Validation-Loss 1.36e+05, Training-Accuracy 4.54e-01, Validation-Accuracy 4.52e-01\n",
      "Epoch 3030, Training-Loss 1.35e+05, Validation-Loss 1.34e+05, Training-Accuracy 4.51e-01, Validation-Accuracy 4.50e-01\n",
      "Epoch 3040, Training-Loss 1.36e+05, Validation-Loss 1.34e+05, Training-Accuracy 4.52e-01, Validation-Accuracy 4.49e-01\n",
      "Epoch 3050, Training-Loss 1.34e+05, Validation-Loss 1.33e+05, Training-Accuracy 4.49e-01, Validation-Accuracy 4.48e-01\n",
      "Epoch 3060, Training-Loss 1.34e+05, Validation-Loss 1.30e+05, Training-Accuracy 4.49e-01, Validation-Accuracy 4.45e-01\n",
      "Epoch 3070, Training-Loss 1.30e+05, Validation-Loss 1.32e+05, Training-Accuracy 4.45e-01, Validation-Accuracy 4.45e-01\n",
      "Epoch 3080, Training-Loss 1.29e+05, Validation-Loss 1.31e+05, Training-Accuracy 4.42e-01, Validation-Accuracy 4.44e-01\n",
      "Epoch 3090, Training-Loss 1.30e+05, Validation-Loss 1.28e+05, Training-Accuracy 4.42e-01, Validation-Accuracy 4.40e-01\n",
      "Epoch 3100, Training-Loss 1.31e+05, Validation-Loss 1.29e+05, Training-Accuracy 4.43e-01, Validation-Accuracy 4.41e-01\n",
      "Epoch 3110, Training-Loss 1.29e+05, Validation-Loss 1.27e+05, Training-Accuracy 4.40e-01, Validation-Accuracy 4.39e-01\n",
      "Epoch 3120, Training-Loss 1.27e+05, Validation-Loss 1.28e+05, Training-Accuracy 4.38e-01, Validation-Accuracy 4.38e-01\n",
      "Epoch 3130, Training-Loss 1.26e+05, Validation-Loss 1.27e+05, Training-Accuracy 4.36e-01, Validation-Accuracy 4.37e-01\n",
      "Epoch 3140, Training-Loss 1.27e+05, Validation-Loss 1.26e+05, Training-Accuracy 4.35e-01, Validation-Accuracy 4.35e-01\n",
      "Epoch 3150, Training-Loss 1.27e+05, Validation-Loss 1.24e+05, Training-Accuracy 4.35e-01, Validation-Accuracy 4.33e-01\n",
      "Epoch 3160, Training-Loss 1.24e+05, Validation-Loss 1.24e+05, Training-Accuracy 4.32e-01, Validation-Accuracy 4.32e-01\n",
      "Epoch 3170, Training-Loss 1.24e+05, Validation-Loss 1.23e+05, Training-Accuracy 4.31e-01, Validation-Accuracy 4.30e-01\n",
      "Epoch 3180, Training-Loss 1.21e+05, Validation-Loss 1.21e+05, Training-Accuracy 4.27e-01, Validation-Accuracy 4.28e-01\n",
      "Epoch 3190, Training-Loss 1.20e+05, Validation-Loss 1.21e+05, Training-Accuracy 4.26e-01, Validation-Accuracy 4.27e-01\n",
      "Epoch 3200, Training-Loss 1.21e+05, Validation-Loss 1.20e+05, Training-Accuracy 4.26e-01, Validation-Accuracy 4.24e-01\n",
      "Epoch 3210, Training-Loss 1.20e+05, Validation-Loss 1.20e+05, Training-Accuracy 4.25e-01, Validation-Accuracy 4.24e-01\n",
      "Epoch 3220, Training-Loss 1.18e+05, Validation-Loss 1.19e+05, Training-Accuracy 4.23e-01, Validation-Accuracy 4.23e-01\n",
      "Epoch 3230, Training-Loss 1.19e+05, Validation-Loss 1.17e+05, Training-Accuracy 4.22e-01, Validation-Accuracy 4.20e-01\n",
      "Epoch 3240, Training-Loss 1.19e+05, Validation-Loss 1.16e+05, Training-Accuracy 4.22e-01, Validation-Accuracy 4.19e-01\n",
      "Epoch 3250, Training-Loss 1.18e+05, Validation-Loss 1.18e+05, Training-Accuracy 4.19e-01, Validation-Accuracy 4.19e-01\n",
      "Epoch 3260, Training-Loss 1.18e+05, Validation-Loss 1.16e+05, Training-Accuracy 4.20e-01, Validation-Accuracy 4.17e-01\n",
      "Epoch 3270, Training-Loss 1.15e+05, Validation-Loss 1.13e+05, Training-Accuracy 4.16e-01, Validation-Accuracy 4.14e-01\n",
      "Epoch 3280, Training-Loss 1.14e+05, Validation-Loss 1.15e+05, Training-Accuracy 4.14e-01, Validation-Accuracy 4.15e-01\n",
      "Epoch 3290, Training-Loss 1.16e+05, Validation-Loss 1.14e+05, Training-Accuracy 4.15e-01, Validation-Accuracy 4.13e-01\n",
      "Epoch 3300, Training-Loss 1.13e+05, Validation-Loss 1.12e+05, Training-Accuracy 4.11e-01, Validation-Accuracy 4.10e-01\n",
      "Epoch 3310, Training-Loss 1.11e+05, Validation-Loss 1.12e+05, Training-Accuracy 4.09e-01, Validation-Accuracy 4.09e-01\n",
      "Epoch 3320, Training-Loss 1.11e+05, Validation-Loss 1.11e+05, Training-Accuracy 4.08e-01, Validation-Accuracy 4.08e-01\n",
      "Epoch 3330, Training-Loss 1.09e+05, Validation-Loss 1.12e+05, Training-Accuracy 4.05e-01, Validation-Accuracy 4.08e-01\n",
      "Epoch 3340, Training-Loss 1.09e+05, Validation-Loss 1.09e+05, Training-Accuracy 4.04e-01, Validation-Accuracy 4.05e-01\n",
      "Epoch 3350, Training-Loss 1.08e+05, Validation-Loss 1.07e+05, Training-Accuracy 4.03e-01, Validation-Accuracy 4.02e-01\n",
      "Epoch 3360, Training-Loss 1.08e+05, Validation-Loss 1.08e+05, Training-Accuracy 4.02e-01, Validation-Accuracy 4.02e-01\n",
      "Epoch 3370, Training-Loss 1.08e+05, Validation-Loss 1.07e+05, Training-Accuracy 4.01e-01, Validation-Accuracy 4.00e-01\n",
      "Epoch 3380, Training-Loss 1.06e+05, Validation-Loss 1.05e+05, Training-Accuracy 3.98e-01, Validation-Accuracy 3.98e-01\n",
      "Epoch 3390, Training-Loss 1.04e+05, Validation-Loss 1.04e+05, Training-Accuracy 3.97e-01, Validation-Accuracy 3.96e-01\n",
      "Epoch 3400, Training-Loss 1.04e+05, Validation-Loss 1.04e+05, Training-Accuracy 3.96e-01, Validation-Accuracy 3.96e-01\n",
      "Epoch 3410, Training-Loss 1.04e+05, Validation-Loss 1.05e+05, Training-Accuracy 3.95e-01, Validation-Accuracy 3.95e-01\n",
      "Epoch 3420, Training-Loss 1.03e+05, Validation-Loss 1.03e+05, Training-Accuracy 3.92e-01, Validation-Accuracy 3.94e-01\n",
      "Epoch 3430, Training-Loss 1.01e+05, Validation-Loss 1.02e+05, Training-Accuracy 3.90e-01, Validation-Accuracy 3.91e-01\n",
      "Epoch 3440, Training-Loss 1.04e+05, Validation-Loss 1.01e+05, Training-Accuracy 3.92e-01, Validation-Accuracy 3.90e-01\n",
      "Epoch 3450, Training-Loss 1.01e+05, Validation-Loss 9.95e+04, Training-Accuracy 3.89e-01, Validation-Accuracy 3.87e-01\n",
      "Epoch 3460, Training-Loss 1.01e+05, Validation-Loss 9.93e+04, Training-Accuracy 3.88e-01, Validation-Accuracy 3.86e-01\n",
      "Epoch 3470, Training-Loss 9.90e+04, Validation-Loss 9.91e+04, Training-Accuracy 3.86e-01, Validation-Accuracy 3.86e-01\n",
      "Epoch 3480, Training-Loss 9.94e+04, Validation-Loss 1.00e+05, Training-Accuracy 3.85e-01, Validation-Accuracy 3.86e-01\n",
      "Epoch 3490, Training-Loss 9.74e+04, Validation-Loss 9.88e+04, Training-Accuracy 3.82e-01, Validation-Accuracy 3.84e-01\n",
      "Epoch 3500, Training-Loss 9.61e+04, Validation-Loss 9.56e+04, Training-Accuracy 3.80e-01, Validation-Accuracy 3.80e-01\n",
      "Epoch 3510, Training-Loss 9.50e+04, Validation-Loss 9.63e+04, Training-Accuracy 3.78e-01, Validation-Accuracy 3.80e-01\n",
      "Epoch 3520, Training-Loss 9.53e+04, Validation-Loss 9.59e+04, Training-Accuracy 3.78e-01, Validation-Accuracy 3.79e-01\n",
      "Epoch 3530, Training-Loss 9.45e+04, Validation-Loss 9.44e+04, Training-Accuracy 3.76e-01, Validation-Accuracy 3.76e-01\n",
      "Epoch 3540, Training-Loss 9.25e+04, Validation-Loss 9.35e+04, Training-Accuracy 3.73e-01, Validation-Accuracy 3.75e-01\n",
      "Epoch 3550, Training-Loss 9.41e+04, Validation-Loss 9.39e+04, Training-Accuracy 3.75e-01, Validation-Accuracy 3.74e-01\n",
      "Epoch 3560, Training-Loss 9.37e+04, Validation-Loss 9.31e+04, Training-Accuracy 3.74e-01, Validation-Accuracy 3.73e-01\n",
      "Epoch 3570, Training-Loss 9.31e+04, Validation-Loss 9.30e+04, Training-Accuracy 3.72e-01, Validation-Accuracy 3.72e-01\n",
      "Epoch 3580, Training-Loss 9.09e+04, Validation-Loss 9.15e+04, Training-Accuracy 3.69e-01, Validation-Accuracy 3.70e-01\n",
      "Epoch 3590, Training-Loss 9.13e+04, Validation-Loss 9.15e+04, Training-Accuracy 3.69e-01, Validation-Accuracy 3.69e-01\n",
      "Epoch 3600, Training-Loss 8.96e+04, Validation-Loss 9.01e+04, Training-Accuracy 3.66e-01, Validation-Accuracy 3.67e-01\n",
      "Epoch 3610, Training-Loss 8.97e+04, Validation-Loss 8.85e+04, Training-Accuracy 3.66e-01, Validation-Accuracy 3.64e-01\n",
      "Epoch 3620, Training-Loss 8.86e+04, Validation-Loss 8.97e+04, Training-Accuracy 3.64e-01, Validation-Accuracy 3.66e-01\n",
      "Epoch 3630, Training-Loss 8.81e+04, Validation-Loss 8.73e+04, Training-Accuracy 3.63e-01, Validation-Accuracy 3.62e-01\n",
      "Epoch 3640, Training-Loss 8.88e+04, Validation-Loss 8.73e+04, Training-Accuracy 3.63e-01, Validation-Accuracy 3.61e-01\n",
      "Epoch 3650, Training-Loss 8.79e+04, Validation-Loss 8.58e+04, Training-Accuracy 3.61e-01, Validation-Accuracy 3.59e-01\n",
      "Epoch 3660, Training-Loss 8.66e+04, Validation-Loss 8.51e+04, Training-Accuracy 3.59e-01, Validation-Accuracy 3.57e-01\n",
      "Epoch 3670, Training-Loss 8.49e+04, Validation-Loss 8.50e+04, Training-Accuracy 3.56e-01, Validation-Accuracy 3.56e-01\n",
      "Epoch 3680, Training-Loss 8.36e+04, Validation-Loss 8.44e+04, Training-Accuracy 3.54e-01, Validation-Accuracy 3.55e-01\n",
      "Epoch 3690, Training-Loss 8.58e+04, Validation-Loss 8.53e+04, Training-Accuracy 3.56e-01, Validation-Accuracy 3.56e-01\n",
      "Epoch 3700, Training-Loss 8.40e+04, Validation-Loss 8.46e+04, Training-Accuracy 3.54e-01, Validation-Accuracy 3.54e-01\n",
      "Epoch 3710, Training-Loss 8.46e+04, Validation-Loss 8.18e+04, Training-Accuracy 3.54e-01, Validation-Accuracy 3.50e-01\n",
      "Epoch 3720, Training-Loss 8.22e+04, Validation-Loss 8.06e+04, Training-Accuracy 3.50e-01, Validation-Accuracy 3.48e-01\n",
      "Epoch 3730, Training-Loss 8.08e+04, Validation-Loss 8.10e+04, Training-Accuracy 3.48e-01, Validation-Accuracy 3.48e-01\n",
      "Epoch 3740, Training-Loss 8.18e+04, Validation-Loss 8.11e+04, Training-Accuracy 3.48e-01, Validation-Accuracy 3.47e-01\n",
      "Epoch 3750, Training-Loss 7.93e+04, Validation-Loss 8.10e+04, Training-Accuracy 3.44e-01, Validation-Accuracy 3.47e-01\n",
      "Epoch 3760, Training-Loss 7.89e+04, Validation-Loss 7.92e+04, Training-Accuracy 3.44e-01, Validation-Accuracy 3.43e-01\n",
      "Epoch 3770, Training-Loss 8.02e+04, Validation-Loss 7.82e+04, Training-Accuracy 3.44e-01, Validation-Accuracy 3.42e-01\n",
      "Epoch 3780, Training-Loss 7.90e+04, Validation-Loss 7.71e+04, Training-Accuracy 3.42e-01, Validation-Accuracy 3.40e-01\n",
      "Epoch 3790, Training-Loss 7.50e+04, Validation-Loss 7.57e+04, Training-Accuracy 3.37e-01, Validation-Accuracy 3.37e-01\n",
      "Epoch 3800, Training-Loss 7.74e+04, Validation-Loss 7.71e+04, Training-Accuracy 3.39e-01, Validation-Accuracy 3.39e-01\n",
      "Epoch 3810, Training-Loss 7.53e+04, Validation-Loss 7.69e+04, Training-Accuracy 3.36e-01, Validation-Accuracy 3.38e-01\n",
      "Epoch 3820, Training-Loss 7.48e+04, Validation-Loss 7.61e+04, Training-Accuracy 3.34e-01, Validation-Accuracy 3.36e-01\n",
      "Epoch 3830, Training-Loss 7.53e+04, Validation-Loss 7.34e+04, Training-Accuracy 3.34e-01, Validation-Accuracy 3.32e-01\n",
      "Epoch 3840, Training-Loss 7.44e+04, Validation-Loss 7.43e+04, Training-Accuracy 3.33e-01, Validation-Accuracy 3.33e-01\n",
      "Epoch 3850, Training-Loss 7.24e+04, Validation-Loss 7.28e+04, Training-Accuracy 3.29e-01, Validation-Accuracy 3.30e-01\n",
      "Epoch 3860, Training-Loss 7.31e+04, Validation-Loss 7.27e+04, Training-Accuracy 3.30e-01, Validation-Accuracy 3.30e-01\n",
      "Epoch 3870, Training-Loss 7.25e+04, Validation-Loss 7.38e+04, Training-Accuracy 3.29e-01, Validation-Accuracy 3.30e-01\n",
      "Epoch 3880, Training-Loss 7.33e+04, Validation-Loss 7.34e+04, Training-Accuracy 3.29e-01, Validation-Accuracy 3.29e-01\n",
      "Epoch 3890, Training-Loss 7.13e+04, Validation-Loss 7.13e+04, Training-Accuracy 3.26e-01, Validation-Accuracy 3.25e-01\n",
      "Epoch 3900, Training-Loss 7.11e+04, Validation-Loss 7.13e+04, Training-Accuracy 3.25e-01, Validation-Accuracy 3.25e-01\n",
      "Epoch 3910, Training-Loss 7.21e+04, Validation-Loss 7.00e+04, Training-Accuracy 3.26e-01, Validation-Accuracy 3.23e-01\n",
      "Epoch 3920, Training-Loss 7.16e+04, Validation-Loss 6.94e+04, Training-Accuracy 3.25e-01, Validation-Accuracy 3.21e-01\n",
      "Epoch 3930, Training-Loss 6.97e+04, Validation-Loss 6.88e+04, Training-Accuracy 3.21e-01, Validation-Accuracy 3.20e-01\n",
      "Epoch 3940, Training-Loss 6.83e+04, Validation-Loss 6.74e+04, Training-Accuracy 3.19e-01, Validation-Accuracy 3.17e-01\n",
      "Epoch 3950, Training-Loss 6.74e+04, Validation-Loss 6.77e+04, Training-Accuracy 3.17e-01, Validation-Accuracy 3.17e-01\n",
      "Epoch 3960, Training-Loss 6.81e+04, Validation-Loss 6.79e+04, Training-Accuracy 3.17e-01, Validation-Accuracy 3.17e-01\n",
      "Epoch 3970, Training-Loss 6.89e+04, Validation-Loss 6.56e+04, Training-Accuracy 3.18e-01, Validation-Accuracy 3.13e-01\n",
      "Epoch 3980, Training-Loss 6.65e+04, Validation-Loss 6.50e+04, Training-Accuracy 3.14e-01, Validation-Accuracy 3.12e-01\n",
      "Epoch 3990, Training-Loss 6.47e+04, Validation-Loss 6.45e+04, Training-Accuracy 3.11e-01, Validation-Accuracy 3.10e-01\n",
      "Epoch 4000, Training-Loss 6.61e+04, Validation-Loss 6.43e+04, Training-Accuracy 3.12e-01, Validation-Accuracy 3.09e-01\n",
      "Epoch 4010, Training-Loss 6.64e+04, Validation-Loss 6.25e+04, Training-Accuracy 3.12e-01, Validation-Accuracy 3.06e-01\n",
      "Epoch 4020, Training-Loss 6.50e+04, Validation-Loss 6.44e+04, Training-Accuracy 3.09e-01, Validation-Accuracy 3.09e-01\n",
      "Epoch 4030, Training-Loss 6.35e+04, Validation-Loss 6.34e+04, Training-Accuracy 3.07e-01, Validation-Accuracy 3.07e-01\n",
      "Epoch 4040, Training-Loss 6.20e+04, Validation-Loss 6.17e+04, Training-Accuracy 3.04e-01, Validation-Accuracy 3.03e-01\n",
      "Epoch 4050, Training-Loss 6.26e+04, Validation-Loss 6.35e+04, Training-Accuracy 3.05e-01, Validation-Accuracy 3.06e-01\n",
      "Epoch 4060, Training-Loss 6.31e+04, Validation-Loss 6.06e+04, Training-Accuracy 3.04e-01, Validation-Accuracy 3.01e-01\n",
      "Epoch 4070, Training-Loss 6.20e+04, Validation-Loss 6.17e+04, Training-Accuracy 3.02e-01, Validation-Accuracy 3.02e-01\n",
      "Epoch 4080, Training-Loss 6.02e+04, Validation-Loss 6.02e+04, Training-Accuracy 2.99e-01, Validation-Accuracy 2.99e-01\n",
      "Epoch 4090, Training-Loss 6.03e+04, Validation-Loss 6.02e+04, Training-Accuracy 2.99e-01, Validation-Accuracy 2.99e-01\n",
      "Epoch 4100, Training-Loss 5.98e+04, Validation-Loss 5.95e+04, Training-Accuracy 2.97e-01, Validation-Accuracy 2.97e-01\n",
      "Epoch 4110, Training-Loss 5.99e+04, Validation-Loss 5.85e+04, Training-Accuracy 2.97e-01, Validation-Accuracy 2.94e-01\n",
      "Epoch 4120, Training-Loss 5.92e+04, Validation-Loss 5.90e+04, Training-Accuracy 2.96e-01, Validation-Accuracy 2.95e-01\n",
      "Epoch 4130, Training-Loss 5.69e+04, Validation-Loss 5.82e+04, Training-Accuracy 2.91e-01, Validation-Accuracy 2.93e-01\n",
      "Epoch 4140, Training-Loss 5.79e+04, Validation-Loss 5.77e+04, Training-Accuracy 2.92e-01, Validation-Accuracy 2.92e-01\n",
      "Epoch 4150, Training-Loss 5.56e+04, Validation-Loss 5.85e+04, Training-Accuracy 2.88e-01, Validation-Accuracy 2.93e-01\n",
      "Epoch 4160, Training-Loss 5.67e+04, Validation-Loss 5.63e+04, Training-Accuracy 2.89e-01, Validation-Accuracy 2.88e-01\n",
      "Epoch 4170, Training-Loss 5.56e+04, Validation-Loss 5.49e+04, Training-Accuracy 2.87e-01, Validation-Accuracy 2.85e-01\n",
      "Epoch 4180, Training-Loss 5.66e+04, Validation-Loss 5.57e+04, Training-Accuracy 2.89e-01, Validation-Accuracy 2.87e-01\n",
      "Epoch 4190, Training-Loss 5.61e+04, Validation-Loss 5.50e+04, Training-Accuracy 2.87e-01, Validation-Accuracy 2.85e-01\n",
      "Epoch 4200, Training-Loss 5.45e+04, Validation-Loss 5.51e+04, Training-Accuracy 2.84e-01, Validation-Accuracy 2.85e-01\n",
      "Epoch 4210, Training-Loss 5.32e+04, Validation-Loss 5.38e+04, Training-Accuracy 2.81e-01, Validation-Accuracy 2.82e-01\n",
      "Epoch 4220, Training-Loss 5.42e+04, Validation-Loss 5.42e+04, Training-Accuracy 2.82e-01, Validation-Accuracy 2.82e-01\n",
      "Epoch 4230, Training-Loss 5.30e+04, Validation-Loss 5.37e+04, Training-Accuracy 2.80e-01, Validation-Accuracy 2.81e-01\n",
      "Epoch 4240, Training-Loss 5.25e+04, Validation-Loss 5.33e+04, Training-Accuracy 2.78e-01, Validation-Accuracy 2.80e-01\n",
      "Epoch 4250, Training-Loss 5.35e+04, Validation-Loss 5.29e+04, Training-Accuracy 2.80e-01, Validation-Accuracy 2.79e-01\n",
      "Epoch 4260, Training-Loss 5.23e+04, Validation-Loss 5.19e+04, Training-Accuracy 2.77e-01, Validation-Accuracy 2.77e-01\n",
      "Epoch 4270, Training-Loss 5.00e+04, Validation-Loss 5.09e+04, Training-Accuracy 2.72e-01, Validation-Accuracy 2.74e-01\n",
      "Epoch 4280, Training-Loss 5.10e+04, Validation-Loss 5.07e+04, Training-Accuracy 2.74e-01, Validation-Accuracy 2.73e-01\n",
      "Epoch 4290, Training-Loss 4.95e+04, Validation-Loss 4.99e+04, Training-Accuracy 2.71e-01, Validation-Accuracy 2.72e-01\n",
      "Epoch 4300, Training-Loss 5.07e+04, Validation-Loss 4.99e+04, Training-Accuracy 2.73e-01, Validation-Accuracy 2.71e-01\n",
      "Epoch 4310, Training-Loss 4.90e+04, Validation-Loss 5.03e+04, Training-Accuracy 2.69e-01, Validation-Accuracy 2.72e-01\n",
      "Epoch 4320, Training-Loss 4.91e+04, Validation-Loss 4.89e+04, Training-Accuracy 2.68e-01, Validation-Accuracy 2.68e-01\n",
      "Epoch 4330, Training-Loss 4.87e+04, Validation-Loss 4.91e+04, Training-Accuracy 2.67e-01, Validation-Accuracy 2.67e-01\n",
      "Epoch 4340, Training-Loss 4.81e+04, Validation-Loss 4.66e+04, Training-Accuracy 2.65e-01, Validation-Accuracy 2.63e-01\n",
      "Epoch 4350, Training-Loss 4.84e+04, Validation-Loss 4.76e+04, Training-Accuracy 2.66e-01, Validation-Accuracy 2.64e-01\n",
      "Epoch 4360, Training-Loss 4.70e+04, Validation-Loss 4.65e+04, Training-Accuracy 2.62e-01, Validation-Accuracy 2.62e-01\n",
      "Epoch 4370, Training-Loss 4.52e+04, Validation-Loss 4.66e+04, Training-Accuracy 2.59e-01, Validation-Accuracy 2.62e-01\n",
      "Epoch 4380, Training-Loss 4.76e+04, Validation-Loss 4.56e+04, Training-Accuracy 2.63e-01, Validation-Accuracy 2.59e-01\n",
      "Epoch 4390, Training-Loss 4.58e+04, Validation-Loss 4.66e+04, Training-Accuracy 2.59e-01, Validation-Accuracy 2.61e-01\n",
      "Epoch 4400, Training-Loss 4.51e+04, Validation-Loss 4.59e+04, Training-Accuracy 2.57e-01, Validation-Accuracy 2.59e-01\n",
      "Epoch 4410, Training-Loss 4.49e+04, Validation-Loss 4.56e+04, Training-Accuracy 2.56e-01, Validation-Accuracy 2.58e-01\n",
      "Epoch 4420, Training-Loss 4.59e+04, Validation-Loss 4.38e+04, Training-Accuracy 2.58e-01, Validation-Accuracy 2.54e-01\n",
      "Epoch 4430, Training-Loss 4.43e+04, Validation-Loss 4.50e+04, Training-Accuracy 2.55e-01, Validation-Accuracy 2.56e-01\n",
      "Epoch 4440, Training-Loss 4.43e+04, Validation-Loss 4.33e+04, Training-Accuracy 2.54e-01, Validation-Accuracy 2.52e-01\n",
      "Epoch 4450, Training-Loss 4.40e+04, Validation-Loss 4.46e+04, Training-Accuracy 2.53e-01, Validation-Accuracy 2.54e-01\n",
      "Epoch 4460, Training-Loss 4.21e+04, Validation-Loss 4.27e+04, Training-Accuracy 2.48e-01, Validation-Accuracy 2.50e-01\n",
      "Epoch 4470, Training-Loss 4.23e+04, Validation-Loss 4.21e+04, Training-Accuracy 2.49e-01, Validation-Accuracy 2.49e-01\n",
      "Epoch 4480, Training-Loss 4.23e+04, Validation-Loss 4.21e+04, Training-Accuracy 2.48e-01, Validation-Accuracy 2.48e-01\n",
      "Epoch 4490, Training-Loss 4.15e+04, Validation-Loss 4.16e+04, Training-Accuracy 2.46e-01, Validation-Accuracy 2.46e-01\n",
      "Epoch 4500, Training-Loss 4.21e+04, Validation-Loss 4.08e+04, Training-Accuracy 2.47e-01, Validation-Accuracy 2.44e-01\n",
      "Epoch 4510, Training-Loss 4.11e+04, Validation-Loss 4.06e+04, Training-Accuracy 2.45e-01, Validation-Accuracy 2.43e-01\n",
      "Epoch 4520, Training-Loss 4.11e+04, Validation-Loss 4.02e+04, Training-Accuracy 2.45e-01, Validation-Accuracy 2.42e-01\n",
      "Epoch 4530, Training-Loss 4.17e+04, Validation-Loss 3.97e+04, Training-Accuracy 2.45e-01, Validation-Accuracy 2.41e-01\n",
      "Epoch 4540, Training-Loss 4.01e+04, Validation-Loss 4.06e+04, Training-Accuracy 2.41e-01, Validation-Accuracy 2.42e-01\n",
      "Epoch 4550, Training-Loss 3.94e+04, Validation-Loss 4.02e+04, Training-Accuracy 2.39e-01, Validation-Accuracy 2.41e-01\n",
      "Epoch 4560, Training-Loss 3.91e+04, Validation-Loss 3.90e+04, Training-Accuracy 2.38e-01, Validation-Accuracy 2.38e-01\n",
      "Epoch 4570, Training-Loss 3.87e+04, Validation-Loss 3.89e+04, Training-Accuracy 2.37e-01, Validation-Accuracy 2.38e-01\n",
      "Epoch 4580, Training-Loss 3.88e+04, Validation-Loss 3.80e+04, Training-Accuracy 2.37e-01, Validation-Accuracy 2.36e-01\n",
      "Epoch 4590, Training-Loss 3.78e+04, Validation-Loss 3.81e+04, Training-Accuracy 2.35e-01, Validation-Accuracy 2.35e-01\n",
      "Epoch 4600, Training-Loss 3.76e+04, Validation-Loss 3.74e+04, Training-Accuracy 2.33e-01, Validation-Accuracy 2.33e-01\n",
      "Epoch 4610, Training-Loss 3.71e+04, Validation-Loss 3.76e+04, Training-Accuracy 2.32e-01, Validation-Accuracy 2.33e-01\n",
      "Epoch 4620, Training-Loss 3.70e+04, Validation-Loss 3.65e+04, Training-Accuracy 2.32e-01, Validation-Accuracy 2.30e-01\n",
      "Epoch 4630, Training-Loss 3.67e+04, Validation-Loss 3.69e+04, Training-Accuracy 2.30e-01, Validation-Accuracy 2.31e-01\n",
      "Epoch 4640, Training-Loss 3.63e+04, Validation-Loss 3.61e+04, Training-Accuracy 2.29e-01, Validation-Accuracy 2.29e-01\n",
      "Epoch 4650, Training-Loss 3.73e+04, Validation-Loss 3.54e+04, Training-Accuracy 2.31e-01, Validation-Accuracy 2.26e-01\n",
      "Epoch 4660, Training-Loss 3.53e+04, Validation-Loss 3.47e+04, Training-Accuracy 2.26e-01, Validation-Accuracy 2.24e-01\n",
      "Epoch 4670, Training-Loss 3.49e+04, Validation-Loss 3.49e+04, Training-Accuracy 2.24e-01, Validation-Accuracy 2.24e-01\n",
      "Epoch 4680, Training-Loss 3.46e+04, Validation-Loss 3.41e+04, Training-Accuracy 2.24e-01, Validation-Accuracy 2.22e-01\n",
      "Epoch 4690, Training-Loss 3.42e+04, Validation-Loss 3.46e+04, Training-Accuracy 2.22e-01, Validation-Accuracy 2.23e-01\n",
      "Epoch 4700, Training-Loss 3.41e+04, Validation-Loss 3.34e+04, Training-Accuracy 2.22e-01, Validation-Accuracy 2.19e-01\n",
      "Epoch 4710, Training-Loss 3.27e+04, Validation-Loss 3.37e+04, Training-Accuracy 2.17e-01, Validation-Accuracy 2.20e-01\n",
      "Epoch 4720, Training-Loss 3.32e+04, Validation-Loss 3.33e+04, Training-Accuracy 2.19e-01, Validation-Accuracy 2.19e-01\n",
      "Epoch 4730, Training-Loss 3.31e+04, Validation-Loss 3.28e+04, Training-Accuracy 2.18e-01, Validation-Accuracy 2.18e-01\n",
      "Epoch 4740, Training-Loss 3.21e+04, Validation-Loss 3.36e+04, Training-Accuracy 2.15e-01, Validation-Accuracy 2.19e-01\n",
      "Epoch 4750, Training-Loss 3.24e+04, Validation-Loss 3.25e+04, Training-Accuracy 2.15e-01, Validation-Accuracy 2.16e-01\n",
      "Epoch 4760, Training-Loss 3.18e+04, Validation-Loss 3.19e+04, Training-Accuracy 2.14e-01, Validation-Accuracy 2.14e-01\n",
      "Epoch 4770, Training-Loss 3.20e+04, Validation-Loss 3.11e+04, Training-Accuracy 2.14e-01, Validation-Accuracy 2.12e-01\n",
      "Epoch 4780, Training-Loss 3.00e+04, Validation-Loss 3.07e+04, Training-Accuracy 2.09e-01, Validation-Accuracy 2.10e-01\n",
      "Epoch 4790, Training-Loss 3.16e+04, Validation-Loss 3.10e+04, Training-Accuracy 2.12e-01, Validation-Accuracy 2.11e-01\n",
      "Epoch 4800, Training-Loss 3.12e+04, Validation-Loss 3.09e+04, Training-Accuracy 2.11e-01, Validation-Accuracy 2.10e-01\n",
      "Epoch 4810, Training-Loss 3.00e+04, Validation-Loss 2.95e+04, Training-Accuracy 2.07e-01, Validation-Accuracy 2.06e-01\n",
      "Epoch 4820, Training-Loss 2.92e+04, Validation-Loss 3.02e+04, Training-Accuracy 2.05e-01, Validation-Accuracy 2.08e-01\n",
      "Epoch 4830, Training-Loss 2.98e+04, Validation-Loss 2.97e+04, Training-Accuracy 2.06e-01, Validation-Accuracy 2.06e-01\n",
      "Epoch 4840, Training-Loss 2.99e+04, Validation-Loss 3.03e+04, Training-Accuracy 2.07e-01, Validation-Accuracy 2.07e-01\n",
      "Epoch 4850, Training-Loss 2.93e+04, Validation-Loss 2.87e+04, Training-Accuracy 2.04e-01, Validation-Accuracy 2.03e-01\n",
      "Epoch 4860, Training-Loss 2.90e+04, Validation-Loss 2.86e+04, Training-Accuracy 2.04e-01, Validation-Accuracy 2.02e-01\n",
      "Epoch 4870, Training-Loss 2.87e+04, Validation-Loss 2.84e+04, Training-Accuracy 2.02e-01, Validation-Accuracy 2.01e-01\n",
      "Epoch 4880, Training-Loss 2.80e+04, Validation-Loss 2.73e+04, Training-Accuracy 2.00e-01, Validation-Accuracy 1.98e-01\n",
      "Epoch 4890, Training-Loss 2.88e+04, Validation-Loss 2.72e+04, Training-Accuracy 2.02e-01, Validation-Accuracy 1.97e-01\n",
      "Epoch 4900, Training-Loss 2.82e+04, Validation-Loss 2.70e+04, Training-Accuracy 2.00e-01, Validation-Accuracy 1.96e-01\n",
      "Epoch 4910, Training-Loss 2.65e+04, Validation-Loss 2.62e+04, Training-Accuracy 1.95e-01, Validation-Accuracy 1.94e-01\n",
      "Epoch 4920, Training-Loss 2.73e+04, Validation-Loss 2.60e+04, Training-Accuracy 1.96e-01, Validation-Accuracy 1.93e-01\n",
      "Epoch 4930, Training-Loss 2.63e+04, Validation-Loss 2.57e+04, Training-Accuracy 1.93e-01, Validation-Accuracy 1.92e-01\n",
      "Epoch 4940, Training-Loss 2.62e+04, Validation-Loss 2.68e+04, Training-Accuracy 1.93e-01, Validation-Accuracy 1.94e-01\n",
      "Epoch 4950, Training-Loss 2.57e+04, Validation-Loss 2.58e+04, Training-Accuracy 1.91e-01, Validation-Accuracy 1.91e-01\n",
      "Epoch 4960, Training-Loss 2.53e+04, Validation-Loss 2.62e+04, Training-Accuracy 1.90e-01, Validation-Accuracy 1.92e-01\n",
      "Epoch 4970, Training-Loss 2.64e+04, Validation-Loss 2.44e+04, Training-Accuracy 1.93e-01, Validation-Accuracy 1.87e-01\n",
      "Epoch 4980, Training-Loss 2.67e+04, Validation-Loss 2.50e+04, Training-Accuracy 1.93e-01, Validation-Accuracy 1.88e-01\n",
      "Epoch 4990, Training-Loss 2.45e+04, Validation-Loss 2.45e+04, Training-Accuracy 1.86e-01, Validation-Accuracy 1.86e-01\n",
      "Epoch 5000, Training-Loss 2.51e+04, Validation-Loss 2.43e+04, Training-Accuracy 1.88e-01, Validation-Accuracy 1.86e-01\n",
      "Epoch 5010, Training-Loss 2.47e+04, Validation-Loss 2.46e+04, Training-Accuracy 1.86e-01, Validation-Accuracy 1.86e-01\n",
      "Epoch 5020, Training-Loss 2.40e+04, Validation-Loss 2.33e+04, Training-Accuracy 1.85e-01, Validation-Accuracy 1.82e-01\n",
      "Epoch 5030, Training-Loss 2.45e+04, Validation-Loss 2.30e+04, Training-Accuracy 1.85e-01, Validation-Accuracy 1.81e-01\n",
      "Epoch 5040, Training-Loss 2.39e+04, Validation-Loss 2.34e+04, Training-Accuracy 1.83e-01, Validation-Accuracy 1.81e-01\n",
      "Epoch 5050, Training-Loss 2.34e+04, Validation-Loss 2.35e+04, Training-Accuracy 1.81e-01, Validation-Accuracy 1.81e-01\n",
      "Epoch 5060, Training-Loss 2.36e+04, Validation-Loss 2.28e+04, Training-Accuracy 1.82e-01, Validation-Accuracy 1.79e-01\n",
      "Epoch 5070, Training-Loss 2.32e+04, Validation-Loss 2.25e+04, Training-Accuracy 1.81e-01, Validation-Accuracy 1.78e-01\n",
      "Epoch 5080, Training-Loss 2.32e+04, Validation-Loss 2.25e+04, Training-Accuracy 1.80e-01, Validation-Accuracy 1.77e-01\n",
      "Epoch 5090, Training-Loss 2.29e+04, Validation-Loss 2.28e+04, Training-Accuracy 1.78e-01, Validation-Accuracy 1.78e-01\n",
      "Epoch 5100, Training-Loss 2.24e+04, Validation-Loss 2.16e+04, Training-Accuracy 1.77e-01, Validation-Accuracy 1.74e-01\n",
      "Epoch 5110, Training-Loss 2.17e+04, Validation-Loss 2.15e+04, Training-Accuracy 1.74e-01, Validation-Accuracy 1.73e-01\n",
      "Epoch 5120, Training-Loss 2.15e+04, Validation-Loss 2.19e+04, Training-Accuracy 1.73e-01, Validation-Accuracy 1.74e-01\n",
      "Epoch 5130, Training-Loss 2.15e+04, Validation-Loss 2.11e+04, Training-Accuracy 1.73e-01, Validation-Accuracy 1.71e-01\n",
      "Epoch 5140, Training-Loss 2.15e+04, Validation-Loss 2.13e+04, Training-Accuracy 1.72e-01, Validation-Accuracy 1.71e-01\n",
      "Epoch 5150, Training-Loss 2.06e+04, Validation-Loss 2.10e+04, Training-Accuracy 1.69e-01, Validation-Accuracy 1.71e-01\n",
      "Epoch 5160, Training-Loss 2.05e+04, Validation-Loss 2.04e+04, Training-Accuracy 1.69e-01, Validation-Accuracy 1.68e-01\n",
      "Epoch 5170, Training-Loss 1.90e+04, Validation-Loss 1.95e+04, Training-Accuracy 1.64e-01, Validation-Accuracy 1.66e-01\n",
      "Epoch 5180, Training-Loss 2.05e+04, Validation-Loss 1.99e+04, Training-Accuracy 1.68e-01, Validation-Accuracy 1.66e-01\n",
      "Epoch 5190, Training-Loss 2.02e+04, Validation-Loss 2.01e+04, Training-Accuracy 1.68e-01, Validation-Accuracy 1.66e-01\n",
      "Epoch 5200, Training-Loss 1.93e+04, Validation-Loss 1.90e+04, Training-Accuracy 1.64e-01, Validation-Accuracy 1.62e-01\n",
      "Epoch 5210, Training-Loss 1.93e+04, Validation-Loss 1.92e+04, Training-Accuracy 1.64e-01, Validation-Accuracy 1.63e-01\n",
      "Epoch 5220, Training-Loss 1.93e+04, Validation-Loss 1.89e+04, Training-Accuracy 1.64e-01, Validation-Accuracy 1.61e-01\n",
      "Epoch 5230, Training-Loss 1.82e+04, Validation-Loss 1.83e+04, Training-Accuracy 1.59e-01, Validation-Accuracy 1.60e-01\n",
      "Epoch 5240, Training-Loss 1.80e+04, Validation-Loss 1.89e+04, Training-Accuracy 1.58e-01, Validation-Accuracy 1.61e-01\n",
      "Epoch 5250, Training-Loss 1.80e+04, Validation-Loss 1.82e+04, Training-Accuracy 1.58e-01, Validation-Accuracy 1.58e-01\n",
      "Epoch 5260, Training-Loss 1.77e+04, Validation-Loss 1.79e+04, Training-Accuracy 1.56e-01, Validation-Accuracy 1.57e-01\n",
      "Epoch 5270, Training-Loss 1.74e+04, Validation-Loss 1.75e+04, Training-Accuracy 1.55e-01, Validation-Accuracy 1.54e-01\n",
      "Epoch 5280, Training-Loss 1.74e+04, Validation-Loss 1.76e+04, Training-Accuracy 1.55e-01, Validation-Accuracy 1.55e-01\n",
      "Epoch 5290, Training-Loss 1.72e+04, Validation-Loss 1.75e+04, Training-Accuracy 1.53e-01, Validation-Accuracy 1.55e-01\n",
      "Epoch 5300, Training-Loss 1.74e+04, Validation-Loss 1.77e+04, Training-Accuracy 1.54e-01, Validation-Accuracy 1.55e-01\n",
      "Epoch 5310, Training-Loss 1.75e+04, Validation-Loss 1.72e+04, Training-Accuracy 1.53e-01, Validation-Accuracy 1.53e-01\n",
      "Epoch 5320, Training-Loss 1.69e+04, Validation-Loss 1.66e+04, Training-Accuracy 1.52e-01, Validation-Accuracy 1.51e-01\n",
      "Epoch 5330, Training-Loss 1.65e+04, Validation-Loss 1.59e+04, Training-Accuracy 1.50e-01, Validation-Accuracy 1.49e-01\n",
      "Epoch 5340, Training-Loss 1.63e+04, Validation-Loss 1.64e+04, Training-Accuracy 1.48e-01, Validation-Accuracy 1.49e-01\n",
      "Epoch 5350, Training-Loss 1.62e+04, Validation-Loss 1.60e+04, Training-Accuracy 1.49e-01, Validation-Accuracy 1.47e-01\n",
      "Epoch 5360, Training-Loss 1.63e+04, Validation-Loss 1.64e+04, Training-Accuracy 1.49e-01, Validation-Accuracy 1.49e-01\n",
      "Epoch 5370, Training-Loss 1.62e+04, Validation-Loss 1.57e+04, Training-Accuracy 1.48e-01, Validation-Accuracy 1.46e-01\n",
      "Epoch 5380, Training-Loss 1.60e+04, Validation-Loss 1.53e+04, Training-Accuracy 1.47e-01, Validation-Accuracy 1.44e-01\n",
      "Epoch 5390, Training-Loss 1.53e+04, Validation-Loss 1.55e+04, Training-Accuracy 1.44e-01, Validation-Accuracy 1.44e-01\n",
      "Epoch 5400, Training-Loss 1.47e+04, Validation-Loss 1.49e+04, Training-Accuracy 1.42e-01, Validation-Accuracy 1.42e-01\n",
      "Epoch 5410, Training-Loss 1.51e+04, Validation-Loss 1.43e+04, Training-Accuracy 1.42e-01, Validation-Accuracy 1.39e-01\n",
      "Epoch 5420, Training-Loss 1.44e+04, Validation-Loss 1.50e+04, Training-Accuracy 1.40e-01, Validation-Accuracy 1.41e-01\n",
      "Epoch 5430, Training-Loss 1.46e+04, Validation-Loss 1.51e+04, Training-Accuracy 1.39e-01, Validation-Accuracy 1.42e-01\n",
      "Epoch 5440, Training-Loss 1.53e+04, Validation-Loss 1.43e+04, Training-Accuracy 1.43e-01, Validation-Accuracy 1.39e-01\n",
      "Epoch 5450, Training-Loss 1.44e+04, Validation-Loss 1.44e+04, Training-Accuracy 1.39e-01, Validation-Accuracy 1.39e-01\n",
      "Epoch 5460, Training-Loss 1.41e+04, Validation-Loss 1.40e+04, Training-Accuracy 1.37e-01, Validation-Accuracy 1.37e-01\n",
      "Epoch 5470, Training-Loss 1.42e+04, Validation-Loss 1.44e+04, Training-Accuracy 1.38e-01, Validation-Accuracy 1.39e-01\n",
      "Epoch 5480, Training-Loss 1.39e+04, Validation-Loss 1.45e+04, Training-Accuracy 1.36e-01, Validation-Accuracy 1.38e-01\n",
      "Epoch 5490, Training-Loss 1.35e+04, Validation-Loss 1.31e+04, Training-Accuracy 1.34e-01, Validation-Accuracy 1.32e-01\n",
      "Epoch 5500, Training-Loss 1.36e+04, Validation-Loss 1.32e+04, Training-Accuracy 1.34e-01, Validation-Accuracy 1.32e-01\n",
      "Epoch 5510, Training-Loss 1.28e+04, Validation-Loss 1.29e+04, Training-Accuracy 1.30e-01, Validation-Accuracy 1.31e-01\n",
      "Epoch 5520, Training-Loss 1.35e+04, Validation-Loss 1.25e+04, Training-Accuracy 1.33e-01, Validation-Accuracy 1.29e-01\n",
      "Epoch 5530, Training-Loss 1.32e+04, Validation-Loss 1.32e+04, Training-Accuracy 1.31e-01, Validation-Accuracy 1.32e-01\n",
      "Epoch 5540, Training-Loss 1.34e+04, Validation-Loss 1.24e+04, Training-Accuracy 1.32e-01, Validation-Accuracy 1.28e-01\n",
      "Epoch 5550, Training-Loss 1.26e+04, Validation-Loss 1.29e+04, Training-Accuracy 1.29e-01, Validation-Accuracy 1.30e-01\n",
      "Epoch 5560, Training-Loss 1.28e+04, Validation-Loss 1.19e+04, Training-Accuracy 1.28e-01, Validation-Accuracy 1.25e-01\n",
      "Epoch 5570, Training-Loss 1.21e+04, Validation-Loss 1.23e+04, Training-Accuracy 1.26e-01, Validation-Accuracy 1.28e-01\n",
      "Epoch 5580, Training-Loss 1.18e+04, Validation-Loss 1.18e+04, Training-Accuracy 1.24e-01, Validation-Accuracy 1.25e-01\n",
      "Epoch 5590, Training-Loss 1.17e+04, Validation-Loss 1.19e+04, Training-Accuracy 1.24e-01, Validation-Accuracy 1.25e-01\n",
      "Epoch 5600, Training-Loss 1.18e+04, Validation-Loss 1.24e+04, Training-Accuracy 1.24e-01, Validation-Accuracy 1.27e-01\n",
      "Epoch 5610, Training-Loss 1.16e+04, Validation-Loss 1.12e+04, Training-Accuracy 1.23e-01, Validation-Accuracy 1.21e-01\n",
      "Epoch 5620, Training-Loss 1.15e+04, Validation-Loss 1.16e+04, Training-Accuracy 1.22e-01, Validation-Accuracy 1.22e-01\n",
      "Epoch 5630, Training-Loss 1.20e+04, Validation-Loss 1.17e+04, Training-Accuracy 1.24e-01, Validation-Accuracy 1.23e-01\n",
      "Epoch 5640, Training-Loss 1.17e+04, Validation-Loss 1.14e+04, Training-Accuracy 1.21e-01, Validation-Accuracy 1.21e-01\n",
      "Epoch 5650, Training-Loss 1.10e+04, Validation-Loss 1.13e+04, Training-Accuracy 1.20e-01, Validation-Accuracy 1.21e-01\n",
      "Epoch 5660, Training-Loss 1.06e+04, Validation-Loss 1.04e+04, Training-Accuracy 1.17e-01, Validation-Accuracy 1.15e-01\n",
      "Epoch 5670, Training-Loss 1.15e+04, Validation-Loss 1.10e+04, Training-Accuracy 1.20e-01, Validation-Accuracy 1.20e-01\n",
      "Epoch 5680, Training-Loss 1.07e+04, Validation-Loss 1.15e+04, Training-Accuracy 1.17e-01, Validation-Accuracy 1.21e-01\n",
      "Epoch 5690, Training-Loss 1.06e+04, Validation-Loss 1.05e+04, Training-Accuracy 1.16e-01, Validation-Accuracy 1.16e-01\n",
      "Epoch 5700, Training-Loss 1.05e+04, Validation-Loss 1.03e+04, Training-Accuracy 1.17e-01, Validation-Accuracy 1.15e-01\n",
      "Epoch 5710, Training-Loss 1.02e+04, Validation-Loss 1.03e+04, Training-Accuracy 1.14e-01, Validation-Accuracy 1.15e-01\n",
      "Epoch 5720, Training-Loss 9.23e+03, Validation-Loss 9.78e+03, Training-Accuracy 1.08e-01, Validation-Accuracy 1.12e-01\n",
      "Epoch 5730, Training-Loss 9.90e+03, Validation-Loss 1.06e+04, Training-Accuracy 1.12e-01, Validation-Accuracy 1.15e-01\n",
      "Epoch 5740, Training-Loss 1.02e+04, Validation-Loss 9.67e+03, Training-Accuracy 1.13e-01, Validation-Accuracy 1.11e-01\n",
      "Epoch 5750, Training-Loss 9.28e+03, Validation-Loss 9.38e+03, Training-Accuracy 1.09e-01, Validation-Accuracy 1.08e-01\n",
      "Epoch 5760, Training-Loss 1.01e+04, Validation-Loss 8.70e+03, Training-Accuracy 1.11e-01, Validation-Accuracy 1.05e-01\n",
      "Epoch 5770, Training-Loss 9.74e+03, Validation-Loss 9.32e+03, Training-Accuracy 1.10e-01, Validation-Accuracy 1.08e-01\n",
      "Epoch 5780, Training-Loss 8.83e+03, Validation-Loss 9.29e+03, Training-Accuracy 1.05e-01, Validation-Accuracy 1.07e-01\n",
      "Epoch 5790, Training-Loss 9.35e+03, Validation-Loss 8.99e+03, Training-Accuracy 1.08e-01, Validation-Accuracy 1.05e-01\n",
      "Epoch 5800, Training-Loss 9.22e+03, Validation-Loss 8.89e+03, Training-Accuracy 1.07e-01, Validation-Accuracy 1.05e-01\n",
      "Epoch 5810, Training-Loss 8.71e+03, Validation-Loss 8.81e+03, Training-Accuracy 1.04e-01, Validation-Accuracy 1.05e-01\n",
      "Epoch 5820, Training-Loss 8.84e+03, Validation-Loss 8.84e+03, Training-Accuracy 1.05e-01, Validation-Accuracy 1.05e-01\n",
      "Epoch 5830, Training-Loss 8.51e+03, Validation-Loss 9.21e+03, Training-Accuracy 1.02e-01, Validation-Accuracy 1.06e-01\n",
      "Epoch 5840, Training-Loss 8.15e+03, Validation-Loss 8.54e+03, Training-Accuracy 9.97e-02, Validation-Accuracy 1.02e-01\n",
      "Epoch 5850, Training-Loss 8.77e+03, Validation-Loss 8.56e+03, Training-Accuracy 1.03e-01, Validation-Accuracy 1.02e-01\n",
      "Epoch 5860, Training-Loss 8.15e+03, Validation-Loss 8.42e+03, Training-Accuracy 1.01e-01, Validation-Accuracy 1.01e-01\n",
      "Epoch 5870, Training-Loss 8.08e+03, Validation-Loss 7.82e+03, Training-Accuracy 9.87e-02, Validation-Accuracy 9.73e-02\n",
      "Epoch 5880, Training-Loss 7.66e+03, Validation-Loss 8.50e+03, Training-Accuracy 9.68e-02, Validation-Accuracy 1.00e-01\n",
      "Epoch 5890, Training-Loss 8.03e+03, Validation-Loss 7.81e+03, Training-Accuracy 9.79e-02, Validation-Accuracy 9.88e-02\n",
      "Epoch 5900, Training-Loss 7.99e+03, Validation-Loss 7.72e+03, Training-Accuracy 9.81e-02, Validation-Accuracy 9.63e-02\n",
      "Epoch 5910, Training-Loss 7.70e+03, Validation-Loss 7.47e+03, Training-Accuracy 9.63e-02, Validation-Accuracy 9.54e-02\n",
      "Epoch 5920, Training-Loss 7.46e+03, Validation-Loss 7.18e+03, Training-Accuracy 9.24e-02, Validation-Accuracy 9.37e-02\n",
      "Epoch 5930, Training-Loss 7.69e+03, Validation-Loss 7.49e+03, Training-Accuracy 9.60e-02, Validation-Accuracy 9.45e-02\n",
      "Epoch 5940, Training-Loss 7.49e+03, Validation-Loss 7.52e+03, Training-Accuracy 9.39e-02, Validation-Accuracy 9.47e-02\n",
      "Epoch 5950, Training-Loss 7.28e+03, Validation-Loss 7.53e+03, Training-Accuracy 9.40e-02, Validation-Accuracy 9.40e-02\n",
      "Epoch 5960, Training-Loss 7.21e+03, Validation-Loss 7.25e+03, Training-Accuracy 9.27e-02, Validation-Accuracy 9.09e-02\n",
      "Epoch 5970, Training-Loss 6.75e+03, Validation-Loss 6.81e+03, Training-Accuracy 8.98e-02, Validation-Accuracy 9.05e-02\n",
      "Epoch 5980, Training-Loss 6.62e+03, Validation-Loss 6.70e+03, Training-Accuracy 8.78e-02, Validation-Accuracy 8.84e-02\n",
      "Epoch 5990, Training-Loss 7.15e+03, Validation-Loss 6.99e+03, Training-Accuracy 9.11e-02, Validation-Accuracy 8.98e-02\n",
      "Epoch 6000, Training-Loss 6.54e+03, Validation-Loss 6.52e+03, Training-Accuracy 8.80e-02, Validation-Accuracy 8.68e-02\n",
      "Epoch 6010, Training-Loss 6.88e+03, Validation-Loss 6.66e+03, Training-Accuracy 8.92e-02, Validation-Accuracy 8.71e-02\n",
      "Epoch 6020, Training-Loss 6.81e+03, Validation-Loss 6.52e+03, Training-Accuracy 8.85e-02, Validation-Accuracy 8.71e-02\n",
      "Epoch 6030, Training-Loss 6.26e+03, Validation-Loss 6.74e+03, Training-Accuracy 8.62e-02, Validation-Accuracy 8.77e-02\n",
      "Epoch 6040, Training-Loss 6.19e+03, Validation-Loss 6.46e+03, Training-Accuracy 8.48e-02, Validation-Accuracy 8.60e-02\n",
      "Epoch 6050, Training-Loss 6.61e+03, Validation-Loss 6.75e+03, Training-Accuracy 8.66e-02, Validation-Accuracy 8.68e-02\n",
      "Epoch 6060, Training-Loss 6.40e+03, Validation-Loss 6.24e+03, Training-Accuracy 8.48e-02, Validation-Accuracy 8.48e-02\n",
      "Epoch 6070, Training-Loss 6.00e+03, Validation-Loss 6.21e+03, Training-Accuracy 8.32e-02, Validation-Accuracy 8.46e-02\n",
      "Epoch 6080, Training-Loss 6.25e+03, Validation-Loss 5.73e+03, Training-Accuracy 8.49e-02, Validation-Accuracy 8.09e-02\n",
      "Epoch 6090, Training-Loss 6.37e+03, Validation-Loss 6.08e+03, Training-Accuracy 8.48e-02, Validation-Accuracy 8.27e-02\n",
      "Epoch 6100, Training-Loss 5.60e+03, Validation-Loss 5.46e+03, Training-Accuracy 7.98e-02, Validation-Accuracy 7.87e-02\n",
      "Epoch 6110, Training-Loss 6.41e+03, Validation-Loss 5.79e+03, Training-Accuracy 8.53e-02, Validation-Accuracy 7.96e-02\n",
      "Epoch 6120, Training-Loss 5.88e+03, Validation-Loss 5.97e+03, Training-Accuracy 8.07e-02, Validation-Accuracy 8.09e-02\n",
      "Epoch 6130, Training-Loss 5.79e+03, Validation-Loss 5.87e+03, Training-Accuracy 8.06e-02, Validation-Accuracy 7.97e-02\n",
      "Epoch 6140, Training-Loss 5.90e+03, Validation-Loss 5.66e+03, Training-Accuracy 8.02e-02, Validation-Accuracy 7.86e-02\n",
      "Epoch 6150, Training-Loss 5.67e+03, Validation-Loss 5.33e+03, Training-Accuracy 7.92e-02, Validation-Accuracy 7.73e-02\n",
      "Epoch 6160, Training-Loss 5.44e+03, Validation-Loss 5.30e+03, Training-Accuracy 7.79e-02, Validation-Accuracy 7.67e-02\n",
      "Epoch 6170, Training-Loss 5.07e+03, Validation-Loss 4.90e+03, Training-Accuracy 7.51e-02, Validation-Accuracy 7.34e-02\n",
      "Epoch 6180, Training-Loss 5.36e+03, Validation-Loss 5.62e+03, Training-Accuracy 7.74e-02, Validation-Accuracy 7.89e-02\n",
      "Epoch 6190, Training-Loss 5.24e+03, Validation-Loss 5.48e+03, Training-Accuracy 7.63e-02, Validation-Accuracy 7.79e-02\n",
      "Epoch 6200, Training-Loss 5.39e+03, Validation-Loss 5.14e+03, Training-Accuracy 7.71e-02, Validation-Accuracy 7.59e-02\n",
      "Epoch 6210, Training-Loss 5.02e+03, Validation-Loss 5.23e+03, Training-Accuracy 7.38e-02, Validation-Accuracy 7.56e-02\n",
      "Epoch 6220, Training-Loss 5.21e+03, Validation-Loss 5.20e+03, Training-Accuracy 7.46e-02, Validation-Accuracy 7.49e-02\n",
      "Epoch 6230, Training-Loss 5.20e+03, Validation-Loss 4.99e+03, Training-Accuracy 7.43e-02, Validation-Accuracy 7.32e-02\n",
      "Epoch 6240, Training-Loss 4.91e+03, Validation-Loss 4.88e+03, Training-Accuracy 7.32e-02, Validation-Accuracy 7.21e-02\n",
      "Epoch 6250, Training-Loss 4.79e+03, Validation-Loss 5.34e+03, Training-Accuracy 7.23e-02, Validation-Accuracy 7.57e-02\n",
      "Epoch 6260, Training-Loss 5.11e+03, Validation-Loss 4.55e+03, Training-Accuracy 7.43e-02, Validation-Accuracy 7.03e-02\n",
      "Epoch 6270, Training-Loss 4.65e+03, Validation-Loss 4.64e+03, Training-Accuracy 7.10e-02, Validation-Accuracy 7.05e-02\n",
      "Epoch 6280, Training-Loss 4.68e+03, Validation-Loss 4.62e+03, Training-Accuracy 7.11e-02, Validation-Accuracy 7.07e-02\n",
      "Epoch 6290, Training-Loss 4.42e+03, Validation-Loss 4.38e+03, Training-Accuracy 6.95e-02, Validation-Accuracy 6.78e-02\n",
      "Epoch 6300, Training-Loss 4.47e+03, Validation-Loss 4.47e+03, Training-Accuracy 6.87e-02, Validation-Accuracy 6.89e-02\n",
      "Epoch 6310, Training-Loss 4.51e+03, Validation-Loss 4.58e+03, Training-Accuracy 6.84e-02, Validation-Accuracy 6.95e-02\n",
      "Epoch 6320, Training-Loss 4.15e+03, Validation-Loss 4.42e+03, Training-Accuracy 6.54e-02, Validation-Accuracy 6.69e-02\n",
      "Epoch 6330, Training-Loss 4.43e+03, Validation-Loss 4.62e+03, Training-Accuracy 6.72e-02, Validation-Accuracy 6.91e-02\n",
      "Epoch 6340, Training-Loss 4.60e+03, Validation-Loss 4.45e+03, Training-Accuracy 6.95e-02, Validation-Accuracy 6.71e-02\n",
      "Epoch 6350, Training-Loss 4.07e+03, Validation-Loss 3.88e+03, Training-Accuracy 6.51e-02, Validation-Accuracy 6.40e-02\n",
      "Epoch 6360, Training-Loss 4.21e+03, Validation-Loss 4.20e+03, Training-Accuracy 6.66e-02, Validation-Accuracy 6.62e-02\n",
      "Epoch 6370, Training-Loss 4.27e+03, Validation-Loss 4.12e+03, Training-Accuracy 6.66e-02, Validation-Accuracy 6.59e-02\n",
      "Epoch 6380, Training-Loss 3.97e+03, Validation-Loss 4.03e+03, Training-Accuracy 6.33e-02, Validation-Accuracy 6.38e-02\n",
      "Epoch 6390, Training-Loss 4.08e+03, Validation-Loss 3.97e+03, Training-Accuracy 6.42e-02, Validation-Accuracy 6.38e-02\n",
      "Epoch 6400, Training-Loss 4.17e+03, Validation-Loss 3.82e+03, Training-Accuracy 6.48e-02, Validation-Accuracy 6.32e-02\n",
      "Epoch 6410, Training-Loss 3.61e+03, Validation-Loss 3.57e+03, Training-Accuracy 6.08e-02, Validation-Accuracy 6.03e-02\n",
      "Epoch 6420, Training-Loss 3.45e+03, Validation-Loss 4.16e+03, Training-Accuracy 5.87e-02, Validation-Accuracy 6.43e-02\n",
      "Epoch 6430, Training-Loss 3.95e+03, Validation-Loss 4.06e+03, Training-Accuracy 6.25e-02, Validation-Accuracy 6.37e-02\n",
      "Epoch 6440, Training-Loss 3.68e+03, Validation-Loss 3.55e+03, Training-Accuracy 6.09e-02, Validation-Accuracy 5.99e-02\n",
      "Epoch 6450, Training-Loss 3.72e+03, Validation-Loss 3.43e+03, Training-Accuracy 6.02e-02, Validation-Accuracy 5.95e-02\n",
      "Epoch 6460, Training-Loss 3.51e+03, Validation-Loss 3.38e+03, Training-Accuracy 5.85e-02, Validation-Accuracy 5.77e-02\n",
      "Epoch 6470, Training-Loss 3.77e+03, Validation-Loss 3.46e+03, Training-Accuracy 6.05e-02, Validation-Accuracy 5.82e-02\n",
      "Epoch 6480, Training-Loss 3.51e+03, Validation-Loss 3.74e+03, Training-Accuracy 5.81e-02, Validation-Accuracy 6.06e-02\n",
      "Epoch 6490, Training-Loss 3.74e+03, Validation-Loss 3.27e+03, Training-Accuracy 6.07e-02, Validation-Accuracy 5.62e-02\n",
      "Epoch 6500, Training-Loss 3.64e+03, Validation-Loss 3.53e+03, Training-Accuracy 5.92e-02, Validation-Accuracy 5.82e-02\n",
      "Epoch 6510, Training-Loss 3.39e+03, Validation-Loss 3.30e+03, Training-Accuracy 5.81e-02, Validation-Accuracy 5.65e-02\n",
      "Epoch 6520, Training-Loss 3.68e+03, Validation-Loss 3.12e+03, Training-Accuracy 5.99e-02, Validation-Accuracy 5.45e-02\n",
      "Epoch 6530, Training-Loss 3.73e+03, Validation-Loss 3.35e+03, Training-Accuracy 5.95e-02, Validation-Accuracy 5.76e-02\n",
      "Epoch 6540, Training-Loss 3.28e+03, Validation-Loss 3.08e+03, Training-Accuracy 5.72e-02, Validation-Accuracy 5.53e-02\n",
      "Epoch 6550, Training-Loss 3.08e+03, Validation-Loss 3.29e+03, Training-Accuracy 5.53e-02, Validation-Accuracy 5.68e-02\n",
      "Epoch 6560, Training-Loss 3.56e+03, Validation-Loss 3.40e+03, Training-Accuracy 5.75e-02, Validation-Accuracy 5.77e-02\n",
      "Epoch 6570, Training-Loss 3.48e+03, Validation-Loss 2.84e+03, Training-Accuracy 5.69e-02, Validation-Accuracy 5.23e-02\n",
      "Epoch 6580, Training-Loss 2.97e+03, Validation-Loss 3.04e+03, Training-Accuracy 5.29e-02, Validation-Accuracy 5.26e-02\n",
      "Epoch 6590, Training-Loss 3.05e+03, Validation-Loss 3.13e+03, Training-Accuracy 5.38e-02, Validation-Accuracy 5.48e-02\n",
      "Epoch 6600, Training-Loss 3.15e+03, Validation-Loss 3.19e+03, Training-Accuracy 5.39e-02, Validation-Accuracy 5.39e-02\n",
      "Epoch 6610, Training-Loss 2.94e+03, Validation-Loss 3.03e+03, Training-Accuracy 5.20e-02, Validation-Accuracy 5.24e-02\n",
      "Epoch 6620, Training-Loss 2.91e+03, Validation-Loss 3.13e+03, Training-Accuracy 5.13e-02, Validation-Accuracy 5.42e-02\n",
      "Epoch 6630, Training-Loss 2.74e+03, Validation-Loss 3.00e+03, Training-Accuracy 5.03e-02, Validation-Accuracy 5.22e-02\n",
      "Epoch 6640, Training-Loss 2.90e+03, Validation-Loss 3.25e+03, Training-Accuracy 5.17e-02, Validation-Accuracy 5.50e-02\n",
      "Epoch 6650, Training-Loss 3.23e+03, Validation-Loss 3.27e+03, Training-Accuracy 5.46e-02, Validation-Accuracy 5.40e-02\n",
      "Epoch 6660, Training-Loss 2.48e+03, Validation-Loss 2.61e+03, Training-Accuracy 4.84e-02, Validation-Accuracy 4.86e-02\n",
      "Epoch 6670, Training-Loss 2.78e+03, Validation-Loss 2.75e+03, Training-Accuracy 5.08e-02, Validation-Accuracy 5.04e-02\n",
      "Epoch 6680, Training-Loss 3.10e+03, Validation-Loss 2.57e+03, Training-Accuracy 5.28e-02, Validation-Accuracy 4.87e-02\n",
      "Epoch 6690, Training-Loss 2.67e+03, Validation-Loss 2.60e+03, Training-Accuracy 4.97e-02, Validation-Accuracy 4.88e-02\n",
      "Epoch 6700, Training-Loss 3.06e+03, Validation-Loss 2.65e+03, Training-Accuracy 5.24e-02, Validation-Accuracy 4.93e-02\n",
      "Epoch 6710, Training-Loss 2.56e+03, Validation-Loss 2.85e+03, Training-Accuracy 4.85e-02, Validation-Accuracy 5.20e-02\n",
      "Epoch 6720, Training-Loss 2.72e+03, Validation-Loss 2.85e+03, Training-Accuracy 4.89e-02, Validation-Accuracy 5.01e-02\n",
      "Epoch 6730, Training-Loss 2.67e+03, Validation-Loss 2.39e+03, Training-Accuracy 4.89e-02, Validation-Accuracy 4.63e-02\n",
      "Epoch 6740, Training-Loss 2.38e+03, Validation-Loss 2.56e+03, Training-Accuracy 4.63e-02, Validation-Accuracy 4.73e-02\n",
      "Epoch 6750, Training-Loss 2.71e+03, Validation-Loss 2.40e+03, Training-Accuracy 4.91e-02, Validation-Accuracy 4.60e-02\n",
      "Epoch 6760, Training-Loss 2.29e+03, Validation-Loss 2.76e+03, Training-Accuracy 4.50e-02, Validation-Accuracy 4.92e-02\n",
      "Epoch 6770, Training-Loss 2.56e+03, Validation-Loss 2.33e+03, Training-Accuracy 4.80e-02, Validation-Accuracy 4.64e-02\n",
      "Epoch 6780, Training-Loss 2.36e+03, Validation-Loss 2.03e+03, Training-Accuracy 4.38e-02, Validation-Accuracy 4.27e-02\n",
      "Epoch 6790, Training-Loss 2.41e+03, Validation-Loss 2.47e+03, Training-Accuracy 4.58e-02, Validation-Accuracy 4.67e-02\n",
      "Epoch 6800, Training-Loss 2.36e+03, Validation-Loss 2.39e+03, Training-Accuracy 4.55e-02, Validation-Accuracy 4.53e-02\n",
      "Epoch 6810, Training-Loss 2.57e+03, Validation-Loss 2.27e+03, Training-Accuracy 4.82e-02, Validation-Accuracy 4.52e-02\n",
      "Epoch 6820, Training-Loss 2.35e+03, Validation-Loss 2.45e+03, Training-Accuracy 4.58e-02, Validation-Accuracy 4.64e-02\n",
      "Epoch 6830, Training-Loss 2.25e+03, Validation-Loss 2.38e+03, Training-Accuracy 4.42e-02, Validation-Accuracy 4.46e-02\n",
      "Epoch 6840, Training-Loss 2.20e+03, Validation-Loss 2.61e+03, Training-Accuracy 4.38e-02, Validation-Accuracy 4.77e-02\n",
      "Epoch 6850, Training-Loss 2.52e+03, Validation-Loss 2.53e+03, Training-Accuracy 4.68e-02, Validation-Accuracy 4.71e-02\n",
      "Epoch 6860, Training-Loss 2.13e+03, Validation-Loss 2.26e+03, Training-Accuracy 4.22e-02, Validation-Accuracy 4.42e-02\n",
      "Epoch 6870, Training-Loss 2.16e+03, Validation-Loss 2.26e+03, Training-Accuracy 4.28e-02, Validation-Accuracy 4.38e-02\n",
      "Epoch 6880, Training-Loss 2.33e+03, Validation-Loss 2.49e+03, Training-Accuracy 4.47e-02, Validation-Accuracy 4.56e-02\n",
      "Epoch 6890, Training-Loss 2.21e+03, Validation-Loss 2.18e+03, Training-Accuracy 4.33e-02, Validation-Accuracy 4.25e-02\n",
      "Epoch 6900, Training-Loss 2.19e+03, Validation-Loss 2.35e+03, Training-Accuracy 4.31e-02, Validation-Accuracy 4.43e-02\n",
      "Epoch 6910, Training-Loss 2.12e+03, Validation-Loss 2.00e+03, Training-Accuracy 4.32e-02, Validation-Accuracy 4.18e-02\n",
      "Epoch 6920, Training-Loss 1.90e+03, Validation-Loss 2.02e+03, Training-Accuracy 4.10e-02, Validation-Accuracy 4.17e-02\n",
      "Epoch 6930, Training-Loss 2.17e+03, Validation-Loss 2.17e+03, Training-Accuracy 4.24e-02, Validation-Accuracy 4.36e-02\n",
      "Epoch 6940, Training-Loss 1.85e+03, Validation-Loss 2.31e+03, Training-Accuracy 4.06e-02, Validation-Accuracy 4.44e-02\n",
      "Epoch 6950, Training-Loss 1.97e+03, Validation-Loss 2.14e+03, Training-Accuracy 4.15e-02, Validation-Accuracy 4.18e-02\n",
      "Epoch 6960, Training-Loss 2.07e+03, Validation-Loss 2.02e+03, Training-Accuracy 4.16e-02, Validation-Accuracy 4.17e-02\n",
      "Epoch 6970, Training-Loss 1.81e+03, Validation-Loss 2.07e+03, Training-Accuracy 3.96e-02, Validation-Accuracy 4.25e-02\n",
      "Epoch 6980, Training-Loss 1.96e+03, Validation-Loss 1.91e+03, Training-Accuracy 4.15e-02, Validation-Accuracy 4.02e-02\n",
      "Epoch 6990, Training-Loss 2.00e+03, Validation-Loss 2.11e+03, Training-Accuracy 4.07e-02, Validation-Accuracy 4.21e-02\n",
      "Epoch 7000, Training-Loss 2.12e+03, Validation-Loss 2.20e+03, Training-Accuracy 4.12e-02, Validation-Accuracy 4.31e-02\n",
      "Epoch 7010, Training-Loss 2.17e+03, Validation-Loss 2.03e+03, Training-Accuracy 4.23e-02, Validation-Accuracy 4.03e-02\n",
      "Epoch 7020, Training-Loss 2.10e+03, Validation-Loss 2.21e+03, Training-Accuracy 4.11e-02, Validation-Accuracy 4.41e-02\n",
      "Epoch 7030, Training-Loss 1.92e+03, Validation-Loss 1.98e+03, Training-Accuracy 4.06e-02, Validation-Accuracy 4.08e-02\n",
      "Epoch 7040, Training-Loss 2.11e+03, Validation-Loss 2.16e+03, Training-Accuracy 4.21e-02, Validation-Accuracy 4.30e-02\n",
      "Epoch 7050, Training-Loss 2.06e+03, Validation-Loss 1.91e+03, Training-Accuracy 4.21e-02, Validation-Accuracy 3.94e-02\n",
      "Epoch 7060, Training-Loss 1.78e+03, Validation-Loss 1.89e+03, Training-Accuracy 3.95e-02, Validation-Accuracy 4.00e-02\n",
      "Epoch 7070, Training-Loss 2.04e+03, Validation-Loss 1.86e+03, Training-Accuracy 4.08e-02, Validation-Accuracy 3.94e-02\n",
      "Epoch 7080, Training-Loss 1.98e+03, Validation-Loss 1.99e+03, Training-Accuracy 4.09e-02, Validation-Accuracy 4.17e-02\n",
      "Epoch 7090, Training-Loss 1.96e+03, Validation-Loss 1.79e+03, Training-Accuracy 4.06e-02, Validation-Accuracy 3.95e-02\n",
      "Epoch 7100, Training-Loss 1.76e+03, Validation-Loss 1.92e+03, Training-Accuracy 3.87e-02, Validation-Accuracy 4.07e-02\n",
      "Epoch 7110, Training-Loss 2.04e+03, Validation-Loss 1.82e+03, Training-Accuracy 4.17e-02, Validation-Accuracy 3.93e-02\n",
      "Epoch 7120, Training-Loss 1.87e+03, Validation-Loss 1.89e+03, Training-Accuracy 4.04e-02, Validation-Accuracy 4.01e-02\n",
      "Epoch 7130, Training-Loss 1.95e+03, Validation-Loss 1.83e+03, Training-Accuracy 3.93e-02, Validation-Accuracy 3.92e-02\n",
      "Epoch 7140, Training-Loss 1.90e+03, Validation-Loss 1.81e+03, Training-Accuracy 3.90e-02, Validation-Accuracy 3.93e-02\n",
      "Epoch 7150, Training-Loss 1.94e+03, Validation-Loss 1.60e+03, Training-Accuracy 4.05e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 7160, Training-Loss 2.06e+03, Validation-Loss 1.89e+03, Training-Accuracy 4.16e-02, Validation-Accuracy 4.01e-02\n",
      "Epoch 7170, Training-Loss 1.80e+03, Validation-Loss 1.72e+03, Training-Accuracy 4.04e-02, Validation-Accuracy 3.92e-02\n",
      "Epoch 7180, Training-Loss 1.91e+03, Validation-Loss 1.88e+03, Training-Accuracy 4.00e-02, Validation-Accuracy 3.93e-02\n",
      "Epoch 7190, Training-Loss 1.78e+03, Validation-Loss 1.70e+03, Training-Accuracy 3.85e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 7200, Training-Loss 1.68e+03, Validation-Loss 1.59e+03, Training-Accuracy 3.86e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 7210, Training-Loss 1.65e+03, Validation-Loss 1.88e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 4.00e-02\n",
      "Epoch 7220, Training-Loss 1.84e+03, Validation-Loss 1.96e+03, Training-Accuracy 3.88e-02, Validation-Accuracy 4.06e-02\n",
      "Epoch 7230, Training-Loss 1.78e+03, Validation-Loss 1.78e+03, Training-Accuracy 3.85e-02, Validation-Accuracy 3.97e-02\n",
      "Epoch 7240, Training-Loss 1.58e+03, Validation-Loss 1.96e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 4.10e-02\n",
      "Epoch 7250, Training-Loss 1.80e+03, Validation-Loss 1.61e+03, Training-Accuracy 3.94e-02, Validation-Accuracy 3.76e-02\n",
      "Epoch 7260, Training-Loss 1.78e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.87e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 7270, Training-Loss 1.66e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.54e-02\n",
      "Epoch 7280, Training-Loss 1.67e+03, Validation-Loss 1.62e+03, Training-Accuracy 3.87e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 7290, Training-Loss 1.90e+03, Validation-Loss 1.75e+03, Training-Accuracy 4.01e-02, Validation-Accuracy 3.82e-02\n",
      "Epoch 7300, Training-Loss 1.43e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.51e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 7310, Training-Loss 1.76e+03, Validation-Loss 1.66e+03, Training-Accuracy 3.84e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 7320, Training-Loss 1.58e+03, Validation-Loss 1.70e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.84e-02\n",
      "Epoch 7330, Training-Loss 1.47e+03, Validation-Loss 1.67e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.79e-02\n",
      "Epoch 7340, Training-Loss 1.79e+03, Validation-Loss 1.80e+03, Training-Accuracy 3.92e-02, Validation-Accuracy 3.95e-02\n",
      "Epoch 7350, Training-Loss 1.63e+03, Validation-Loss 1.64e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.79e-02\n",
      "Epoch 7360, Training-Loss 1.77e+03, Validation-Loss 1.62e+03, Training-Accuracy 3.87e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 7370, Training-Loss 1.67e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.85e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 7380, Training-Loss 1.53e+03, Validation-Loss 1.83e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.91e-02\n",
      "Epoch 7390, Training-Loss 1.66e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 7400, Training-Loss 1.75e+03, Validation-Loss 1.70e+03, Training-Accuracy 3.82e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 7410, Training-Loss 1.82e+03, Validation-Loss 1.50e+03, Training-Accuracy 4.00e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 7420, Training-Loss 1.70e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.69e-02\n",
      "Epoch 7430, Training-Loss 1.51e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 7440, Training-Loss 1.63e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 7450, Training-Loss 1.70e+03, Validation-Loss 1.66e+03, Training-Accuracy 3.90e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 7460, Training-Loss 1.70e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 7470, Training-Loss 1.71e+03, Validation-Loss 1.63e+03, Training-Accuracy 3.82e-02, Validation-Accuracy 3.83e-02\n",
      "Epoch 7480, Training-Loss 1.57e+03, Validation-Loss 1.58e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 7490, Training-Loss 1.68e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.84e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 7500, Training-Loss 1.83e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.91e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 7510, Training-Loss 1.63e+03, Validation-Loss 1.60e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 7520, Training-Loss 1.69e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.83e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 7530, Training-Loss 1.43e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.54e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 7540, Training-Loss 1.41e+03, Validation-Loss 1.67e+03, Training-Accuracy 3.49e-02, Validation-Accuracy 3.82e-02\n",
      "Epoch 7550, Training-Loss 1.76e+03, Validation-Loss 1.63e+03, Training-Accuracy 3.97e-02, Validation-Accuracy 3.82e-02\n",
      "Epoch 7560, Training-Loss 1.68e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.84e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 7570, Training-Loss 1.67e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 7580, Training-Loss 1.61e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 7590, Training-Loss 1.52e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 7600, Training-Loss 1.51e+03, Validation-Loss 1.36e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.44e-02\n",
      "Epoch 7610, Training-Loss 1.66e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.83e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 7620, Training-Loss 1.41e+03, Validation-Loss 1.60e+03, Training-Accuracy 3.50e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 7630, Training-Loss 1.70e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.82e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 7640, Training-Loss 1.59e+03, Validation-Loss 1.59e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.82e-02\n",
      "Epoch 7650, Training-Loss 1.57e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 7660, Training-Loss 1.42e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.57e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 7670, Training-Loss 1.41e+03, Validation-Loss 1.36e+03, Training-Accuracy 3.51e-02, Validation-Accuracy 3.54e-02\n",
      "Epoch 7680, Training-Loss 1.52e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 7690, Training-Loss 1.42e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 7700, Training-Loss 1.50e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 7710, Training-Loss 1.52e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 7720, Training-Loss 1.50e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 7730, Training-Loss 1.53e+03, Validation-Loss 1.67e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.81e-02\n",
      "Epoch 7740, Training-Loss 1.41e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.56e-02\n",
      "Epoch 7750, Training-Loss 1.61e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 7760, Training-Loss 1.45e+03, Validation-Loss 1.63e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.83e-02\n",
      "Epoch 7770, Training-Loss 1.54e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 7780, Training-Loss 1.65e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 7790, Training-Loss 1.63e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 7800, Training-Loss 1.51e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 7810, Training-Loss 1.51e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 7820, Training-Loss 1.66e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.87e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 7830, Training-Loss 1.40e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 7840, Training-Loss 1.46e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.53e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 7850, Training-Loss 1.43e+03, Validation-Loss 1.32e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.36e-02\n",
      "Epoch 7860, Training-Loss 1.63e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 7870, Training-Loss 1.56e+03, Validation-Loss 1.65e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.85e-02\n",
      "Epoch 7880, Training-Loss 1.54e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 7890, Training-Loss 1.40e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.57e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 7900, Training-Loss 1.46e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.69e-02\n",
      "Epoch 7910, Training-Loss 1.43e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 7920, Training-Loss 1.50e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 7930, Training-Loss 1.40e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 7940, Training-Loss 1.44e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 7950, Training-Loss 1.48e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 7960, Training-Loss 1.48e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 7970, Training-Loss 1.55e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 7980, Training-Loss 1.46e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 7990, Training-Loss 1.44e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 8000, Training-Loss 1.42e+03, Validation-Loss 1.31e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.40e-02\n",
      "Epoch 8010, Training-Loss 1.41e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.54e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 8020, Training-Loss 1.40e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 8030, Training-Loss 1.57e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 8040, Training-Loss 1.55e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 8050, Training-Loss 1.53e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 8060, Training-Loss 1.44e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 8070, Training-Loss 1.58e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 8080, Training-Loss 1.53e+03, Validation-Loss 1.30e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.47e-02\n",
      "Epoch 8090, Training-Loss 1.55e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 8100, Training-Loss 1.50e+03, Validation-Loss 1.60e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.81e-02\n",
      "Epoch 8110, Training-Loss 1.47e+03, Validation-Loss 1.36e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 8120, Training-Loss 1.38e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.56e-02\n",
      "Epoch 8130, Training-Loss 1.44e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.55e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 8140, Training-Loss 1.55e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.49e-02\n",
      "Epoch 8150, Training-Loss 1.37e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.48e-02, Validation-Accuracy 3.56e-02\n",
      "Epoch 8160, Training-Loss 1.54e+03, Validation-Loss 1.64e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.87e-02\n",
      "Epoch 8170, Training-Loss 1.43e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.56e-02\n",
      "Epoch 8180, Training-Loss 1.47e+03, Validation-Loss 1.30e+03, Training-Accuracy 3.81e-02, Validation-Accuracy 3.43e-02\n",
      "Epoch 8190, Training-Loss 1.49e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 8200, Training-Loss 1.53e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 8210, Training-Loss 1.41e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.53e-02\n",
      "Epoch 8220, Training-Loss 1.49e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 8230, Training-Loss 1.41e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 8240, Training-Loss 1.51e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 8250, Training-Loss 1.37e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.51e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 8260, Training-Loss 1.47e+03, Validation-Loss 1.59e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.86e-02\n",
      "Epoch 8270, Training-Loss 1.59e+03, Validation-Loss 1.31e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.34e-02\n",
      "Epoch 8280, Training-Loss 1.57e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 8290, Training-Loss 1.36e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 8300, Training-Loss 1.30e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.51e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 8310, Training-Loss 1.36e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 8320, Training-Loss 1.45e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 8330, Training-Loss 1.55e+03, Validation-Loss 1.32e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 8340, Training-Loss 1.42e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 8350, Training-Loss 1.39e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 8360, Training-Loss 1.46e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 8370, Training-Loss 1.42e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 8380, Training-Loss 1.42e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 8390, Training-Loss 1.55e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 8400, Training-Loss 1.59e+03, Validation-Loss 1.33e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.38e-02\n",
      "Epoch 8410, Training-Loss 1.41e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 8420, Training-Loss 1.50e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.79e-02\n",
      "Epoch 8430, Training-Loss 1.52e+03, Validation-Loss 1.59e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.84e-02\n",
      "Epoch 8440, Training-Loss 1.39e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 8450, Training-Loss 1.51e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.76e-02\n",
      "Epoch 8460, Training-Loss 1.41e+03, Validation-Loss 1.33e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.47e-02\n",
      "Epoch 8470, Training-Loss 1.48e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.87e-02\n",
      "Epoch 8480, Training-Loss 1.30e+03, Validation-Loss 1.30e+03, Training-Accuracy 3.45e-02, Validation-Accuracy 3.43e-02\n",
      "Epoch 8490, Training-Loss 1.47e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 8500, Training-Loss 1.48e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 8510, Training-Loss 1.45e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 8520, Training-Loss 1.44e+03, Validation-Loss 1.25e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.41e-02\n",
      "Epoch 8530, Training-Loss 1.39e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 8540, Training-Loss 1.41e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.57e-02, Validation-Accuracy 3.56e-02\n",
      "Epoch 8550, Training-Loss 1.55e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.85e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 8560, Training-Loss 1.45e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 8570, Training-Loss 1.58e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 8580, Training-Loss 1.57e+03, Validation-Loss 1.60e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.80e-02\n",
      "Epoch 8590, Training-Loss 1.35e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.53e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 8600, Training-Loss 1.52e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 8610, Training-Loss 1.35e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.54e-02\n",
      "Epoch 8620, Training-Loss 1.41e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 8630, Training-Loss 1.52e+03, Validation-Loss 1.33e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.49e-02\n",
      "Epoch 8640, Training-Loss 1.42e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 8650, Training-Loss 1.55e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.86e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 8660, Training-Loss 1.34e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.53e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 8670, Training-Loss 1.30e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.51e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 8680, Training-Loss 1.46e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.51e-02\n",
      "Epoch 8690, Training-Loss 1.45e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 8700, Training-Loss 1.49e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 8710, Training-Loss 1.43e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.79e-02\n",
      "Epoch 8720, Training-Loss 1.54e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 8730, Training-Loss 1.41e+03, Validation-Loss 1.65e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.88e-02\n",
      "Epoch 8740, Training-Loss 1.39e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.57e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 8750, Training-Loss 1.48e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 8760, Training-Loss 1.30e+03, Validation-Loss 1.58e+03, Training-Accuracy 3.51e-02, Validation-Accuracy 3.80e-02\n",
      "Epoch 8770, Training-Loss 1.47e+03, Validation-Loss 1.59e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.80e-02\n",
      "Epoch 8780, Training-Loss 1.46e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 8790, Training-Loss 1.51e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 8800, Training-Loss 1.47e+03, Validation-Loss 1.25e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.41e-02\n",
      "Epoch 8810, Training-Loss 1.38e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 8820, Training-Loss 1.54e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 8830, Training-Loss 1.51e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.83e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 8840, Training-Loss 1.46e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.76e-02\n",
      "Epoch 8850, Training-Loss 1.36e+03, Validation-Loss 1.63e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.90e-02\n",
      "Epoch 8860, Training-Loss 1.49e+03, Validation-Loss 1.61e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.83e-02\n",
      "Epoch 8870, Training-Loss 1.47e+03, Validation-Loss 1.30e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.52e-02\n",
      "Epoch 8880, Training-Loss 1.48e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 8890, Training-Loss 1.60e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.83e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 8900, Training-Loss 1.33e+03, Validation-Loss 1.66e+03, Training-Accuracy 3.43e-02, Validation-Accuracy 4.02e-02\n",
      "Epoch 8910, Training-Loss 1.50e+03, Validation-Loss 1.58e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.80e-02\n",
      "Epoch 8920, Training-Loss 1.52e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 8930, Training-Loss 1.45e+03, Validation-Loss 1.32e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.49e-02\n",
      "Epoch 8940, Training-Loss 1.45e+03, Validation-Loss 1.56e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.85e-02\n",
      "Epoch 8950, Training-Loss 1.42e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.69e-02\n",
      "Epoch 8960, Training-Loss 1.58e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.85e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 8970, Training-Loss 1.45e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 8980, Training-Loss 1.49e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 8990, Training-Loss 1.43e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 9000, Training-Loss 1.38e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 9010, Training-Loss 1.48e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 9020, Training-Loss 1.44e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 9030, Training-Loss 1.34e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.54e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 9040, Training-Loss 1.49e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.52e-02\n",
      "Epoch 9050, Training-Loss 1.55e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 9060, Training-Loss 1.40e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.54e-02, Validation-Accuracy 3.85e-02\n",
      "Epoch 9070, Training-Loss 1.34e+03, Validation-Loss 1.31e+03, Training-Accuracy 3.54e-02, Validation-Accuracy 3.50e-02\n",
      "Epoch 9080, Training-Loss 1.39e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 9090, Training-Loss 1.41e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 9100, Training-Loss 1.63e+03, Validation-Loss 1.56e+03, Training-Accuracy 3.83e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 9110, Training-Loss 1.53e+03, Validation-Loss 1.58e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 9120, Training-Loss 1.53e+03, Validation-Loss 1.56e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.79e-02\n",
      "Epoch 9130, Training-Loss 1.58e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.89e-02, Validation-Accuracy 3.69e-02\n",
      "Epoch 9140, Training-Loss 1.52e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 9150, Training-Loss 1.56e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 9160, Training-Loss 1.47e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 9170, Training-Loss 1.20e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.42e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 9180, Training-Loss 1.43e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.79e-02\n",
      "Epoch 9190, Training-Loss 1.35e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 9200, Training-Loss 1.61e+03, Validation-Loss 1.32e+03, Training-Accuracy 3.81e-02, Validation-Accuracy 3.46e-02\n",
      "Epoch 9210, Training-Loss 1.47e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 9220, Training-Loss 1.44e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.69e-02\n",
      "Epoch 9230, Training-Loss 1.35e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 9240, Training-Loss 1.49e+03, Validation-Loss 1.58e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.83e-02\n",
      "Epoch 9250, Training-Loss 1.49e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.48e-02\n",
      "Epoch 9260, Training-Loss 1.44e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 9270, Training-Loss 1.51e+03, Validation-Loss 1.59e+03, Training-Accuracy 3.81e-02, Validation-Accuracy 3.81e-02\n",
      "Epoch 9280, Training-Loss 1.34e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.55e-02, Validation-Accuracy 3.84e-02\n",
      "Epoch 9290, Training-Loss 1.44e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 9300, Training-Loss 1.30e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.49e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 9310, Training-Loss 1.37e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.76e-02\n",
      "Epoch 9320, Training-Loss 1.42e+03, Validation-Loss 1.68e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.97e-02\n",
      "Epoch 9330, Training-Loss 1.24e+03, Validation-Loss 1.66e+03, Training-Accuracy 3.34e-02, Validation-Accuracy 3.90e-02\n",
      "Epoch 9340, Training-Loss 1.47e+03, Validation-Loss 1.28e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.49e-02\n",
      "Epoch 9350, Training-Loss 1.39e+03, Validation-Loss 1.26e+03, Training-Accuracy 3.52e-02, Validation-Accuracy 3.41e-02\n",
      "Epoch 9360, Training-Loss 1.46e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 9370, Training-Loss 1.30e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.49e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 9380, Training-Loss 1.53e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.49e-02\n",
      "Epoch 9390, Training-Loss 1.56e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 9400, Training-Loss 1.53e+03, Validation-Loss 1.65e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.86e-02\n",
      "Epoch 9410, Training-Loss 1.59e+03, Validation-Loss 1.33e+03, Training-Accuracy 3.86e-02, Validation-Accuracy 3.52e-02\n",
      "Epoch 9420, Training-Loss 1.41e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 9430, Training-Loss 1.41e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 9440, Training-Loss 1.38e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 9450, Training-Loss 1.44e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.76e-02\n",
      "Epoch 9460, Training-Loss 1.48e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 9470, Training-Loss 1.45e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 9480, Training-Loss 1.52e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 9490, Training-Loss 1.46e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 9500, Training-Loss 1.48e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 9510, Training-Loss 1.50e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.84e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 9520, Training-Loss 1.46e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 9530, Training-Loss 1.31e+03, Validation-Loss 1.56e+03, Training-Accuracy 3.46e-02, Validation-Accuracy 3.87e-02\n",
      "Epoch 9540, Training-Loss 1.53e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 9550, Training-Loss 1.43e+03, Validation-Loss 1.33e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.47e-02\n",
      "Epoch 9560, Training-Loss 1.57e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 9570, Training-Loss 1.50e+03, Validation-Loss 1.29e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.48e-02\n",
      "Epoch 9580, Training-Loss 1.51e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 9590, Training-Loss 1.62e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.86e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 9600, Training-Loss 1.62e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 9610, Training-Loss 1.55e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 9620, Training-Loss 1.35e+03, Validation-Loss 1.64e+03, Training-Accuracy 3.55e-02, Validation-Accuracy 3.97e-02\n",
      "Epoch 9630, Training-Loss 1.57e+03, Validation-Loss 1.30e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.46e-02\n",
      "Epoch 9640, Training-Loss 1.40e+03, Validation-Loss 1.70e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.99e-02\n",
      "Epoch 9650, Training-Loss 1.56e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.82e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 9660, Training-Loss 1.36e+03, Validation-Loss 1.33e+03, Training-Accuracy 3.50e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 9670, Training-Loss 1.49e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 9680, Training-Loss 1.45e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.76e-02\n",
      "Epoch 9690, Training-Loss 1.58e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.88e-02, Validation-Accuracy 3.85e-02\n",
      "Epoch 9700, Training-Loss 1.44e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 9710, Training-Loss 1.47e+03, Validation-Loss 1.36e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 9720, Training-Loss 1.59e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.89e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 9730, Training-Loss 1.54e+03, Validation-Loss 1.31e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.51e-02\n",
      "Epoch 9740, Training-Loss 1.51e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.80e-02\n",
      "Epoch 9750, Training-Loss 1.53e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 9760, Training-Loss 1.39e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.76e-02\n",
      "Epoch 9770, Training-Loss 1.36e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 9780, Training-Loss 1.41e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 9790, Training-Loss 1.31e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.46e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 9800, Training-Loss 1.50e+03, Validation-Loss 1.30e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.49e-02\n",
      "Epoch 9810, Training-Loss 1.49e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.51e-02\n",
      "Epoch 9820, Training-Loss 1.45e+03, Validation-Loss 1.58e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.84e-02\n",
      "Epoch 9830, Training-Loss 1.44e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 9840, Training-Loss 1.57e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 9850, Training-Loss 1.26e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.47e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 9860, Training-Loss 1.52e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 9870, Training-Loss 1.29e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.46e-02, Validation-Accuracy 3.89e-02\n",
      "Epoch 9880, Training-Loss 1.35e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.50e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 9890, Training-Loss 1.55e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 9900, Training-Loss 1.51e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 9910, Training-Loss 1.41e+03, Validation-Loss 1.59e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 9920, Training-Loss 1.41e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.56e-02\n",
      "Epoch 9930, Training-Loss 1.42e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.56e-02\n",
      "Epoch 9940, Training-Loss 1.44e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 9950, Training-Loss 1.27e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.41e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 9960, Training-Loss 1.55e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.81e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 9970, Training-Loss 1.43e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 9980, Training-Loss 1.32e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.51e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 9990, Training-Loss 1.43e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.57e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 10000, Training-Loss 1.51e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 10010, Training-Loss 1.43e+03, Validation-Loss 1.67e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.94e-02\n",
      "Epoch 10020, Training-Loss 1.47e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 10030, Training-Loss 1.49e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.85e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 10040, Training-Loss 1.50e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 10050, Training-Loss 1.51e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 10060, Training-Loss 1.59e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.89e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 10070, Training-Loss 1.39e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.55e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 10080, Training-Loss 1.53e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 10090, Training-Loss 1.49e+03, Validation-Loss 1.64e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.96e-02\n",
      "Epoch 10100, Training-Loss 1.44e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 10110, Training-Loss 1.66e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.96e-02, Validation-Accuracy 3.80e-02\n",
      "Epoch 10120, Training-Loss 1.34e+03, Validation-Loss 1.62e+03, Training-Accuracy 3.50e-02, Validation-Accuracy 3.94e-02\n",
      "Epoch 10130, Training-Loss 1.42e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 10140, Training-Loss 1.59e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.89e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 10150, Training-Loss 1.48e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 10160, Training-Loss 1.38e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 10170, Training-Loss 1.50e+03, Validation-Loss 1.59e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.81e-02\n",
      "Epoch 10180, Training-Loss 1.58e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.84e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 10190, Training-Loss 1.41e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.57e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 10200, Training-Loss 1.50e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 10210, Training-Loss 1.57e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 10220, Training-Loss 1.40e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 10230, Training-Loss 1.52e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.82e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 10240, Training-Loss 1.61e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.84e-02, Validation-Accuracy 3.79e-02\n",
      "Epoch 10250, Training-Loss 1.35e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.46e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 10260, Training-Loss 1.51e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 10270, Training-Loss 1.50e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.69e-02\n",
      "Epoch 10280, Training-Loss 1.35e+03, Validation-Loss 1.60e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.84e-02\n",
      "Epoch 10290, Training-Loss 1.52e+03, Validation-Loss 1.30e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.51e-02\n",
      "Epoch 10300, Training-Loss 1.53e+03, Validation-Loss 1.65e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.88e-02\n",
      "Epoch 10310, Training-Loss 1.51e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.82e-02\n",
      "Epoch 10320, Training-Loss 1.44e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 10330, Training-Loss 1.60e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 10340, Training-Loss 1.47e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 10350, Training-Loss 1.35e+03, Validation-Loss 1.56e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.80e-02\n",
      "Epoch 10360, Training-Loss 1.58e+03, Validation-Loss 1.29e+03, Training-Accuracy 3.89e-02, Validation-Accuracy 3.48e-02\n",
      "Epoch 10370, Training-Loss 1.59e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.84e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 10380, Training-Loss 1.30e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.49e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 10390, Training-Loss 1.56e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.84e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 10400, Training-Loss 1.46e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 10410, Training-Loss 1.47e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 10420, Training-Loss 1.52e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 10430, Training-Loss 1.47e+03, Validation-Loss 1.58e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.83e-02\n",
      "Epoch 10440, Training-Loss 1.50e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 10450, Training-Loss 1.53e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 10460, Training-Loss 1.40e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.83e-02\n",
      "Epoch 10470, Training-Loss 1.54e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.81e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 10480, Training-Loss 1.49e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 10490, Training-Loss 1.54e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.81e-02, Validation-Accuracy 3.90e-02\n",
      "Epoch 10500, Training-Loss 1.51e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 10510, Training-Loss 1.47e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.53e-02\n",
      "Epoch 10520, Training-Loss 1.55e+03, Validation-Loss 1.28e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.38e-02\n",
      "Epoch 10530, Training-Loss 1.45e+03, Validation-Loss 1.60e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.84e-02\n",
      "Epoch 10540, Training-Loss 1.53e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 10550, Training-Loss 1.50e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 10560, Training-Loss 1.44e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 10570, Training-Loss 1.39e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.84e-02\n",
      "Epoch 10580, Training-Loss 1.45e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 10590, Training-Loss 1.54e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.81e-02, Validation-Accuracy 3.80e-02\n",
      "Epoch 10600, Training-Loss 1.52e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.82e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 10610, Training-Loss 1.60e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.90e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 10620, Training-Loss 1.31e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.47e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 10630, Training-Loss 1.42e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.57e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 10640, Training-Loss 1.49e+03, Validation-Loss 1.32e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.56e-02\n",
      "Epoch 10650, Training-Loss 1.46e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.66e-02\n",
      "Epoch 10660, Training-Loss 1.61e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.91e-02, Validation-Accuracy 3.76e-02\n",
      "Epoch 10670, Training-Loss 1.54e+03, Validation-Loss 1.56e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 10680, Training-Loss 1.34e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.50e-02, Validation-Accuracy 3.69e-02\n",
      "Epoch 10690, Training-Loss 1.46e+03, Validation-Loss 1.32e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.49e-02\n",
      "Epoch 10700, Training-Loss 1.69e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.95e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 10710, Training-Loss 1.32e+03, Validation-Loss 1.56e+03, Training-Accuracy 3.55e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 10720, Training-Loss 1.51e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.80e-02\n",
      "Epoch 10730, Training-Loss 1.51e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 10740, Training-Loss 1.59e+03, Validation-Loss 1.60e+03, Training-Accuracy 3.86e-02, Validation-Accuracy 3.88e-02\n",
      "Epoch 10750, Training-Loss 1.38e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 10760, Training-Loss 1.40e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 10770, Training-Loss 1.53e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.54e-02\n",
      "Epoch 10780, Training-Loss 1.39e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 10790, Training-Loss 1.47e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 10800, Training-Loss 1.41e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.52e-02\n",
      "Epoch 10810, Training-Loss 1.46e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 10820, Training-Loss 1.49e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.81e-02, Validation-Accuracy 3.51e-02\n",
      "Epoch 10830, Training-Loss 1.48e+03, Validation-Loss 1.31e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.47e-02\n",
      "Epoch 10840, Training-Loss 1.42e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 10850, Training-Loss 1.52e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 10860, Training-Loss 1.43e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.51e-02\n",
      "Epoch 10870, Training-Loss 1.45e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 10880, Training-Loss 1.45e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 10890, Training-Loss 1.49e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 10900, Training-Loss 1.48e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 10910, Training-Loss 1.38e+03, Validation-Loss 1.64e+03, Training-Accuracy 3.54e-02, Validation-Accuracy 3.88e-02\n",
      "Epoch 10920, Training-Loss 1.63e+03, Validation-Loss 1.69e+03, Training-Accuracy 3.84e-02, Validation-Accuracy 3.93e-02\n",
      "Epoch 10930, Training-Loss 1.59e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 10940, Training-Loss 1.52e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 10950, Training-Loss 1.46e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.54e-02\n",
      "Epoch 10960, Training-Loss 1.70e+03, Validation-Loss 1.52e+03, Training-Accuracy 4.08e-02, Validation-Accuracy 3.83e-02\n",
      "Epoch 10970, Training-Loss 1.40e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.53e-02\n",
      "Epoch 10980, Training-Loss 1.29e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.52e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 10990, Training-Loss 1.56e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 11000, Training-Loss 1.64e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.92e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 11010, Training-Loss 1.43e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.54e-02\n",
      "Epoch 11020, Training-Loss 1.46e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 11030, Training-Loss 1.52e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 11040, Training-Loss 1.52e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.83e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 11050, Training-Loss 1.36e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.52e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 11060, Training-Loss 1.57e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 11070, Training-Loss 1.44e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 11080, Training-Loss 1.51e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 11090, Training-Loss 1.57e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.85e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 11100, Training-Loss 1.57e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.85e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 11110, Training-Loss 1.64e+03, Validation-Loss 1.36e+03, Training-Accuracy 3.92e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 11120, Training-Loss 1.49e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.52e-02\n",
      "Epoch 11130, Training-Loss 1.52e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 11140, Training-Loss 1.35e+03, Validation-Loss 1.33e+03, Training-Accuracy 3.46e-02, Validation-Accuracy 3.53e-02\n",
      "Epoch 11150, Training-Loss 1.27e+03, Validation-Loss 1.29e+03, Training-Accuracy 3.48e-02, Validation-Accuracy 3.46e-02\n",
      "Epoch 11160, Training-Loss 1.54e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.85e-02, Validation-Accuracy 3.46e-02\n",
      "Epoch 11170, Training-Loss 1.43e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 11180, Training-Loss 1.42e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 11190, Training-Loss 1.45e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.88e-02\n",
      "Epoch 11200, Training-Loss 1.44e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 11210, Training-Loss 1.43e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 11220, Training-Loss 1.56e+03, Validation-Loss 1.66e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.95e-02\n",
      "Epoch 11230, Training-Loss 1.51e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 11240, Training-Loss 1.46e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.69e-02\n",
      "Epoch 11250, Training-Loss 1.47e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 11260, Training-Loss 1.53e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.69e-02\n",
      "Epoch 11270, Training-Loss 1.39e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 11280, Training-Loss 1.52e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.81e-02\n",
      "Epoch 11290, Training-Loss 1.38e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.81e-02\n",
      "Epoch 11300, Training-Loss 1.48e+03, Validation-Loss 1.60e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.88e-02\n",
      "Epoch 11310, Training-Loss 1.50e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.49e-02\n",
      "Epoch 11320, Training-Loss 1.64e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.90e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 11330, Training-Loss 1.55e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.83e-02, Validation-Accuracy 3.54e-02\n",
      "Epoch 11340, Training-Loss 1.48e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 11350, Training-Loss 1.51e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.69e-02\n",
      "Epoch 11360, Training-Loss 1.41e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 11370, Training-Loss 1.39e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 11380, Training-Loss 1.36e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 11390, Training-Loss 1.37e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.55e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 11400, Training-Loss 1.49e+03, Validation-Loss 1.27e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.52e-02\n",
      "Epoch 11410, Training-Loss 1.47e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 11420, Training-Loss 1.35e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.59e-02\n",
      "Epoch 11430, Training-Loss 1.51e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.51e-02\n",
      "Epoch 11440, Training-Loss 1.43e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 11450, Training-Loss 1.44e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 11460, Training-Loss 1.51e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 11470, Training-Loss 1.62e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.82e-02, Validation-Accuracy 3.51e-02\n",
      "Epoch 11480, Training-Loss 1.53e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 11490, Training-Loss 1.44e+03, Validation-Loss 1.63e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 4.01e-02\n",
      "Epoch 11500, Training-Loss 1.52e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 11510, Training-Loss 1.42e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 11520, Training-Loss 1.39e+03, Validation-Loss 1.22e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.44e-02\n",
      "Epoch 11530, Training-Loss 1.46e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 11540, Training-Loss 1.40e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.57e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 11550, Training-Loss 1.53e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 11560, Training-Loss 1.48e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 11570, Training-Loss 1.41e+03, Validation-Loss 1.31e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 11580, Training-Loss 1.44e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.72e-02\n",
      "Epoch 11590, Training-Loss 1.43e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 11600, Training-Loss 1.43e+03, Validation-Loss 1.36e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 11610, Training-Loss 1.34e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.53e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 11620, Training-Loss 1.43e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 11630, Training-Loss 1.61e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.96e-02, Validation-Accuracy 3.53e-02\n",
      "Epoch 11640, Training-Loss 1.42e+03, Validation-Loss 1.31e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 11650, Training-Loss 1.31e+03, Validation-Loss 1.36e+03, Training-Accuracy 3.47e-02, Validation-Accuracy 3.53e-02\n",
      "Epoch 11660, Training-Loss 1.55e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 11670, Training-Loss 1.41e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 11680, Training-Loss 1.35e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.52e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 11690, Training-Loss 1.41e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 11700, Training-Loss 1.52e+03, Validation-Loss 1.64e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.95e-02\n",
      "Epoch 11710, Training-Loss 1.33e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.51e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 11720, Training-Loss 1.40e+03, Validation-Loss 1.60e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.88e-02\n",
      "Epoch 11730, Training-Loss 1.55e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 11740, Training-Loss 1.53e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 11750, Training-Loss 1.39e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 11760, Training-Loss 1.60e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 11770, Training-Loss 1.50e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 11780, Training-Loss 1.44e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 11790, Training-Loss 1.46e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 11800, Training-Loss 1.37e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.51e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 11810, Training-Loss 1.39e+03, Validation-Loss 1.34e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 11820, Training-Loss 1.48e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.85e-02\n",
      "Epoch 11830, Training-Loss 1.38e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 11840, Training-Loss 1.50e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 11850, Training-Loss 1.46e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 11860, Training-Loss 1.46e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.53e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 11870, Training-Loss 1.36e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.79e-02\n",
      "Epoch 11880, Training-Loss 1.44e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 11890, Training-Loss 1.47e+03, Validation-Loss 1.59e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.83e-02\n",
      "Epoch 11900, Training-Loss 1.44e+03, Validation-Loss 1.26e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.50e-02\n",
      "Epoch 11910, Training-Loss 1.60e+03, Validation-Loss 1.28e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.42e-02\n",
      "Epoch 11920, Training-Loss 1.40e+03, Validation-Loss 1.61e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.85e-02\n",
      "Epoch 11930, Training-Loss 1.51e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.82e-02\n",
      "Epoch 11940, Training-Loss 1.43e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 11950, Training-Loss 1.51e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 11960, Training-Loss 1.54e+03, Validation-Loss 1.55e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.80e-02\n",
      "Epoch 11970, Training-Loss 1.52e+03, Validation-Loss 1.49e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 11980, Training-Loss 1.51e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.64e-02\n",
      "Epoch 11990, Training-Loss 1.39e+03, Validation-Loss 1.46e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 12000, Training-Loss 1.54e+03, Validation-Loss 1.30e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.53e-02\n",
      "Epoch 12010, Training-Loss 1.51e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.81e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 12020, Training-Loss 1.55e+03, Validation-Loss 1.45e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.62e-02\n",
      "Epoch 12030, Training-Loss 1.66e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.93e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 12040, Training-Loss 1.42e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.64e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 12050, Training-Loss 1.38e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.55e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 12060, Training-Loss 1.40e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 12070, Training-Loss 1.49e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 12080, Training-Loss 1.58e+03, Validation-Loss 1.54e+03, Training-Accuracy 3.87e-02, Validation-Accuracy 3.86e-02\n",
      "Epoch 12090, Training-Loss 1.50e+03, Validation-Loss 1.37e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 12100, Training-Loss 1.51e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 12110, Training-Loss 1.51e+03, Validation-Loss 1.29e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.51e-02\n",
      "Epoch 12120, Training-Loss 1.47e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 12130, Training-Loss 1.36e+03, Validation-Loss 1.59e+03, Training-Accuracy 3.57e-02, Validation-Accuracy 3.76e-02\n",
      "Epoch 12140, Training-Loss 1.69e+03, Validation-Loss 1.31e+03, Training-Accuracy 3.94e-02, Validation-Accuracy 3.54e-02\n",
      "Epoch 12150, Training-Loss 1.37e+03, Validation-Loss 1.31e+03, Training-Accuracy 3.62e-02, Validation-Accuracy 3.51e-02\n",
      "Epoch 12160, Training-Loss 1.44e+03, Validation-Loss 1.60e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.85e-02\n",
      "Epoch 12170, Training-Loss 1.42e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 12180, Training-Loss 1.57e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.73e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 12190, Training-Loss 1.53e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 12200, Training-Loss 1.36e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 12210, Training-Loss 1.49e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 12220, Training-Loss 1.48e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.71e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 12230, Training-Loss 1.29e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.70e-02\n",
      "Epoch 12240, Training-Loss 1.47e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.54e-02\n",
      "Epoch 12250, Training-Loss 1.52e+03, Validation-Loss 1.58e+03, Training-Accuracy 3.79e-02, Validation-Accuracy 3.78e-02\n",
      "Epoch 12260, Training-Loss 1.50e+03, Validation-Loss 1.35e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 12270, Training-Loss 1.35e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.56e-02, Validation-Accuracy 3.60e-02\n",
      "Epoch 12280, Training-Loss 1.46e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.68e-02, Validation-Accuracy 3.91e-02\n",
      "Epoch 12290, Training-Loss 1.44e+03, Validation-Loss 1.28e+03, Training-Accuracy 3.69e-02, Validation-Accuracy 3.52e-02\n",
      "Epoch 12300, Training-Loss 1.59e+03, Validation-Loss 1.33e+03, Training-Accuracy 3.85e-02, Validation-Accuracy 3.58e-02\n",
      "Epoch 12310, Training-Loss 1.42e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 12320, Training-Loss 1.56e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.83e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 12330, Training-Loss 1.39e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.82e-02\n",
      "Epoch 12340, Training-Loss 1.48e+03, Validation-Loss 1.44e+03, Training-Accuracy 3.70e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 12350, Training-Loss 1.56e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.80e-02, Validation-Accuracy 3.68e-02\n",
      "Epoch 12360, Training-Loss 1.51e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.65e-02, Validation-Accuracy 3.56e-02\n",
      "Epoch 12370, Training-Loss 1.38e+03, Validation-Loss 1.53e+03, Training-Accuracy 3.61e-02, Validation-Accuracy 3.82e-02\n",
      "Epoch 12380, Training-Loss 1.51e+03, Validation-Loss 1.48e+03, Training-Accuracy 3.77e-02, Validation-Accuracy 3.75e-02\n",
      "Epoch 12390, Training-Loss 1.36e+03, Validation-Loss 1.57e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.84e-02\n",
      "Epoch 12400, Training-Loss 1.45e+03, Validation-Loss 1.32e+03, Training-Accuracy 3.72e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 12410, Training-Loss 1.35e+03, Validation-Loss 1.47e+03, Training-Accuracy 3.51e-02, Validation-Accuracy 3.71e-02\n",
      "Epoch 12420, Training-Loss 1.47e+03, Validation-Loss 1.38e+03, Training-Accuracy 3.66e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 12430, Training-Loss 1.56e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.75e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 12440, Training-Loss 1.40e+03, Validation-Loss 1.42e+03, Training-Accuracy 3.59e-02, Validation-Accuracy 3.67e-02\n",
      "Epoch 12450, Training-Loss 1.57e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.81e-02, Validation-Accuracy 3.61e-02\n",
      "Epoch 12460, Training-Loss 1.54e+03, Validation-Loss 1.50e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.74e-02\n",
      "Epoch 12470, Training-Loss 1.39e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 12480, Training-Loss 1.61e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.83e-02, Validation-Accuracy 3.55e-02\n",
      "Epoch 12490, Training-Loss 1.62e+03, Validation-Loss 1.52e+03, Training-Accuracy 3.86e-02, Validation-Accuracy 3.77e-02\n",
      "Epoch 12500, Training-Loss 1.34e+03, Validation-Loss 1.41e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.63e-02\n",
      "Epoch 12510, Training-Loss 1.50e+03, Validation-Loss 1.40e+03, Training-Accuracy 3.78e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 12520, Training-Loss 1.38e+03, Validation-Loss 1.61e+03, Training-Accuracy 3.58e-02, Validation-Accuracy 3.85e-02\n",
      "Epoch 12530, Training-Loss 1.41e+03, Validation-Loss 1.39e+03, Training-Accuracy 3.63e-02, Validation-Accuracy 3.65e-02\n",
      "Epoch 12540, Training-Loss 1.47e+03, Validation-Loss 1.43e+03, Training-Accuracy 3.67e-02, Validation-Accuracy 3.57e-02\n",
      "Epoch 12550, Training-Loss 1.47e+03, Validation-Loss 1.21e+03, Training-Accuracy 3.74e-02, Validation-Accuracy 3.35e-02\n",
      "Epoch 12560, Training-Loss 1.37e+03, Validation-Loss 1.51e+03, Training-Accuracy 3.60e-02, Validation-Accuracy 3.73e-02\n",
      "Epoch 12570, Training-Loss 1.49e+03, Validation-Loss 1.33e+03, Training-Accuracy 3.76e-02, Validation-Accuracy 3.48e-02\n",
      "Epoch 12580, Training-Loss 9.46e+02, Validation-Loss 7.99e+02, Training-Accuracy 2.38e-02, Validation-Accuracy 2.36e-02\n",
      "Epoch 12590, Training-Loss 7.88e+02, Validation-Loss 8.70e+02, Training-Accuracy 2.27e-02, Validation-Accuracy 2.42e-02\n",
      "Epoch 12600, Training-Loss 8.76e+02, Validation-Loss 8.24e+02, Training-Accuracy 2.38e-02, Validation-Accuracy 2.34e-02\n",
      "Epoch 12610, Training-Loss 8.21e+02, Validation-Loss 6.94e+02, Training-Accuracy 2.28e-02, Validation-Accuracy 2.11e-02\n",
      "Epoch 12620, Training-Loss 8.16e+02, Validation-Loss 7.25e+02, Training-Accuracy 2.21e-02, Validation-Accuracy 2.14e-02\n",
      "Epoch 12630, Training-Loss 5.50e+02, Validation-Loss 5.74e+02, Training-Accuracy 1.77e-02, Validation-Accuracy 1.80e-02\n",
      "Epoch 12640, Training-Loss 5.93e+02, Validation-Loss 5.69e+02, Training-Accuracy 1.60e-02, Validation-Accuracy 1.60e-02\n",
      "Epoch 12650, Training-Loss 6.22e+02, Validation-Loss 4.34e+02, Training-Accuracy 1.67e-02, Validation-Accuracy 1.47e-02\n",
      "Epoch 12660, Training-Loss 5.23e+02, Validation-Loss 5.21e+02, Training-Accuracy 1.56e-02, Validation-Accuracy 1.49e-02\n",
      "Epoch 12670, Training-Loss 4.54e+02, Validation-Loss 4.80e+02, Training-Accuracy 1.43e-02, Validation-Accuracy 1.40e-02\n",
      "Epoch 12680, Training-Loss 3.84e+02, Validation-Loss 4.46e+02, Training-Accuracy 1.25e-02, Validation-Accuracy 1.34e-02\n",
      "Epoch 12690, Training-Loss 4.34e+02, Validation-Loss 3.70e+02, Training-Accuracy 1.40e-02, Validation-Accuracy 1.20e-02\n",
      "Epoch 12700, Training-Loss 3.17e+02, Validation-Loss 4.14e+02, Training-Accuracy 1.07e-02, Validation-Accuracy 1.24e-02\n",
      "Epoch 12710, Training-Loss 3.38e+02, Validation-Loss 3.47e+02, Training-Accuracy 1.12e-02, Validation-Accuracy 1.16e-02\n",
      "Epoch 12720, Training-Loss 3.25e+02, Validation-Loss 3.09e+02, Training-Accuracy 1.11e-02, Validation-Accuracy 1.04e-02\n",
      "Epoch 12730, Training-Loss 3.63e+02, Validation-Loss 2.03e+02, Training-Accuracy 1.14e-02, Validation-Accuracy 9.12e-03\n",
      "Epoch 12740, Training-Loss 3.03e+02, Validation-Loss 3.57e+02, Training-Accuracy 1.03e-02, Validation-Accuracy 1.10e-02\n",
      "Epoch 12750, Training-Loss 2.44e+02, Validation-Loss 2.50e+02, Training-Accuracy 8.99e-03, Validation-Accuracy 9.31e-03\n",
      "Epoch 12760, Training-Loss 2.53e+02, Validation-Loss 2.50e+02, Training-Accuracy 9.06e-03, Validation-Accuracy 9.30e-03\n",
      "Epoch 12770, Training-Loss 2.65e+02, Validation-Loss 2.41e+02, Training-Accuracy 9.36e-03, Validation-Accuracy 8.89e-03\n",
      "Epoch 12780, Training-Loss 2.65e+02, Validation-Loss 2.53e+02, Training-Accuracy 9.45e-03, Validation-Accuracy 9.16e-03\n",
      "Epoch 12790, Training-Loss 2.50e+02, Validation-Loss 2.06e+02, Training-Accuracy 9.34e-03, Validation-Accuracy 8.29e-03\n",
      "Epoch 12800, Training-Loss 2.29e+02, Validation-Loss 2.49e+02, Training-Accuracy 8.61e-03, Validation-Accuracy 8.89e-03\n",
      "Epoch 12810, Training-Loss 1.94e+02, Validation-Loss 1.95e+02, Training-Accuracy 8.20e-03, Validation-Accuracy 8.40e-03\n",
      "Epoch 12820, Training-Loss 2.02e+02, Validation-Loss 1.69e+02, Training-Accuracy 8.14e-03, Validation-Accuracy 7.69e-03\n",
      "Epoch 12830, Training-Loss 2.09e+02, Validation-Loss 2.06e+02, Training-Accuracy 7.79e-03, Validation-Accuracy 8.46e-03\n",
      "Epoch 12840, Training-Loss 1.92e+02, Validation-Loss 2.02e+02, Training-Accuracy 7.61e-03, Validation-Accuracy 7.93e-03\n",
      "Epoch 12850, Training-Loss 2.29e+02, Validation-Loss 1.61e+02, Training-Accuracy 8.66e-03, Validation-Accuracy 7.27e-03\n",
      "Epoch 12860, Training-Loss 2.09e+02, Validation-Loss 2.08e+02, Training-Accuracy 7.94e-03, Validation-Accuracy 7.99e-03\n",
      "Epoch 12870, Training-Loss 2.35e+02, Validation-Loss 1.55e+02, Training-Accuracy 8.18e-03, Validation-Accuracy 6.89e-03\n",
      "Epoch 12880, Training-Loss 2.09e+02, Validation-Loss 2.12e+02, Training-Accuracy 8.21e-03, Validation-Accuracy 8.09e-03\n",
      "Epoch 12890, Training-Loss 1.72e+02, Validation-Loss 1.66e+02, Training-Accuracy 7.31e-03, Validation-Accuracy 7.25e-03\n",
      "Epoch 12900, Training-Loss 1.52e+02, Validation-Loss 1.57e+02, Training-Accuracy 6.85e-03, Validation-Accuracy 6.75e-03\n",
      "Epoch 12910, Training-Loss 1.21e+02, Validation-Loss 1.49e+02, Training-Accuracy 6.12e-03, Validation-Accuracy 6.88e-03\n",
      "Epoch 12920, Training-Loss 1.44e+02, Validation-Loss 1.49e+02, Training-Accuracy 7.03e-03, Validation-Accuracy 6.76e-03\n",
      "Epoch 12930, Training-Loss 1.71e+02, Validation-Loss 1.42e+02, Training-Accuracy 7.24e-03, Validation-Accuracy 6.97e-03\n",
      "Epoch 12940, Training-Loss 1.71e+02, Validation-Loss 1.42e+02, Training-Accuracy 7.19e-03, Validation-Accuracy 6.77e-03\n",
      "Epoch 12950, Training-Loss 1.52e+02, Validation-Loss 9.84e+01, Training-Accuracy 7.37e-03, Validation-Accuracy 6.49e-03\n",
      "Epoch 12960, Training-Loss 1.72e+02, Validation-Loss 1.16e+02, Training-Accuracy 7.40e-03, Validation-Accuracy 6.94e-03\n",
      "Epoch 12970, Training-Loss 1.38e+02, Validation-Loss 1.26e+02, Training-Accuracy 6.56e-03, Validation-Accuracy 6.39e-03\n",
      "Epoch 12980, Training-Loss 1.51e+02, Validation-Loss 1.39e+02, Training-Accuracy 6.96e-03, Validation-Accuracy 6.81e-03\n",
      "Epoch 12990, Training-Loss 1.14e+02, Validation-Loss 1.19e+02, Training-Accuracy 6.14e-03, Validation-Accuracy 5.94e-03\n",
      "Epoch 13000, Training-Loss 1.22e+02, Validation-Loss 1.15e+02, Training-Accuracy 6.46e-03, Validation-Accuracy 6.54e-03\n",
      "Epoch 13010, Training-Loss 1.36e+02, Validation-Loss 1.38e+02, Training-Accuracy 6.63e-03, Validation-Accuracy 6.89e-03\n",
      "Epoch 13020, Training-Loss 1.44e+02, Validation-Loss 1.31e+02, Training-Accuracy 6.69e-03, Validation-Accuracy 6.66e-03\n",
      "Epoch 13030, Training-Loss 1.07e+02, Validation-Loss 9.32e+01, Training-Accuracy 5.95e-03, Validation-Accuracy 5.68e-03\n",
      "Epoch 13040, Training-Loss 1.15e+02, Validation-Loss 1.08e+02, Training-Accuracy 6.06e-03, Validation-Accuracy 5.64e-03\n",
      "Epoch 13050, Training-Loss 1.23e+02, Validation-Loss 1.32e+02, Training-Accuracy 6.20e-03, Validation-Accuracy 6.41e-03\n",
      "Epoch 13060, Training-Loss 8.42e+01, Validation-Loss 9.94e+01, Training-Accuracy 5.08e-03, Validation-Accuracy 5.59e-03\n",
      "Epoch 13070, Training-Loss 9.75e+01, Validation-Loss 9.59e+01, Training-Accuracy 5.57e-03, Validation-Accuracy 5.55e-03\n",
      "Epoch 13080, Training-Loss 1.16e+02, Validation-Loss 8.99e+01, Training-Accuracy 6.14e-03, Validation-Accuracy 5.28e-03\n",
      "Epoch 13090, Training-Loss 1.07e+02, Validation-Loss 8.02e+01, Training-Accuracy 5.44e-03, Validation-Accuracy 5.20e-03\n",
      "Epoch 13100, Training-Loss 1.08e+02, Validation-Loss 9.84e+01, Training-Accuracy 5.67e-03, Validation-Accuracy 5.67e-03\n",
      "Epoch 13110, Training-Loss 8.49e+01, Validation-Loss 7.35e+01, Training-Accuracy 5.17e-03, Validation-Accuracy 4.91e-03\n",
      "Epoch 13120, Training-Loss 9.12e+01, Validation-Loss 9.34e+01, Training-Accuracy 5.06e-03, Validation-Accuracy 4.88e-03\n",
      "Epoch 13130, Training-Loss 1.10e+02, Validation-Loss 1.02e+02, Training-Accuracy 5.45e-03, Validation-Accuracy 5.20e-03\n",
      "Epoch 13140, Training-Loss 7.90e+01, Validation-Loss 6.38e+01, Training-Accuracy 4.67e-03, Validation-Accuracy 4.21e-03\n",
      "Epoch 13150, Training-Loss 1.00e+02, Validation-Loss 8.08e+01, Training-Accuracy 5.10e-03, Validation-Accuracy 4.51e-03\n",
      "Epoch 13160, Training-Loss 8.63e+01, Validation-Loss 7.48e+01, Training-Accuracy 4.80e-03, Validation-Accuracy 4.59e-03\n",
      "Epoch 13170, Training-Loss 9.18e+01, Validation-Loss 9.92e+01, Training-Accuracy 4.94e-03, Validation-Accuracy 4.96e-03\n",
      "Epoch 13180, Training-Loss 6.84e+01, Validation-Loss 8.17e+01, Training-Accuracy 4.67e-03, Validation-Accuracy 4.81e-03\n",
      "Epoch 13190, Training-Loss 7.07e+01, Validation-Loss 6.55e+01, Training-Accuracy 4.50e-03, Validation-Accuracy 4.33e-03\n",
      "Epoch 13200, Training-Loss 6.38e+01, Validation-Loss 9.42e+01, Training-Accuracy 4.01e-03, Validation-Accuracy 5.06e-03\n",
      "Epoch 13210, Training-Loss 6.87e+01, Validation-Loss 6.61e+01, Training-Accuracy 4.57e-03, Validation-Accuracy 4.25e-03\n",
      "Epoch 13220, Training-Loss 6.92e+01, Validation-Loss 7.79e+01, Training-Accuracy 4.61e-03, Validation-Accuracy 4.45e-03\n",
      "Epoch 13230, Training-Loss 7.06e+01, Validation-Loss 6.54e+01, Training-Accuracy 4.25e-03, Validation-Accuracy 4.28e-03\n",
      "Epoch 13240, Training-Loss 5.47e+01, Validation-Loss 5.63e+01, Training-Accuracy 4.01e-03, Validation-Accuracy 4.00e-03\n",
      "Epoch 13250, Training-Loss 6.07e+01, Validation-Loss 6.37e+01, Training-Accuracy 4.23e-03, Validation-Accuracy 4.46e-03\n",
      "Epoch 13260, Training-Loss 6.36e+01, Validation-Loss 7.03e+01, Training-Accuracy 4.21e-03, Validation-Accuracy 4.43e-03\n",
      "Epoch 13270, Training-Loss 4.87e+01, Validation-Loss 4.99e+01, Training-Accuracy 3.66e-03, Validation-Accuracy 3.89e-03\n",
      "Epoch 13280, Training-Loss 5.69e+01, Validation-Loss 7.22e+01, Training-Accuracy 3.98e-03, Validation-Accuracy 4.33e-03\n",
      "Epoch 13290, Training-Loss 7.39e+01, Validation-Loss 6.00e+01, Training-Accuracy 4.38e-03, Validation-Accuracy 4.02e-03\n",
      "Epoch 13300, Training-Loss 4.65e+01, Validation-Loss 5.91e+01, Training-Accuracy 3.54e-03, Validation-Accuracy 4.10e-03\n",
      "Epoch 13310, Training-Loss 4.88e+01, Validation-Loss 4.32e+01, Training-Accuracy 3.36e-03, Validation-Accuracy 3.52e-03\n",
      "Epoch 13320, Training-Loss 5.75e+01, Validation-Loss 5.10e+01, Training-Accuracy 3.90e-03, Validation-Accuracy 3.68e-03\n",
      "Epoch 13330, Training-Loss 4.98e+01, Validation-Loss 5.95e+01, Training-Accuracy 3.64e-03, Validation-Accuracy 3.94e-03\n",
      "Epoch 13340, Training-Loss 4.70e+01, Validation-Loss 5.14e+01, Training-Accuracy 3.40e-03, Validation-Accuracy 3.53e-03\n",
      "Epoch 13350, Training-Loss 4.25e+01, Validation-Loss 4.76e+01, Training-Accuracy 3.26e-03, Validation-Accuracy 3.68e-03\n",
      "Epoch 13360, Training-Loss 4.28e+01, Validation-Loss 3.56e+01, Training-Accuracy 3.52e-03, Validation-Accuracy 3.15e-03\n",
      "Epoch 13370, Training-Loss 4.96e+01, Validation-Loss 5.20e+01, Training-Accuracy 3.61e-03, Validation-Accuracy 3.68e-03\n",
      "Epoch 13380, Training-Loss 6.02e+01, Validation-Loss 3.93e+01, Training-Accuracy 3.98e-03, Validation-Accuracy 3.26e-03\n",
      "Epoch 13390, Training-Loss 6.24e+01, Validation-Loss 4.72e+01, Training-Accuracy 3.87e-03, Validation-Accuracy 3.49e-03\n",
      "Epoch 13400, Training-Loss 5.40e+01, Validation-Loss 4.84e+01, Training-Accuracy 3.65e-03, Validation-Accuracy 3.29e-03\n",
      "Epoch 13410, Training-Loss 6.14e+01, Validation-Loss 4.00e+01, Training-Accuracy 3.83e-03, Validation-Accuracy 3.06e-03\n",
      "Epoch 13420, Training-Loss 3.96e+01, Validation-Loss 4.70e+01, Training-Accuracy 3.31e-03, Validation-Accuracy 3.33e-03\n",
      "Epoch 13430, Training-Loss 3.58e+01, Validation-Loss 4.01e+01, Training-Accuracy 2.98e-03, Validation-Accuracy 3.13e-03\n",
      "Epoch 13440, Training-Loss 4.57e+01, Validation-Loss 3.25e+01, Training-Accuracy 3.24e-03, Validation-Accuracy 2.74e-03\n",
      "Epoch 13450, Training-Loss 3.16e+01, Validation-Loss 3.20e+01, Training-Accuracy 2.81e-03, Validation-Accuracy 2.72e-03\n",
      "Epoch 13460, Training-Loss 3.83e+01, Validation-Loss 4.80e+01, Training-Accuracy 3.09e-03, Validation-Accuracy 3.23e-03\n",
      "Epoch 13470, Training-Loss 4.52e+01, Validation-Loss 2.74e+01, Training-Accuracy 3.35e-03, Validation-Accuracy 2.61e-03\n",
      "Epoch 13480, Training-Loss 1.61e+01, Validation-Loss 2.43e+01, Training-Accuracy 2.42e-03, Validation-Accuracy 2.54e-03\n",
      "Epoch 13490, Training-Loss 2.87e+01, Validation-Loss 4.05e+01, Training-Accuracy 2.82e-03, Validation-Accuracy 3.36e-03\n",
      "Epoch 13500, Training-Loss 2.98e+01, Validation-Loss 4.26e+01, Training-Accuracy 2.61e-03, Validation-Accuracy 3.11e-03\n",
      "Epoch 13510, Training-Loss 2.18e+01, Validation-Loss 2.57e+01, Training-Accuracy 2.29e-03, Validation-Accuracy 2.45e-03\n",
      "Epoch 13520, Training-Loss 2.96e+01, Validation-Loss 3.34e+01, Training-Accuracy 2.67e-03, Validation-Accuracy 2.62e-03\n",
      "Epoch 13530, Training-Loss 3.33e+01, Validation-Loss 1.88e+01, Training-Accuracy 2.90e-03, Validation-Accuracy 2.34e-03\n",
      "Epoch 13540, Training-Loss 3.11e+01, Validation-Loss 3.11e+01, Training-Accuracy 2.64e-03, Validation-Accuracy 2.66e-03\n",
      "Epoch 13550, Training-Loss 2.52e+01, Validation-Loss 1.95e+01, Training-Accuracy 2.32e-03, Validation-Accuracy 2.21e-03\n",
      "Epoch 13560, Training-Loss 2.07e+01, Validation-Loss 3.15e+01, Training-Accuracy 2.24e-03, Validation-Accuracy 2.68e-03\n",
      "Epoch 13570, Training-Loss 3.95e+01, Validation-Loss 3.11e+01, Training-Accuracy 2.85e-03, Validation-Accuracy 2.49e-03\n",
      "Epoch 13580, Training-Loss 2.56e+01, Validation-Loss 2.89e+01, Training-Accuracy 2.42e-03, Validation-Accuracy 2.51e-03\n",
      "Epoch 13590, Training-Loss 2.48e+01, Validation-Loss 2.34e+01, Training-Accuracy 2.25e-03, Validation-Accuracy 2.43e-03\n",
      "Epoch 13600, Training-Loss 2.59e+01, Validation-Loss 2.50e+01, Training-Accuracy 2.51e-03, Validation-Accuracy 2.34e-03\n",
      "Epoch 13610, Training-Loss 1.08e+01, Validation-Loss 2.81e+01, Training-Accuracy 1.74e-03, Validation-Accuracy 2.36e-03\n",
      "Epoch 13620, Training-Loss 2.20e+01, Validation-Loss 2.90e+01, Training-Accuracy 2.17e-03, Validation-Accuracy 2.37e-03\n",
      "Epoch 13630, Training-Loss 3.28e+01, Validation-Loss 1.66e+01, Training-Accuracy 2.49e-03, Validation-Accuracy 1.87e-03\n",
      "Epoch 13640, Training-Loss 2.67e+01, Validation-Loss 1.95e+01, Training-Accuracy 2.42e-03, Validation-Accuracy 2.19e-03\n",
      "Epoch 13650, Training-Loss 2.57e+01, Validation-Loss 1.56e+01, Training-Accuracy 2.26e-03, Validation-Accuracy 2.02e-03\n",
      "Epoch 13660, Training-Loss 2.76e+01, Validation-Loss 2.36e+01, Training-Accuracy 2.43e-03, Validation-Accuracy 2.16e-03\n",
      "Epoch 13670, Training-Loss 1.50e+01, Validation-Loss 1.83e+01, Training-Accuracy 1.72e-03, Validation-Accuracy 2.07e-03\n",
      "Epoch 13680, Training-Loss 1.95e+01, Validation-Loss 2.85e+01, Training-Accuracy 2.07e-03, Validation-Accuracy 2.44e-03\n",
      "Epoch 13690, Training-Loss 2.03e+01, Validation-Loss 2.60e+01, Training-Accuracy 2.06e-03, Validation-Accuracy 2.22e-03\n",
      "Epoch 13700, Training-Loss 2.27e+01, Validation-Loss 1.80e+01, Training-Accuracy 2.15e-03, Validation-Accuracy 1.98e-03\n",
      "Epoch 13710, Training-Loss 1.90e+01, Validation-Loss 2.05e+01, Training-Accuracy 1.98e-03, Validation-Accuracy 2.16e-03\n",
      "Epoch 13720, Training-Loss 1.12e+01, Validation-Loss 1.67e+01, Training-Accuracy 1.71e-03, Validation-Accuracy 1.76e-03\n",
      "Epoch 13730, Training-Loss 2.51e+01, Validation-Loss 2.10e+01, Training-Accuracy 2.24e-03, Validation-Accuracy 2.01e-03\n",
      "Epoch 13740, Training-Loss 1.99e+01, Validation-Loss 1.10e+01, Training-Accuracy 2.12e-03, Validation-Accuracy 1.79e-03\n",
      "Epoch 13750, Training-Loss 1.46e+01, Validation-Loss 1.88e+01, Training-Accuracy 1.67e-03, Validation-Accuracy 1.96e-03\n",
      "Epoch 13760, Training-Loss 1.16e+01, Validation-Loss 1.48e+01, Training-Accuracy 1.74e-03, Validation-Accuracy 1.81e-03\n",
      "Epoch 13770, Training-Loss 2.40e+01, Validation-Loss 1.14e+01, Training-Accuracy 2.24e-03, Validation-Accuracy 1.62e-03\n",
      "Epoch 13780, Training-Loss 1.27e+01, Validation-Loss 1.93e+01, Training-Accuracy 1.64e-03, Validation-Accuracy 1.98e-03\n",
      "Epoch 13790, Training-Loss 1.24e+01, Validation-Loss 1.58e+01, Training-Accuracy 1.73e-03, Validation-Accuracy 1.85e-03\n",
      "Epoch 13800, Training-Loss 1.35e+01, Validation-Loss 1.81e+01, Training-Accuracy 1.70e-03, Validation-Accuracy 1.77e-03\n",
      "Epoch 13810, Training-Loss 2.13e+01, Validation-Loss 1.42e+01, Training-Accuracy 2.28e-03, Validation-Accuracy 1.97e-03\n",
      "Epoch 13820, Training-Loss 1.94e+01, Validation-Loss 1.84e+01, Training-Accuracy 1.96e-03, Validation-Accuracy 1.80e-03\n",
      "Epoch 13830, Training-Loss 1.60e+01, Validation-Loss 1.60e+01, Training-Accuracy 1.88e-03, Validation-Accuracy 1.67e-03\n",
      "Epoch 13840, Training-Loss 1.62e+01, Validation-Loss 1.21e+01, Training-Accuracy 1.63e-03, Validation-Accuracy 1.62e-03\n",
      "Epoch 13850, Training-Loss 1.15e+01, Validation-Loss 1.31e+01, Training-Accuracy 1.53e-03, Validation-Accuracy 1.59e-03\n",
      "Epoch 13860, Training-Loss 1.50e+01, Validation-Loss 1.57e+01, Training-Accuracy 1.68e-03, Validation-Accuracy 1.84e-03\n",
      "Epoch 13870, Training-Loss 1.15e+01, Validation-Loss 1.10e+01, Training-Accuracy 1.81e-03, Validation-Accuracy 1.75e-03\n",
      "Epoch 13880, Training-Loss 1.57e+01, Validation-Loss 1.14e+01, Training-Accuracy 2.16e-03, Validation-Accuracy 1.74e-03\n",
      "Epoch 13890, Training-Loss 1.22e+01, Validation-Loss 7.70e+00, Training-Accuracy 1.64e-03, Validation-Accuracy 1.65e-03\n",
      "Epoch 13900, Training-Loss 1.04e+01, Validation-Loss 1.91e+01, Training-Accuracy 1.51e-03, Validation-Accuracy 1.79e-03\n",
      "Epoch 13910, Training-Loss 9.90e+00, Validation-Loss 1.55e+01, Training-Accuracy 1.47e-03, Validation-Accuracy 1.70e-03\n",
      "Epoch 13920, Training-Loss 8.59e+00, Validation-Loss 1.43e+01, Training-Accuracy 1.32e-03, Validation-Accuracy 1.76e-03\n",
      "Epoch 13930, Training-Loss 1.09e+01, Validation-Loss 1.07e+01, Training-Accuracy 1.53e-03, Validation-Accuracy 1.49e-03\n",
      "Epoch 13940, Training-Loss 7.74e+00, Validation-Loss 1.34e+01, Training-Accuracy 1.36e-03, Validation-Accuracy 1.64e-03\n",
      "Epoch 13950, Training-Loss 1.66e+01, Validation-Loss 1.57e+01, Training-Accuracy 1.73e-03, Validation-Accuracy 2.00e-03\n",
      "Epoch 13960, Training-Loss 1.30e+01, Validation-Loss 9.80e+00, Training-Accuracy 1.63e-03, Validation-Accuracy 1.48e-03\n",
      "Epoch 13970, Training-Loss 1.06e+01, Validation-Loss 1.11e+01, Training-Accuracy 1.47e-03, Validation-Accuracy 1.42e-03\n",
      "Epoch 13980, Training-Loss 9.14e+00, Validation-Loss 1.35e+01, Training-Accuracy 1.44e-03, Validation-Accuracy 1.69e-03\n",
      "Epoch 13990, Training-Loss 1.43e+01, Validation-Loss 8.05e+00, Training-Accuracy 1.68e-03, Validation-Accuracy 1.33e-03\n",
      "Epoch 14000, Training-Loss 8.88e+00, Validation-Loss 1.37e+01, Training-Accuracy 1.43e-03, Validation-Accuracy 1.81e-03\n",
      "Epoch 14010, Training-Loss 1.11e+01, Validation-Loss 9.62e+00, Training-Accuracy 1.66e-03, Validation-Accuracy 1.50e-03\n",
      "Epoch 14020, Training-Loss 8.70e+00, Validation-Loss 9.63e+00, Training-Accuracy 1.50e-03, Validation-Accuracy 1.48e-03\n",
      "Epoch 14030, Training-Loss 4.27e+00, Validation-Loss 1.29e+01, Training-Accuracy 1.08e-03, Validation-Accuracy 1.53e-03\n",
      "Epoch 14040, Training-Loss 1.02e+01, Validation-Loss 8.31e+00, Training-Accuracy 1.80e-03, Validation-Accuracy 1.83e-03\n",
      "Epoch 14050, Training-Loss 1.43e+01, Validation-Loss 7.56e+00, Training-Accuracy 1.69e-03, Validation-Accuracy 1.39e-03\n",
      "Epoch 14060, Training-Loss 6.02e+00, Validation-Loss 1.28e+01, Training-Accuracy 1.36e-03, Validation-Accuracy 1.63e-03\n",
      "Epoch 14070, Training-Loss 1.21e+01, Validation-Loss 6.02e+00, Training-Accuracy 1.45e-03, Validation-Accuracy 1.26e-03\n",
      "Epoch 14080, Training-Loss 7.80e+00, Validation-Loss 7.75e+00, Training-Accuracy 1.20e-03, Validation-Accuracy 1.34e-03\n",
      "Epoch 14090, Training-Loss 8.70e+00, Validation-Loss 8.21e+00, Training-Accuracy 1.38e-03, Validation-Accuracy 1.34e-03\n",
      "Epoch 14100, Training-Loss 1.01e+01, Validation-Loss 7.33e+00, Training-Accuracy 1.76e-03, Validation-Accuracy 1.44e-03\n",
      "Epoch 14110, Training-Loss 1.07e+01, Validation-Loss 9.18e+00, Training-Accuracy 1.47e-03, Validation-Accuracy 1.94e-03\n",
      "Epoch 14120, Training-Loss 9.20e+00, Validation-Loss 9.08e+00, Training-Accuracy 1.32e-03, Validation-Accuracy 1.50e-03\n",
      "Epoch 14130, Training-Loss 9.58e+00, Validation-Loss 6.23e+00, Training-Accuracy 1.39e-03, Validation-Accuracy 1.21e-03\n",
      "Epoch 14140, Training-Loss 6.39e+00, Validation-Loss 7.26e+00, Training-Accuracy 1.24e-03, Validation-Accuracy 1.47e-03\n",
      "Epoch 14150, Training-Loss 9.03e+00, Validation-Loss 7.31e+00, Training-Accuracy 1.35e-03, Validation-Accuracy 1.26e-03\n",
      "Epoch 14160, Training-Loss 7.31e+00, Validation-Loss 6.27e+00, Training-Accuracy 1.23e-03, Validation-Accuracy 1.23e-03\n",
      "Epoch 14170, Training-Loss 7.80e+00, Validation-Loss 7.37e+00, Training-Accuracy 1.37e-03, Validation-Accuracy 1.37e-03\n",
      "Epoch 14180, Training-Loss 8.13e+00, Validation-Loss 6.84e+00, Training-Accuracy 1.26e-03, Validation-Accuracy 1.27e-03\n",
      "Epoch 14190, Training-Loss 8.34e+00, Validation-Loss 6.24e+00, Training-Accuracy 1.40e-03, Validation-Accuracy 1.15e-03\n",
      "Epoch 14200, Training-Loss 6.29e+00, Validation-Loss 9.13e+00, Training-Accuracy 1.22e-03, Validation-Accuracy 1.32e-03\n",
      "Epoch 14210, Training-Loss 6.73e+00, Validation-Loss 5.99e+00, Training-Accuracy 1.41e-03, Validation-Accuracy 1.32e-03\n",
      "Epoch 14220, Training-Loss 8.45e+00, Validation-Loss 6.11e+00, Training-Accuracy 1.37e-03, Validation-Accuracy 1.31e-03\n",
      "Epoch 14230, Training-Loss 5.71e+00, Validation-Loss 5.04e+00, Training-Accuracy 1.13e-03, Validation-Accuracy 1.15e-03\n",
      "Epoch 14240, Training-Loss 7.09e+00, Validation-Loss 7.07e+00, Training-Accuracy 1.23e-03, Validation-Accuracy 1.35e-03\n",
      "Epoch 14250, Training-Loss 8.10e+00, Validation-Loss 4.66e+00, Training-Accuracy 1.38e-03, Validation-Accuracy 1.15e-03\n",
      "Epoch 14260, Training-Loss 5.38e+00, Validation-Loss 6.40e+00, Training-Accuracy 1.24e-03, Validation-Accuracy 1.28e-03\n",
      "Epoch 14270, Training-Loss 6.97e+00, Validation-Loss 5.84e+00, Training-Accuracy 1.29e-03, Validation-Accuracy 1.21e-03\n",
      "Epoch 14280, Training-Loss 6.18e+00, Validation-Loss 5.93e+00, Training-Accuracy 1.15e-03, Validation-Accuracy 1.15e-03\n",
      "Epoch 14290, Training-Loss 4.36e+00, Validation-Loss 6.97e+00, Training-Accuracy 1.24e-03, Validation-Accuracy 1.21e-03\n",
      "Epoch 14300, Training-Loss 5.66e+00, Validation-Loss 4.17e+00, Training-Accuracy 1.26e-03, Validation-Accuracy 1.11e-03\n",
      "Epoch 14310, Training-Loss 5.06e+00, Validation-Loss 5.60e+00, Training-Accuracy 1.04e-03, Validation-Accuracy 1.15e-03\n",
      "Epoch 14320, Training-Loss 3.64e+00, Validation-Loss 6.84e+00, Training-Accuracy 1.02e-03, Validation-Accuracy 1.29e-03\n",
      "Epoch 14330, Training-Loss 6.79e+00, Validation-Loss 5.16e+00, Training-Accuracy 1.32e-03, Validation-Accuracy 1.25e-03\n",
      "Epoch 14340, Training-Loss 6.88e+00, Validation-Loss 6.73e+00, Training-Accuracy 1.23e-03, Validation-Accuracy 1.31e-03\n",
      "Epoch 14350, Training-Loss 6.98e+00, Validation-Loss 2.60e+00, Training-Accuracy 1.71e-03, Validation-Accuracy 1.09e-03\n",
      "Epoch 14360, Training-Loss 6.50e+00, Validation-Loss 6.87e+00, Training-Accuracy 1.14e-03, Validation-Accuracy 1.27e-03\n",
      "Epoch 14370, Training-Loss 6.07e+00, Validation-Loss 4.27e+00, Training-Accuracy 1.26e-03, Validation-Accuracy 1.12e-03\n",
      "Epoch 14380, Training-Loss 4.01e+00, Validation-Loss 5.43e+00, Training-Accuracy 1.04e-03, Validation-Accuracy 1.23e-03\n",
      "Epoch 14390, Training-Loss 6.55e+00, Validation-Loss 6.43e+00, Training-Accuracy 1.35e-03, Validation-Accuracy 1.23e-03\n",
      "Epoch 14400, Training-Loss 7.12e+00, Validation-Loss 5.64e+00, Training-Accuracy 1.71e-03, Validation-Accuracy 1.49e-03\n",
      "Epoch 14410, Training-Loss 4.62e+00, Validation-Loss 6.17e+00, Training-Accuracy 1.31e-03, Validation-Accuracy 1.33e-03\n",
      "Epoch 14420, Training-Loss 6.91e+00, Validation-Loss 5.19e+00, Training-Accuracy 1.17e-03, Validation-Accuracy 1.10e-03\n",
      "Epoch 14430, Training-Loss 5.15e+00, Validation-Loss 3.98e+00, Training-Accuracy 1.08e-03, Validation-Accuracy 1.02e-03\n",
      "Epoch 14440, Training-Loss 7.92e+00, Validation-Loss 4.89e+00, Training-Accuracy 1.25e-03, Validation-Accuracy 1.10e-03\n",
      "Epoch 14450, Training-Loss 6.29e+00, Validation-Loss 4.01e+00, Training-Accuracy 1.10e-03, Validation-Accuracy 1.06e-03\n",
      "Epoch 14460, Training-Loss 4.08e+00, Validation-Loss 5.16e+00, Training-Accuracy 1.12e-03, Validation-Accuracy 1.16e-03\n",
      "Epoch 14470, Training-Loss 3.76e+00, Validation-Loss 3.06e+00, Training-Accuracy 1.01e-03, Validation-Accuracy 9.61e-04\n",
      "Epoch 14480, Training-Loss 5.19e+00, Validation-Loss 2.97e+00, Training-Accuracy 1.10e-03, Validation-Accuracy 1.01e-03\n",
      "Epoch 14490, Training-Loss 3.63e+00, Validation-Loss 5.62e+00, Training-Accuracy 1.06e-03, Validation-Accuracy 1.17e-03\n",
      "Epoch 14500, Training-Loss 4.42e+00, Validation-Loss 4.59e+00, Training-Accuracy 1.13e-03, Validation-Accuracy 1.09e-03\n",
      "Epoch 14510, Training-Loss 4.79e+00, Validation-Loss 2.73e+00, Training-Accuracy 1.15e-03, Validation-Accuracy 1.01e-03\n",
      "Epoch 14520, Training-Loss 5.64e+00, Validation-Loss 6.79e+00, Training-Accuracy 1.20e-03, Validation-Accuracy 1.29e-03\n",
      "Epoch 14530, Training-Loss 3.77e+00, Validation-Loss 5.83e+00, Training-Accuracy 9.84e-04, Validation-Accuracy 1.26e-03\n",
      "Epoch 14540, Training-Loss 6.72e+00, Validation-Loss 4.50e+00, Training-Accuracy 1.53e-03, Validation-Accuracy 1.45e-03\n",
      "Epoch 14550, Training-Loss 4.65e+00, Validation-Loss 4.26e+00, Training-Accuracy 1.11e-03, Validation-Accuracy 1.09e-03\n",
      "Epoch 14560, Training-Loss 6.65e+00, Validation-Loss 3.46e+00, Training-Accuracy 1.29e-03, Validation-Accuracy 1.13e-03\n",
      "Epoch 14570, Training-Loss 4.56e+00, Validation-Loss 5.24e+00, Training-Accuracy 1.06e-03, Validation-Accuracy 1.15e-03\n",
      "Epoch 14580, Training-Loss 3.88e+00, Validation-Loss 3.25e+00, Training-Accuracy 1.12e-03, Validation-Accuracy 1.05e-03\n",
      "Epoch 14590, Training-Loss 5.29e+00, Validation-Loss 3.83e+00, Training-Accuracy 1.54e-03, Validation-Accuracy 1.07e-03\n",
      "Epoch 14600, Training-Loss 4.57e+00, Validation-Loss 3.32e+00, Training-Accuracy 1.09e-03, Validation-Accuracy 9.53e-04\n",
      "Epoch 14610, Training-Loss 5.40e+00, Validation-Loss 3.79e+00, Training-Accuracy 1.19e-03, Validation-Accuracy 1.09e-03\n",
      "Epoch 14620, Training-Loss 6.32e+00, Validation-Loss 4.96e+00, Training-Accuracy 1.35e-03, Validation-Accuracy 1.23e-03\n",
      "Epoch 14630, Training-Loss 4.34e+00, Validation-Loss 3.93e+00, Training-Accuracy 1.04e-03, Validation-Accuracy 1.23e-03\n",
      "Epoch 14640, Training-Loss 3.84e+00, Validation-Loss 4.69e+00, Training-Accuracy 1.15e-03, Validation-Accuracy 1.16e-03\n",
      "Epoch 14650, Training-Loss 3.03e+00, Validation-Loss 2.54e+00, Training-Accuracy 1.02e-03, Validation-Accuracy 8.78e-04\n",
      "Epoch 14660, Training-Loss 3.28e+00, Validation-Loss 1.71e+00, Training-Accuracy 9.74e-04, Validation-Accuracy 8.13e-04\n",
      "Epoch 14670, Training-Loss 5.37e+00, Validation-Loss 2.99e+00, Training-Accuracy 1.06e-03, Validation-Accuracy 1.03e-03\n",
      "Epoch 14680, Training-Loss 4.07e+00, Validation-Loss 2.76e+00, Training-Accuracy 1.19e-03, Validation-Accuracy 8.91e-04\n",
      "Epoch 14690, Training-Loss 3.94e+00, Validation-Loss 2.91e+00, Training-Accuracy 1.52e-03, Validation-Accuracy 1.21e-03\n",
      "Epoch 14700, Training-Loss 5.21e+00, Validation-Loss 2.89e+00, Training-Accuracy 1.08e-03, Validation-Accuracy 1.18e-03\n",
      "Epoch 14710, Training-Loss 4.58e+00, Validation-Loss 3.56e+00, Training-Accuracy 1.40e-03, Validation-Accuracy 1.30e-03\n",
      "Epoch 14720, Training-Loss 2.46e+00, Validation-Loss 4.40e+00, Training-Accuracy 9.55e-04, Validation-Accuracy 1.17e-03\n",
      "Epoch 14730, Training-Loss 6.44e+00, Validation-Loss 2.74e+00, Training-Accuracy 1.56e-03, Validation-Accuracy 9.50e-04\n",
      "Epoch 14740, Training-Loss 3.11e+00, Validation-Loss 4.82e+00, Training-Accuracy 9.59e-04, Validation-Accuracy 1.16e-03\n",
      "Epoch 14750, Training-Loss 2.55e+00, Validation-Loss 2.32e+00, Training-Accuracy 9.63e-04, Validation-Accuracy 9.69e-04\n",
      "Epoch 14760, Training-Loss 2.83e+00, Validation-Loss 3.20e+00, Training-Accuracy 9.84e-04, Validation-Accuracy 1.01e-03\n",
      "Epoch 14770, Training-Loss 3.30e+00, Validation-Loss 3.30e+00, Training-Accuracy 1.00e-03, Validation-Accuracy 1.11e-03\n",
      "Epoch 14780, Training-Loss 2.89e+00, Validation-Loss 2.48e+00, Training-Accuracy 9.79e-04, Validation-Accuracy 8.93e-04\n",
      "Epoch 14790, Training-Loss 2.42e+00, Validation-Loss 3.54e+00, Training-Accuracy 8.61e-04, Validation-Accuracy 9.90e-04\n",
      "Epoch 14800, Training-Loss 2.76e+00, Validation-Loss 2.77e+00, Training-Accuracy 1.09e-03, Validation-Accuracy 9.27e-04\n",
      "Epoch 14810, Training-Loss 3.54e+00, Validation-Loss 2.46e+00, Training-Accuracy 1.02e-03, Validation-Accuracy 9.40e-04\n",
      "Epoch 14820, Training-Loss 2.25e+00, Validation-Loss 2.24e+00, Training-Accuracy 1.01e-03, Validation-Accuracy 9.71e-04\n",
      "Epoch 14830, Training-Loss 2.11e+00, Validation-Loss 3.68e+00, Training-Accuracy 8.80e-04, Validation-Accuracy 1.40e-03\n",
      "Epoch 14840, Training-Loss 3.30e+00, Validation-Loss 2.81e+00, Training-Accuracy 1.20e-03, Validation-Accuracy 9.10e-04\n",
      "Epoch 14850, Training-Loss 3.68e+00, Validation-Loss 2.12e+00, Training-Accuracy 1.32e-03, Validation-Accuracy 9.25e-04\n",
      "Epoch 14860, Training-Loss 4.40e+00, Validation-Loss 3.09e+00, Training-Accuracy 1.15e-03, Validation-Accuracy 1.28e-03\n",
      "Epoch 14870, Training-Loss 2.45e+00, Validation-Loss 3.58e+00, Training-Accuracy 1.16e-03, Validation-Accuracy 1.46e-03\n",
      "Epoch 14880, Training-Loss 2.40e+00, Validation-Loss 3.31e+00, Training-Accuracy 9.53e-04, Validation-Accuracy 1.30e-03\n",
      "Epoch 14890, Training-Loss 2.40e+00, Validation-Loss 2.92e+00, Training-Accuracy 9.18e-04, Validation-Accuracy 8.99e-04\n",
      "Epoch 14900, Training-Loss 2.62e+00, Validation-Loss 2.78e+00, Training-Accuracy 9.66e-04, Validation-Accuracy 1.09e-03\n",
      "Epoch 14910, Training-Loss 2.01e+00, Validation-Loss 3.67e+00, Training-Accuracy 8.03e-04, Validation-Accuracy 1.03e-03\n",
      "Epoch 14920, Training-Loss 1.37e+00, Validation-Loss 4.46e+00, Training-Accuracy 8.43e-04, Validation-Accuracy 1.26e-03\n",
      "Epoch 14930, Training-Loss 3.24e+00, Validation-Loss 3.30e+00, Training-Accuracy 8.94e-04, Validation-Accuracy 1.14e-03\n",
      "Epoch 14940, Training-Loss 3.25e+00, Validation-Loss 3.48e+00, Training-Accuracy 9.75e-04, Validation-Accuracy 1.17e-03\n",
      "Epoch 14950, Training-Loss 2.30e+00, Validation-Loss 3.69e+00, Training-Accuracy 9.70e-04, Validation-Accuracy 1.43e-03\n",
      "Epoch 14960, Training-Loss 3.68e+00, Validation-Loss 3.25e+00, Training-Accuracy 1.07e-03, Validation-Accuracy 1.43e-03\n",
      "Epoch 14970, Training-Loss 2.93e+00, Validation-Loss 2.52e+00, Training-Accuracy 9.46e-04, Validation-Accuracy 1.16e-03\n",
      "Epoch 14980, Training-Loss 3.19e+00, Validation-Loss 2.43e+00, Training-Accuracy 1.07e-03, Validation-Accuracy 1.07e-03\n",
      "Epoch 14990, Training-Loss 2.58e+00, Validation-Loss 3.82e+00, Training-Accuracy 1.05e-03, Validation-Accuracy 1.34e-03\n",
      "Epoch 15000, Training-Loss 2.23e+00, Validation-Loss 3.09e+00, Training-Accuracy 8.23e-04, Validation-Accuracy 1.14e-03\n",
      "Epoch 15010, Training-Loss 3.00e+00, Validation-Loss 2.37e+00, Training-Accuracy 1.01e-03, Validation-Accuracy 1.07e-03\n",
      "Epoch 15020, Training-Loss 2.13e+00, Validation-Loss 2.85e+00, Training-Accuracy 8.73e-04, Validation-Accuracy 1.01e-03\n",
      "Epoch 15030, Training-Loss 2.11e+00, Validation-Loss 2.36e+00, Training-Accuracy 8.64e-04, Validation-Accuracy 9.85e-04\n",
      "Epoch 15040, Training-Loss 1.71e+00, Validation-Loss 2.78e+00, Training-Accuracy 7.92e-04, Validation-Accuracy 9.83e-04\n",
      "Epoch 15050, Training-Loss 3.75e+00, Validation-Loss 3.07e+00, Training-Accuracy 1.05e-03, Validation-Accuracy 1.17e-03\n",
      "Epoch 15060, Training-Loss 2.57e+00, Validation-Loss 2.75e+00, Training-Accuracy 1.12e-03, Validation-Accuracy 1.08e-03\n",
      "Epoch 15070, Training-Loss 2.32e+00, Validation-Loss 2.86e+00, Training-Accuracy 1.00e-03, Validation-Accuracy 9.43e-04\n",
      "Epoch 15080, Training-Loss 2.48e+00, Validation-Loss 2.03e+00, Training-Accuracy 1.02e-03, Validation-Accuracy 8.26e-04\n",
      "Epoch 15090, Training-Loss 2.38e+00, Validation-Loss 1.61e+00, Training-Accuracy 9.17e-04, Validation-Accuracy 8.44e-04\n",
      "Epoch 15100, Training-Loss 2.82e+00, Validation-Loss 2.01e+00, Training-Accuracy 1.14e-03, Validation-Accuracy 1.24e-03\n",
      "Epoch 15110, Training-Loss 2.45e+00, Validation-Loss 1.63e+00, Training-Accuracy 9.13e-04, Validation-Accuracy 1.07e-03\n",
      "Epoch 15120, Training-Loss 1.41e+00, Validation-Loss 1.38e+00, Training-Accuracy 9.83e-04, Validation-Accuracy 7.92e-04\n",
      "Epoch 15130, Training-Loss 2.03e+00, Validation-Loss 2.68e+00, Training-Accuracy 9.73e-04, Validation-Accuracy 9.96e-04\n",
      "Epoch 15140, Training-Loss 2.46e+00, Validation-Loss 1.72e+00, Training-Accuracy 9.28e-04, Validation-Accuracy 8.81e-04\n",
      "Epoch 15150, Training-Loss 2.45e+00, Validation-Loss 2.78e+00, Training-Accuracy 1.03e-03, Validation-Accuracy 1.07e-03\n",
      "Epoch 15160, Training-Loss 3.10e+00, Validation-Loss 2.05e+00, Training-Accuracy 9.55e-04, Validation-Accuracy 9.91e-04\n",
      "Epoch 15170, Training-Loss 2.05e+00, Validation-Loss 2.77e+00, Training-Accuracy 8.65e-04, Validation-Accuracy 1.02e-03\n",
      "Epoch 15180, Training-Loss 1.72e+00, Validation-Loss 2.49e+00, Training-Accuracy 7.74e-04, Validation-Accuracy 9.70e-04\n",
      "Epoch 15190, Training-Loss 2.15e+00, Validation-Loss 1.62e+00, Training-Accuracy 8.20e-04, Validation-Accuracy 9.12e-04\n",
      "Epoch 15200, Training-Loss 1.71e+00, Validation-Loss 2.51e+00, Training-Accuracy 7.92e-04, Validation-Accuracy 1.02e-03\n",
      "Epoch 15210, Training-Loss 2.35e+00, Validation-Loss 1.48e+00, Training-Accuracy 9.84e-04, Validation-Accuracy 7.42e-04\n",
      "Epoch 15220, Training-Loss 1.87e+00, Validation-Loss 2.06e+00, Training-Accuracy 8.42e-04, Validation-Accuracy 8.27e-04\n",
      "Epoch 15230, Training-Loss 2.31e+00, Validation-Loss 1.62e+00, Training-Accuracy 8.42e-04, Validation-Accuracy 7.82e-04\n",
      "Epoch 15240, Training-Loss 1.56e+00, Validation-Loss 2.10e+00, Training-Accuracy 8.96e-04, Validation-Accuracy 8.48e-04\n",
      "Epoch 15250, Training-Loss 1.90e+00, Validation-Loss 1.71e+00, Training-Accuracy 8.10e-04, Validation-Accuracy 8.06e-04\n",
      "Epoch 15260, Training-Loss 1.65e+00, Validation-Loss 2.27e+00, Training-Accuracy 8.39e-04, Validation-Accuracy 1.05e-03\n",
      "Epoch 15270, Training-Loss 1.19e+00, Validation-Loss 2.14e+00, Training-Accuracy 7.55e-04, Validation-Accuracy 8.90e-04\n",
      "Epoch 15280, Training-Loss 2.08e+00, Validation-Loss 1.72e+00, Training-Accuracy 8.49e-04, Validation-Accuracy 8.58e-04\n",
      "Epoch 15290, Training-Loss 1.29e+00, Validation-Loss 1.57e+00, Training-Accuracy 7.44e-04, Validation-Accuracy 9.02e-04\n",
      "Epoch 15300, Training-Loss 1.85e+00, Validation-Loss 1.26e+00, Training-Accuracy 8.82e-04, Validation-Accuracy 8.59e-04\n",
      "Epoch 15310, Training-Loss 1.84e+00, Validation-Loss 1.93e+00, Training-Accuracy 1.07e-03, Validation-Accuracy 8.87e-04\n",
      "Epoch 15320, Training-Loss 1.81e+00, Validation-Loss 2.93e+00, Training-Accuracy 1.10e-03, Validation-Accuracy 1.39e-03\n",
      "Epoch 15330, Training-Loss 1.81e+00, Validation-Loss 1.96e+00, Training-Accuracy 8.48e-04, Validation-Accuracy 8.62e-04\n",
      "Epoch 15340, Training-Loss 2.07e+00, Validation-Loss 1.24e+00, Training-Accuracy 8.38e-04, Validation-Accuracy 8.54e-04\n",
      "Epoch 15350, Training-Loss 1.43e+00, Validation-Loss 1.51e+00, Training-Accuracy 8.15e-04, Validation-Accuracy 8.25e-04\n",
      "Epoch 15360, Training-Loss 2.68e+00, Validation-Loss 2.55e+00, Training-Accuracy 1.05e-03, Validation-Accuracy 9.37e-04\n",
      "Epoch 15370, Training-Loss 1.35e+00, Validation-Loss 1.32e+00, Training-Accuracy 7.93e-04, Validation-Accuracy 6.86e-04\n",
      "Epoch 15380, Training-Loss 2.04e+00, Validation-Loss 1.17e+00, Training-Accuracy 8.11e-04, Validation-Accuracy 9.21e-04\n",
      "Epoch 15390, Training-Loss 2.06e+00, Validation-Loss 1.70e+00, Training-Accuracy 1.03e-03, Validation-Accuracy 7.97e-04\n",
      "Epoch 15400, Training-Loss 2.27e+00, Validation-Loss 2.38e+00, Training-Accuracy 1.25e-03, Validation-Accuracy 1.16e-03\n",
      "Epoch 15410, Training-Loss 1.56e+00, Validation-Loss 1.85e+00, Training-Accuracy 8.15e-04, Validation-Accuracy 9.07e-04\n",
      "Epoch 15420, Training-Loss 1.41e+00, Validation-Loss 1.40e+00, Training-Accuracy 7.80e-04, Validation-Accuracy 7.24e-04\n",
      "Epoch 15430, Training-Loss 1.66e+00, Validation-Loss 2.01e+00, Training-Accuracy 7.82e-04, Validation-Accuracy 8.35e-04\n",
      "Epoch 15440, Training-Loss 1.30e+00, Validation-Loss 1.27e+00, Training-Accuracy 7.41e-04, Validation-Accuracy 7.26e-04\n",
      "Epoch 15450, Training-Loss 1.60e+00, Validation-Loss 2.26e+00, Training-Accuracy 8.20e-04, Validation-Accuracy 9.33e-04\n",
      "Epoch 15460, Training-Loss 1.61e+00, Validation-Loss 2.61e+00, Training-Accuracy 9.29e-04, Validation-Accuracy 1.22e-03\n",
      "Epoch 15470, Training-Loss 1.71e+00, Validation-Loss 2.09e+00, Training-Accuracy 7.85e-04, Validation-Accuracy 8.97e-04\n",
      "Epoch 15480, Training-Loss 2.59e+00, Validation-Loss 1.25e+00, Training-Accuracy 1.08e-03, Validation-Accuracy 8.08e-04\n",
      "Epoch 15490, Training-Loss 1.82e+00, Validation-Loss 1.59e+00, Training-Accuracy 8.86e-04, Validation-Accuracy 9.65e-04\n",
      "Epoch 15500, Training-Loss 1.40e+00, Validation-Loss 2.15e+00, Training-Accuracy 7.41e-04, Validation-Accuracy 1.22e-03\n",
      "Epoch 15510, Training-Loss 1.63e+00, Validation-Loss 1.73e+00, Training-Accuracy 9.68e-04, Validation-Accuracy 8.05e-04\n",
      "Epoch 15520, Training-Loss 1.55e+00, Validation-Loss 1.81e+00, Training-Accuracy 8.54e-04, Validation-Accuracy 1.13e-03\n",
      "Epoch 15530, Training-Loss 1.57e+00, Validation-Loss 1.63e+00, Training-Accuracy 9.13e-04, Validation-Accuracy 7.92e-04\n",
      "Epoch 15540, Training-Loss 1.84e+00, Validation-Loss 1.48e+00, Training-Accuracy 8.98e-04, Validation-Accuracy 7.71e-04\n",
      "Epoch 15550, Training-Loss 1.69e+00, Validation-Loss 1.57e+00, Training-Accuracy 8.93e-04, Validation-Accuracy 8.34e-04\n",
      "Epoch 15560, Training-Loss 2.06e+00, Validation-Loss 1.18e+00, Training-Accuracy 8.46e-04, Validation-Accuracy 7.81e-04\n",
      "Epoch 15570, Training-Loss 1.54e+00, Validation-Loss 1.52e+00, Training-Accuracy 8.35e-04, Validation-Accuracy 8.57e-04\n",
      "Epoch 15580, Training-Loss 1.04e+00, Validation-Loss 1.51e+00, Training-Accuracy 6.55e-04, Validation-Accuracy 8.31e-04\n",
      "Epoch 15590, Training-Loss 1.61e+00, Validation-Loss 1.83e+00, Training-Accuracy 8.22e-04, Validation-Accuracy 8.10e-04\n",
      "Epoch 15600, Training-Loss 1.35e+00, Validation-Loss 2.09e+00, Training-Accuracy 7.37e-04, Validation-Accuracy 1.12e-03\n",
      "Epoch 15610, Training-Loss 1.15e+00, Validation-Loss 1.74e+00, Training-Accuracy 8.65e-04, Validation-Accuracy 9.49e-04\n",
      "Epoch 15620, Training-Loss 1.38e+00, Validation-Loss 8.64e-01, Training-Accuracy 8.71e-04, Validation-Accuracy 7.46e-04\n",
      "Epoch 15630, Training-Loss 1.73e+00, Validation-Loss 1.28e+00, Training-Accuracy 8.98e-04, Validation-Accuracy 7.89e-04\n",
      "Epoch 15640, Training-Loss 1.13e+00, Validation-Loss 1.88e+00, Training-Accuracy 8.03e-04, Validation-Accuracy 9.50e-04\n",
      "Epoch 15650, Training-Loss 1.49e+00, Validation-Loss 1.57e+00, Training-Accuracy 7.83e-04, Validation-Accuracy 8.85e-04\n",
      "Epoch 15660, Training-Loss 1.52e+00, Validation-Loss 1.94e+00, Training-Accuracy 7.87e-04, Validation-Accuracy 9.88e-04\n",
      "Epoch 15670, Training-Loss 1.54e+00, Validation-Loss 9.91e-01, Training-Accuracy 7.84e-04, Validation-Accuracy 7.36e-04\n",
      "Epoch 15680, Training-Loss 1.34e+00, Validation-Loss 1.03e+00, Training-Accuracy 8.57e-04, Validation-Accuracy 7.45e-04\n",
      "Epoch 15690, Training-Loss 2.04e+00, Validation-Loss 1.52e+00, Training-Accuracy 1.02e-03, Validation-Accuracy 9.68e-04\n",
      "Epoch 15700, Training-Loss 1.55e+00, Validation-Loss 1.38e+00, Training-Accuracy 7.62e-04, Validation-Accuracy 7.93e-04\n",
      "Epoch 15710, Training-Loss 1.49e+00, Validation-Loss 1.77e+00, Training-Accuracy 7.96e-04, Validation-Accuracy 8.97e-04\n",
      "Epoch 15720, Training-Loss 1.03e+00, Validation-Loss 1.52e+00, Training-Accuracy 7.17e-04, Validation-Accuracy 7.57e-04\n",
      "Epoch 15730, Training-Loss 1.72e+00, Validation-Loss 1.33e+00, Training-Accuracy 8.10e-04, Validation-Accuracy 7.64e-04\n",
      "Epoch 15740, Training-Loss 1.26e+00, Validation-Loss 1.96e+00, Training-Accuracy 9.38e-04, Validation-Accuracy 9.36e-04\n",
      "Epoch 15750, Training-Loss 1.48e+00, Validation-Loss 9.51e-01, Training-Accuracy 7.99e-04, Validation-Accuracy 7.17e-04\n",
      "Epoch 15760, Training-Loss 1.21e+00, Validation-Loss 8.33e-01, Training-Accuracy 8.68e-04, Validation-Accuracy 6.82e-04\n",
      "Epoch 15770, Training-Loss 1.03e+00, Validation-Loss 1.18e+00, Training-Accuracy 7.24e-04, Validation-Accuracy 7.24e-04\n",
      "Epoch 15780, Training-Loss 1.07e+00, Validation-Loss 1.49e+00, Training-Accuracy 6.66e-04, Validation-Accuracy 7.83e-04\n",
      "Epoch 15790, Training-Loss 1.26e+00, Validation-Loss 1.27e+00, Training-Accuracy 9.64e-04, Validation-Accuracy 8.35e-04\n",
      "Epoch 15800, Training-Loss 1.07e+00, Validation-Loss 1.23e+00, Training-Accuracy 6.65e-04, Validation-Accuracy 7.70e-04\n",
      "Epoch 15810, Training-Loss 1.57e+00, Validation-Loss 1.33e+00, Training-Accuracy 7.75e-04, Validation-Accuracy 7.91e-04\n",
      "Epoch 15820, Training-Loss 1.41e+00, Validation-Loss 1.56e+00, Training-Accuracy 8.97e-04, Validation-Accuracy 8.93e-04\n",
      "Epoch 15830, Training-Loss 1.48e+00, Validation-Loss 1.11e+00, Training-Accuracy 1.02e-03, Validation-Accuracy 9.64e-04\n",
      "Epoch 15840, Training-Loss 1.29e+00, Validation-Loss 2.08e+00, Training-Accuracy 7.32e-04, Validation-Accuracy 1.27e-03\n",
      "Epoch 15850, Training-Loss 1.82e+00, Validation-Loss 1.38e+00, Training-Accuracy 1.08e-03, Validation-Accuracy 9.91e-04\n",
      "Epoch 15860, Training-Loss 8.42e-01, Validation-Loss 1.49e+00, Training-Accuracy 7.38e-04, Validation-Accuracy 9.73e-04\n",
      "Epoch 15870, Training-Loss 9.07e-01, Validation-Loss 7.45e-01, Training-Accuracy 6.83e-04, Validation-Accuracy 7.04e-04\n",
      "Epoch 15880, Training-Loss 1.15e+00, Validation-Loss 1.07e+00, Training-Accuracy 7.64e-04, Validation-Accuracy 7.90e-04\n",
      "Epoch 15890, Training-Loss 8.72e-01, Validation-Loss 1.11e+00, Training-Accuracy 7.04e-04, Validation-Accuracy 6.90e-04\n",
      "Epoch 15900, Training-Loss 1.34e+00, Validation-Loss 1.56e+00, Training-Accuracy 8.20e-04, Validation-Accuracy 7.51e-04\n",
      "Epoch 15910, Training-Loss 1.43e+00, Validation-Loss 1.49e+00, Training-Accuracy 9.12e-04, Validation-Accuracy 8.72e-04\n",
      "Epoch 15920, Training-Loss 9.24e-01, Validation-Loss 1.12e+00, Training-Accuracy 7.71e-04, Validation-Accuracy 7.62e-04\n",
      "Epoch 15930, Training-Loss 9.37e-01, Validation-Loss 1.32e+00, Training-Accuracy 6.60e-04, Validation-Accuracy 8.02e-04\n",
      "Epoch 15940, Training-Loss 9.94e-01, Validation-Loss 1.16e+00, Training-Accuracy 6.96e-04, Validation-Accuracy 7.25e-04\n",
      "Epoch 15950, Training-Loss 1.01e+00, Validation-Loss 6.84e-01, Training-Accuracy 7.44e-04, Validation-Accuracy 7.04e-04\n",
      "Epoch 15960, Training-Loss 9.30e-01, Validation-Loss 1.03e+00, Training-Accuracy 7.74e-04, Validation-Accuracy 7.16e-04\n",
      "Epoch 15970, Training-Loss 1.24e+00, Validation-Loss 1.37e+00, Training-Accuracy 7.32e-04, Validation-Accuracy 7.31e-04\n",
      "Epoch 15980, Training-Loss 9.52e-01, Validation-Loss 1.07e+00, Training-Accuracy 7.18e-04, Validation-Accuracy 8.18e-04\n",
      "Epoch 15990, Training-Loss 8.26e-01, Validation-Loss 1.16e+00, Training-Accuracy 6.66e-04, Validation-Accuracy 7.24e-04\n",
      "Epoch 16000, Training-Loss 1.38e+00, Validation-Loss 9.86e-01, Training-Accuracy 8.62e-04, Validation-Accuracy 8.55e-04\n",
      "Epoch 16010, Training-Loss 1.05e+00, Validation-Loss 1.05e+00, Training-Accuracy 7.69e-04, Validation-Accuracy 7.17e-04\n",
      "Epoch 16020, Training-Loss 1.07e+00, Validation-Loss 9.55e-01, Training-Accuracy 6.83e-04, Validation-Accuracy 7.07e-04\n",
      "Epoch 16030, Training-Loss 9.84e-01, Validation-Loss 9.65e-01, Training-Accuracy 6.67e-04, Validation-Accuracy 7.22e-04\n",
      "Epoch 16040, Training-Loss 8.30e-01, Validation-Loss 1.30e+00, Training-Accuracy 7.24e-04, Validation-Accuracy 9.46e-04\n",
      "Epoch 16050, Training-Loss 1.41e+00, Validation-Loss 7.59e-01, Training-Accuracy 7.29e-04, Validation-Accuracy 6.86e-04\n",
      "Epoch 16060, Training-Loss 1.14e+00, Validation-Loss 1.21e+00, Training-Accuracy 8.28e-04, Validation-Accuracy 8.89e-04\n",
      "Epoch 16070, Training-Loss 1.08e+00, Validation-Loss 8.74e-01, Training-Accuracy 9.25e-04, Validation-Accuracy 6.64e-04\n",
      "Epoch 16080, Training-Loss 1.29e+00, Validation-Loss 1.05e+00, Training-Accuracy 7.28e-04, Validation-Accuracy 7.98e-04\n",
      "Epoch 16090, Training-Loss 8.78e-01, Validation-Loss 1.05e+00, Training-Accuracy 6.60e-04, Validation-Accuracy 7.82e-04\n",
      "Epoch 16100, Training-Loss 1.34e+00, Validation-Loss 1.26e+00, Training-Accuracy 7.71e-04, Validation-Accuracy 7.81e-04\n",
      "Epoch 16110, Training-Loss 9.56e-01, Validation-Loss 1.02e+00, Training-Accuracy 7.11e-04, Validation-Accuracy 6.99e-04\n",
      "Epoch 16120, Training-Loss 9.00e-01, Validation-Loss 1.37e+00, Training-Accuracy 7.47e-04, Validation-Accuracy 8.76e-04\n",
      "Epoch 16130, Training-Loss 1.25e+00, Validation-Loss 1.15e+00, Training-Accuracy 1.07e-03, Validation-Accuracy 7.78e-04\n",
      "Epoch 16140, Training-Loss 1.52e+00, Validation-Loss 9.40e-01, Training-Accuracy 1.22e-03, Validation-Accuracy 7.65e-04\n",
      "Epoch 16150, Training-Loss 1.40e+00, Validation-Loss 1.45e+00, Training-Accuracy 7.78e-04, Validation-Accuracy 1.10e-03\n",
      "Epoch 16160, Training-Loss 8.63e-01, Validation-Loss 1.25e+00, Training-Accuracy 6.93e-04, Validation-Accuracy 7.55e-04\n",
      "Epoch 16170, Training-Loss 1.18e+00, Validation-Loss 7.49e-01, Training-Accuracy 7.85e-04, Validation-Accuracy 6.88e-04\n",
      "Epoch 16180, Training-Loss 1.14e+00, Validation-Loss 9.20e-01, Training-Accuracy 7.07e-04, Validation-Accuracy 6.22e-04\n",
      "Epoch 16190, Training-Loss 1.01e+00, Validation-Loss 7.42e-01, Training-Accuracy 7.33e-04, Validation-Accuracy 6.95e-04\n",
      "Epoch 16200, Training-Loss 1.26e+00, Validation-Loss 8.51e-01, Training-Accuracy 8.06e-04, Validation-Accuracy 7.81e-04\n",
      "Epoch 16210, Training-Loss 1.15e+00, Validation-Loss 7.92e-01, Training-Accuracy 8.81e-04, Validation-Accuracy 7.38e-04\n",
      "Epoch 16220, Training-Loss 6.67e-01, Validation-Loss 5.87e-01, Training-Accuracy 5.92e-04, Validation-Accuracy 6.30e-04\n",
      "Epoch 16230, Training-Loss 1.15e+00, Validation-Loss 6.40e-01, Training-Accuracy 9.56e-04, Validation-Accuracy 6.59e-04\n",
      "Epoch 16240, Training-Loss 1.06e+00, Validation-Loss 1.08e+00, Training-Accuracy 7.06e-04, Validation-Accuracy 7.68e-04\n",
      "Epoch 16250, Training-Loss 9.06e-01, Validation-Loss 1.22e+00, Training-Accuracy 7.55e-04, Validation-Accuracy 8.93e-04\n",
      "Epoch 16260, Training-Loss 9.01e-01, Validation-Loss 1.11e+00, Training-Accuracy 7.74e-04, Validation-Accuracy 8.91e-04\n",
      "Epoch 16270, Training-Loss 1.13e+00, Validation-Loss 8.04e-01, Training-Accuracy 7.93e-04, Validation-Accuracy 6.87e-04\n",
      "Epoch 16280, Training-Loss 1.15e+00, Validation-Loss 9.76e-01, Training-Accuracy 7.19e-04, Validation-Accuracy 7.61e-04\n",
      "Epoch 16290, Training-Loss 7.15e-01, Validation-Loss 8.33e-01, Training-Accuracy 6.63e-04, Validation-Accuracy 6.40e-04\n",
      "Epoch 16300, Training-Loss 1.07e+00, Validation-Loss 9.80e-01, Training-Accuracy 6.53e-04, Validation-Accuracy 7.60e-04\n",
      "Epoch 16310, Training-Loss 7.78e-01, Validation-Loss 1.18e+00, Training-Accuracy 6.62e-04, Validation-Accuracy 8.24e-04\n",
      "Epoch 16320, Training-Loss 7.89e-01, Validation-Loss 1.00e+00, Training-Accuracy 7.63e-04, Validation-Accuracy 7.55e-04\n",
      "Epoch 16330, Training-Loss 1.25e+00, Validation-Loss 1.04e+00, Training-Accuracy 1.11e-03, Validation-Accuracy 8.25e-04\n",
      "Epoch 16340, Training-Loss 1.41e+00, Validation-Loss 8.90e-01, Training-Accuracy 1.06e-03, Validation-Accuracy 7.69e-04\n",
      "Epoch 16350, Training-Loss 1.09e+00, Validation-Loss 9.39e-01, Training-Accuracy 7.23e-04, Validation-Accuracy 6.91e-04\n",
      "Epoch 16360, Training-Loss 1.18e+00, Validation-Loss 6.87e-01, Training-Accuracy 7.41e-04, Validation-Accuracy 6.34e-04\n",
      "Epoch 16370, Training-Loss 8.41e-01, Validation-Loss 1.09e+00, Training-Accuracy 6.26e-04, Validation-Accuracy 7.61e-04\n",
      "Epoch 16380, Training-Loss 8.69e-01, Validation-Loss 8.15e-01, Training-Accuracy 7.36e-04, Validation-Accuracy 7.42e-04\n",
      "Epoch 16390, Training-Loss 1.27e+00, Validation-Loss 9.49e-01, Training-Accuracy 8.30e-04, Validation-Accuracy 7.31e-04\n",
      "Epoch 16400, Training-Loss 7.74e-01, Validation-Loss 1.22e+00, Training-Accuracy 7.30e-04, Validation-Accuracy 8.64e-04\n",
      "Epoch 16410, Training-Loss 1.29e+00, Validation-Loss 7.03e-01, Training-Accuracy 8.03e-04, Validation-Accuracy 6.70e-04\n",
      "Epoch 16420, Training-Loss 1.20e+00, Validation-Loss 1.50e+00, Training-Accuracy 1.03e-03, Validation-Accuracy 1.12e-03\n",
      "Epoch 16430, Training-Loss 1.05e+00, Validation-Loss 9.04e-01, Training-Accuracy 9.25e-04, Validation-Accuracy 6.64e-04\n",
      "Epoch 16440, Training-Loss 6.07e-01, Validation-Loss 9.85e-01, Training-Accuracy 5.86e-04, Validation-Accuracy 7.70e-04\n",
      "Epoch 16450, Training-Loss 1.04e+00, Validation-Loss 1.12e+00, Training-Accuracy 7.10e-04, Validation-Accuracy 7.21e-04\n",
      "Epoch 16460, Training-Loss 1.25e+00, Validation-Loss 8.78e-01, Training-Accuracy 9.02e-04, Validation-Accuracy 8.39e-04\n",
      "Epoch 16470, Training-Loss 7.30e-01, Validation-Loss 8.64e-01, Training-Accuracy 6.92e-04, Validation-Accuracy 7.13e-04\n",
      "Epoch 16480, Training-Loss 9.56e-01, Validation-Loss 1.10e+00, Training-Accuracy 8.41e-04, Validation-Accuracy 9.22e-04\n",
      "Epoch 16490, Training-Loss 1.64e+00, Validation-Loss 1.63e+00, Training-Accuracy 1.32e-03, Validation-Accuracy 1.24e-03\n",
      "Epoch 16500, Training-Loss 6.65e-01, Validation-Loss 8.25e-01, Training-Accuracy 6.14e-04, Validation-Accuracy 6.51e-04\n",
      "Epoch 16510, Training-Loss 5.93e-01, Validation-Loss 1.00e+00, Training-Accuracy 5.85e-04, Validation-Accuracy 7.00e-04\n",
      "Epoch 16520, Training-Loss 1.43e+00, Validation-Loss 1.51e+00, Training-Accuracy 1.15e-03, Validation-Accuracy 1.21e-03\n",
      "Epoch 16530, Training-Loss 8.11e-01, Validation-Loss 7.00e-01, Training-Accuracy 7.05e-04, Validation-Accuracy 6.58e-04\n",
      "Epoch 16540, Training-Loss 7.95e-01, Validation-Loss 9.93e-01, Training-Accuracy 6.96e-04, Validation-Accuracy 8.31e-04\n",
      "Epoch 16550, Training-Loss 1.09e+00, Validation-Loss 9.42e-01, Training-Accuracy 7.59e-04, Validation-Accuracy 8.23e-04\n",
      "Epoch 16560, Training-Loss 7.45e-01, Validation-Loss 6.80e-01, Training-Accuracy 6.14e-04, Validation-Accuracy 6.54e-04\n",
      "Epoch 16570, Training-Loss 9.93e-01, Validation-Loss 7.95e-01, Training-Accuracy 7.07e-04, Validation-Accuracy 6.71e-04\n",
      "Epoch 16580, Training-Loss 8.25e-01, Validation-Loss 8.01e-01, Training-Accuracy 6.65e-04, Validation-Accuracy 6.28e-04\n",
      "Epoch 16590, Training-Loss 5.95e-01, Validation-Loss 1.04e+00, Training-Accuracy 6.50e-04, Validation-Accuracy 7.33e-04\n",
      "Epoch 16600, Training-Loss 1.20e+00, Validation-Loss 1.06e+00, Training-Accuracy 9.92e-04, Validation-Accuracy 8.58e-04\n",
      "Epoch 16610, Training-Loss 5.73e-01, Validation-Loss 8.64e-01, Training-Accuracy 6.07e-04, Validation-Accuracy 7.26e-04\n",
      "Epoch 16620, Training-Loss 7.26e-01, Validation-Loss 9.57e-01, Training-Accuracy 6.10e-04, Validation-Accuracy 6.44e-04\n",
      "Epoch 16630, Training-Loss 7.31e-01, Validation-Loss 7.09e-01, Training-Accuracy 6.07e-04, Validation-Accuracy 6.75e-04\n",
      "Epoch 16640, Training-Loss 8.15e-01, Validation-Loss 7.60e-01, Training-Accuracy 7.11e-04, Validation-Accuracy 6.34e-04\n",
      "Epoch 16650, Training-Loss 9.20e-01, Validation-Loss 7.81e-01, Training-Accuracy 7.80e-04, Validation-Accuracy 6.23e-04\n",
      "Epoch 16660, Training-Loss 8.24e-01, Validation-Loss 9.62e-01, Training-Accuracy 7.30e-04, Validation-Accuracy 7.97e-04\n",
      "Epoch 16670, Training-Loss 9.99e-01, Validation-Loss 1.05e+00, Training-Accuracy 7.07e-04, Validation-Accuracy 9.68e-04\n",
      "Epoch 16680, Training-Loss 5.82e-01, Validation-Loss 6.11e-01, Training-Accuracy 5.81e-04, Validation-Accuracy 6.42e-04\n",
      "Epoch 16690, Training-Loss 8.50e-01, Validation-Loss 8.41e-01, Training-Accuracy 6.44e-04, Validation-Accuracy 7.12e-04\n",
      "Epoch 16700, Training-Loss 8.80e-01, Validation-Loss 6.30e-01, Training-Accuracy 8.35e-04, Validation-Accuracy 7.28e-04\n",
      "Epoch 16710, Training-Loss 7.91e-01, Validation-Loss 5.74e-01, Training-Accuracy 8.14e-04, Validation-Accuracy 5.87e-04\n",
      "Epoch 16720, Training-Loss 9.41e-01, Validation-Loss 6.68e-01, Training-Accuracy 6.79e-04, Validation-Accuracy 6.03e-04\n",
      "Epoch 16730, Training-Loss 8.24e-01, Validation-Loss 9.70e-01, Training-Accuracy 8.71e-04, Validation-Accuracy 8.01e-04\n",
      "Epoch 16740, Training-Loss 1.18e+00, Validation-Loss 1.01e+00, Training-Accuracy 9.85e-04, Validation-Accuracy 8.98e-04\n",
      "Epoch 16750, Training-Loss 5.37e-01, Validation-Loss 7.58e-01, Training-Accuracy 5.59e-04, Validation-Accuracy 6.61e-04\n",
      "Epoch 16760, Training-Loss 1.53e+00, Validation-Loss 9.30e-01, Training-Accuracy 1.15e-03, Validation-Accuracy 8.45e-04\n",
      "Epoch 16770, Training-Loss 8.91e-01, Validation-Loss 9.97e-01, Training-Accuracy 7.40e-04, Validation-Accuracy 8.82e-04\n",
      "Epoch 16780, Training-Loss 8.74e-01, Validation-Loss 7.06e-01, Training-Accuracy 7.39e-04, Validation-Accuracy 6.07e-04\n",
      "Epoch 16790, Training-Loss 8.45e-01, Validation-Loss 5.96e-01, Training-Accuracy 8.19e-04, Validation-Accuracy 5.87e-04\n",
      "Epoch 16800, Training-Loss 9.14e-01, Validation-Loss 8.61e-01, Training-Accuracy 6.83e-04, Validation-Accuracy 6.65e-04\n",
      "Epoch 16810, Training-Loss 7.00e-01, Validation-Loss 8.71e-01, Training-Accuracy 6.28e-04, Validation-Accuracy 7.37e-04\n",
      "Epoch 16820, Training-Loss 1.07e+00, Validation-Loss 9.29e-01, Training-Accuracy 8.46e-04, Validation-Accuracy 7.40e-04\n",
      "Epoch 16830, Training-Loss 6.52e-01, Validation-Loss 6.52e-01, Training-Accuracy 6.14e-04, Validation-Accuracy 6.19e-04\n",
      "Epoch 16840, Training-Loss 7.15e-01, Validation-Loss 5.68e-01, Training-Accuracy 5.74e-04, Validation-Accuracy 5.55e-04\n",
      "Epoch 16850, Training-Loss 7.90e-01, Validation-Loss 6.04e-01, Training-Accuracy 6.88e-04, Validation-Accuracy 6.06e-04\n",
      "Epoch 16860, Training-Loss 8.91e-01, Validation-Loss 1.25e+00, Training-Accuracy 7.79e-04, Validation-Accuracy 1.09e-03\n",
      "Epoch 16870, Training-Loss 1.06e+00, Validation-Loss 7.85e-01, Training-Accuracy 8.73e-04, Validation-Accuracy 6.90e-04\n",
      "Epoch 16880, Training-Loss 8.70e-01, Validation-Loss 8.91e-01, Training-Accuracy 8.80e-04, Validation-Accuracy 8.03e-04\n",
      "Epoch 16890, Training-Loss 1.17e+00, Validation-Loss 1.07e+00, Training-Accuracy 1.08e-03, Validation-Accuracy 9.73e-04\n",
      "Epoch 16900, Training-Loss 1.04e+00, Validation-Loss 6.88e-01, Training-Accuracy 8.94e-04, Validation-Accuracy 5.87e-04\n",
      "Epoch 16910, Training-Loss 1.07e+00, Validation-Loss 7.25e-01, Training-Accuracy 7.79e-04, Validation-Accuracy 6.65e-04\n",
      "Epoch 16920, Training-Loss 6.97e-01, Validation-Loss 5.01e-01, Training-Accuracy 7.70e-04, Validation-Accuracy 5.87e-04\n",
      "Epoch 16930, Training-Loss 9.21e-01, Validation-Loss 7.52e-01, Training-Accuracy 7.90e-04, Validation-Accuracy 8.31e-04\n",
      "Epoch 16940, Training-Loss 1.15e+00, Validation-Loss 8.79e-01, Training-Accuracy 1.06e-03, Validation-Accuracy 8.73e-04\n",
      "Epoch 16950, Training-Loss 7.99e-01, Validation-Loss 8.68e-01, Training-Accuracy 7.07e-04, Validation-Accuracy 7.97e-04\n",
      "Epoch 16960, Training-Loss 7.05e-01, Validation-Loss 6.51e-01, Training-Accuracy 7.34e-04, Validation-Accuracy 6.97e-04\n",
      "Epoch 16970, Training-Loss 9.56e-01, Validation-Loss 5.85e-01, Training-Accuracy 7.41e-04, Validation-Accuracy 5.66e-04\n",
      "Epoch 16980, Training-Loss 6.24e-01, Validation-Loss 7.29e-01, Training-Accuracy 6.16e-04, Validation-Accuracy 6.30e-04\n",
      "Epoch 16990, Training-Loss 9.39e-01, Validation-Loss 7.00e-01, Training-Accuracy 8.66e-04, Validation-Accuracy 6.25e-04\n",
      "Epoch 17000, Training-Loss 8.15e-01, Validation-Loss 1.25e+00, Training-Accuracy 7.77e-04, Validation-Accuracy 9.34e-04\n",
      "Epoch 17010, Training-Loss 1.27e+00, Validation-Loss 5.66e-01, Training-Accuracy 1.04e-03, Validation-Accuracy 6.24e-04\n",
      "Epoch 17020, Training-Loss 1.32e+00, Validation-Loss 1.63e+00, Training-Accuracy 1.01e-03, Validation-Accuracy 1.30e-03\n",
      "Epoch 17030, Training-Loss 8.75e-01, Validation-Loss 8.10e-01, Training-Accuracy 7.79e-04, Validation-Accuracy 6.40e-04\n",
      "Epoch 17040, Training-Loss 6.05e-01, Validation-Loss 7.20e-01, Training-Accuracy 5.92e-04, Validation-Accuracy 6.22e-04\n",
      "Epoch 17050, Training-Loss 6.88e-01, Validation-Loss 9.89e-01, Training-Accuracy 7.28e-04, Validation-Accuracy 8.19e-04\n",
      "Epoch 17060, Training-Loss 5.74e-01, Validation-Loss 1.13e+00, Training-Accuracy 5.95e-04, Validation-Accuracy 8.68e-04\n",
      "Epoch 17070, Training-Loss 1.29e+00, Validation-Loss 1.02e+00, Training-Accuracy 1.17e-03, Validation-Accuracy 8.63e-04\n",
      "Epoch 17080, Training-Loss 7.16e-01, Validation-Loss 6.21e-01, Training-Accuracy 6.62e-04, Validation-Accuracy 5.99e-04\n",
      "Epoch 17090, Training-Loss 7.43e-01, Validation-Loss 6.22e-01, Training-Accuracy 6.75e-04, Validation-Accuracy 5.84e-04\n",
      "Epoch 17100, Training-Loss 7.80e-01, Validation-Loss 9.94e-01, Training-Accuracy 6.46e-04, Validation-Accuracy 8.46e-04\n",
      "Epoch 17110, Training-Loss 1.29e+00, Validation-Loss 7.75e-01, Training-Accuracy 1.12e-03, Validation-Accuracy 6.92e-04\n",
      "Epoch 17120, Training-Loss 6.96e-01, Validation-Loss 7.38e-01, Training-Accuracy 6.87e-04, Validation-Accuracy 8.45e-04\n",
      "Epoch 17130, Training-Loss 6.68e-01, Validation-Loss 6.95e-01, Training-Accuracy 6.41e-04, Validation-Accuracy 7.33e-04\n",
      "Epoch 17140, Training-Loss 6.77e-01, Validation-Loss 8.28e-01, Training-Accuracy 6.51e-04, Validation-Accuracy 7.49e-04\n",
      "Epoch 17150, Training-Loss 6.91e-01, Validation-Loss 7.64e-01, Training-Accuracy 6.37e-04, Validation-Accuracy 7.00e-04\n",
      "Epoch 17160, Training-Loss 8.72e-01, Validation-Loss 8.79e-01, Training-Accuracy 7.22e-04, Validation-Accuracy 8.24e-04\n",
      "Epoch 17170, Training-Loss 7.05e-01, Validation-Loss 8.52e-01, Training-Accuracy 6.72e-04, Validation-Accuracy 7.35e-04\n",
      "Epoch 17180, Training-Loss 6.46e-01, Validation-Loss 7.30e-01, Training-Accuracy 5.84e-04, Validation-Accuracy 6.29e-04\n",
      "Epoch 17190, Training-Loss 6.38e-01, Validation-Loss 6.76e-01, Training-Accuracy 5.93e-04, Validation-Accuracy 6.58e-04\n",
      "Epoch 17200, Training-Loss 7.67e-01, Validation-Loss 7.98e-01, Training-Accuracy 8.12e-04, Validation-Accuracy 7.36e-04\n",
      "Epoch 17210, Training-Loss 6.96e-01, Validation-Loss 7.54e-01, Training-Accuracy 7.01e-04, Validation-Accuracy 6.90e-04\n",
      "Epoch 17220, Training-Loss 8.57e-01, Validation-Loss 7.91e-01, Training-Accuracy 6.55e-04, Validation-Accuracy 7.62e-04\n",
      "Epoch 17230, Training-Loss 8.02e-01, Validation-Loss 6.05e-01, Training-Accuracy 7.23e-04, Validation-Accuracy 6.11e-04\n",
      "Epoch 17240, Training-Loss 6.32e-01, Validation-Loss 7.39e-01, Training-Accuracy 5.65e-04, Validation-Accuracy 6.63e-04\n",
      "Epoch 17250, Training-Loss 6.87e-01, Validation-Loss 6.74e-01, Training-Accuracy 6.37e-04, Validation-Accuracy 6.95e-04\n",
      "Epoch 17260, Training-Loss 7.71e-01, Validation-Loss 6.45e-01, Training-Accuracy 6.24e-04, Validation-Accuracy 6.11e-04\n",
      "Epoch 17270, Training-Loss 1.13e+00, Validation-Loss 1.14e+00, Training-Accuracy 7.80e-04, Validation-Accuracy 1.04e-03\n",
      "Epoch 17280, Training-Loss 1.21e+00, Validation-Loss 7.30e-01, Training-Accuracy 8.98e-04, Validation-Accuracy 7.45e-04\n",
      "Epoch 17290, Training-Loss 6.54e-01, Validation-Loss 5.62e-01, Training-Accuracy 6.06e-04, Validation-Accuracy 5.87e-04\n",
      "Epoch 17300, Training-Loss 8.77e-01, Validation-Loss 6.76e-01, Training-Accuracy 7.37e-04, Validation-Accuracy 6.19e-04\n",
      "Epoch 17310, Training-Loss 6.20e-01, Validation-Loss 7.65e-01, Training-Accuracy 6.08e-04, Validation-Accuracy 7.36e-04\n",
      "Epoch 17320, Training-Loss 1.04e+00, Validation-Loss 5.64e-01, Training-Accuracy 8.99e-04, Validation-Accuracy 7.10e-04\n",
      "Epoch 17330, Training-Loss 6.67e-01, Validation-Loss 7.24e-01, Training-Accuracy 6.40e-04, Validation-Accuracy 6.70e-04\n",
      "Epoch 17340, Training-Loss 5.59e-01, Validation-Loss 5.97e-01, Training-Accuracy 5.45e-04, Validation-Accuracy 6.39e-04\n",
      "Epoch 17350, Training-Loss 9.20e-01, Validation-Loss 7.62e-01, Training-Accuracy 7.59e-04, Validation-Accuracy 6.70e-04\n",
      "Epoch 17360, Training-Loss 6.96e-01, Validation-Loss 5.90e-01, Training-Accuracy 6.32e-04, Validation-Accuracy 5.81e-04\n",
      "Epoch 17370, Training-Loss 7.16e-01, Validation-Loss 6.21e-01, Training-Accuracy 5.80e-04, Validation-Accuracy 6.32e-04\n",
      "Epoch 17380, Training-Loss 7.82e-01, Validation-Loss 7.16e-01, Training-Accuracy 6.47e-04, Validation-Accuracy 7.32e-04\n",
      "Epoch 17390, Training-Loss 7.25e-01, Validation-Loss 7.07e-01, Training-Accuracy 7.79e-04, Validation-Accuracy 6.72e-04\n",
      "Epoch 17400, Training-Loss 6.36e-01, Validation-Loss 5.50e-01, Training-Accuracy 6.25e-04, Validation-Accuracy 5.96e-04\n",
      "Epoch 17410, Training-Loss 8.50e-01, Validation-Loss 1.09e+00, Training-Accuracy 7.41e-04, Validation-Accuracy 1.01e-03\n",
      "Epoch 17420, Training-Loss 7.55e-01, Validation-Loss 5.79e-01, Training-Accuracy 6.50e-04, Validation-Accuracy 5.64e-04\n",
      "Epoch 17430, Training-Loss 5.82e-01, Validation-Loss 5.89e-01, Training-Accuracy 5.69e-04, Validation-Accuracy 6.00e-04\n",
      "Epoch 17440, Training-Loss 4.94e-01, Validation-Loss 1.08e+00, Training-Accuracy 6.09e-04, Validation-Accuracy 8.81e-04\n",
      "Epoch 17450, Training-Loss 1.64e+00, Validation-Loss 1.00e+00, Training-Accuracy 1.30e-03, Validation-Accuracy 1.02e-03\n",
      "Epoch 17460, Training-Loss 1.53e+00, Validation-Loss 6.32e-01, Training-Accuracy 1.23e-03, Validation-Accuracy 6.46e-04\n",
      "Epoch 17470, Training-Loss 6.12e-01, Validation-Loss 6.42e-01, Training-Accuracy 6.15e-04, Validation-Accuracy 6.80e-04\n",
      "Epoch 17480, Training-Loss 8.49e-01, Validation-Loss 7.69e-01, Training-Accuracy 6.89e-04, Validation-Accuracy 7.53e-04\n",
      "Epoch 17490, Training-Loss 1.11e+00, Validation-Loss 6.20e-01, Training-Accuracy 1.02e-03, Validation-Accuracy 6.74e-04\n",
      "Epoch 17500, Training-Loss 7.48e-01, Validation-Loss 5.37e-01, Training-Accuracy 7.35e-04, Validation-Accuracy 6.45e-04\n",
      "Epoch 17510, Training-Loss 6.08e-01, Validation-Loss 5.33e-01, Training-Accuracy 6.18e-04, Validation-Accuracy 5.64e-04\n",
      "Epoch 17520, Training-Loss 5.37e-01, Validation-Loss 6.04e-01, Training-Accuracy 5.27e-04, Validation-Accuracy 6.00e-04\n",
      "Epoch 17530, Training-Loss 8.76e-01, Validation-Loss 6.61e-01, Training-Accuracy 8.54e-04, Validation-Accuracy 7.19e-04\n",
      "Epoch 17540, Training-Loss 6.50e-01, Validation-Loss 6.14e-01, Training-Accuracy 6.66e-04, Validation-Accuracy 6.22e-04\n",
      "Epoch 17550, Training-Loss 6.23e-01, Validation-Loss 8.09e-01, Training-Accuracy 7.02e-04, Validation-Accuracy 8.29e-04\n",
      "Epoch 17560, Training-Loss 8.79e-01, Validation-Loss 5.47e-01, Training-Accuracy 7.56e-04, Validation-Accuracy 5.82e-04\n",
      "Epoch 17570, Training-Loss 8.05e-01, Validation-Loss 1.58e+00, Training-Accuracy 7.86e-04, Validation-Accuracy 1.28e-03\n",
      "Epoch 17580, Training-Loss 8.90e-01, Validation-Loss 5.70e-01, Training-Accuracy 8.89e-04, Validation-Accuracy 6.32e-04\n",
      "Epoch 17590, Training-Loss 9.47e-01, Validation-Loss 1.00e+00, Training-Accuracy 7.78e-04, Validation-Accuracy 1.04e-03\n",
      "Epoch 17600, Training-Loss 7.15e-01, Validation-Loss 7.05e-01, Training-Accuracy 6.88e-04, Validation-Accuracy 6.97e-04\n",
      "Epoch 17610, Training-Loss 8.21e-01, Validation-Loss 8.25e-01, Training-Accuracy 7.31e-04, Validation-Accuracy 8.12e-04\n",
      "Epoch 17620, Training-Loss 6.91e-01, Validation-Loss 5.98e-01, Training-Accuracy 6.35e-04, Validation-Accuracy 5.88e-04\n",
      "Epoch 17630, Training-Loss 1.38e+00, Validation-Loss 9.79e-01, Training-Accuracy 1.13e-03, Validation-Accuracy 9.01e-04\n",
      "Epoch 17640, Training-Loss 5.84e-01, Validation-Loss 8.33e-01, Training-Accuracy 7.33e-04, Validation-Accuracy 9.10e-04\n",
      "Epoch 17650, Training-Loss 2.43e+00, Validation-Loss 1.47e+00, Training-Accuracy 1.56e-03, Validation-Accuracy 1.17e-03\n",
      "Epoch 17660, Training-Loss 6.00e-01, Validation-Loss 6.10e-01, Training-Accuracy 5.90e-04, Validation-Accuracy 6.26e-04\n",
      "Epoch 17670, Training-Loss 5.92e-01, Validation-Loss 6.74e-01, Training-Accuracy 6.59e-04, Validation-Accuracy 6.39e-04\n",
      "Epoch 17680, Training-Loss 7.44e-01, Validation-Loss 4.96e-01, Training-Accuracy 7.48e-04, Validation-Accuracy 5.72e-04\n",
      "Epoch 17690, Training-Loss 5.74e-01, Validation-Loss 5.15e-01, Training-Accuracy 6.17e-04, Validation-Accuracy 5.56e-04\n",
      "Epoch 17700, Training-Loss 6.31e-01, Validation-Loss 7.61e-01, Training-Accuracy 6.32e-04, Validation-Accuracy 6.87e-04\n",
      "Epoch 17710, Training-Loss 8.31e-01, Validation-Loss 9.85e-01, Training-Accuracy 7.58e-04, Validation-Accuracy 9.63e-04\n",
      "Epoch 17720, Training-Loss 6.04e-01, Validation-Loss 6.15e-01, Training-Accuracy 6.07e-04, Validation-Accuracy 5.85e-04\n",
      "Epoch 17730, Training-Loss 1.61e+00, Validation-Loss 7.48e-01, Training-Accuracy 1.30e-03, Validation-Accuracy 8.80e-04\n",
      "Epoch 17740, Training-Loss 7.61e-01, Validation-Loss 7.32e-01, Training-Accuracy 7.74e-04, Validation-Accuracy 7.23e-04\n",
      "Epoch 17750, Training-Loss 7.32e-01, Validation-Loss 5.04e-01, Training-Accuracy 6.59e-04, Validation-Accuracy 5.41e-04\n",
      "Epoch 17760, Training-Loss 6.66e-01, Validation-Loss 4.98e-01, Training-Accuracy 6.06e-04, Validation-Accuracy 5.72e-04\n",
      "Epoch 17770, Training-Loss 6.51e-01, Validation-Loss 4.78e-01, Training-Accuracy 7.67e-04, Validation-Accuracy 5.51e-04\n",
      "Epoch 17780, Training-Loss 5.51e-01, Validation-Loss 1.03e+00, Training-Accuracy 6.11e-04, Validation-Accuracy 8.16e-04\n",
      "Epoch 17790, Training-Loss 5.53e-01, Validation-Loss 5.94e-01, Training-Accuracy 6.10e-04, Validation-Accuracy 6.56e-04\n",
      "Epoch 17800, Training-Loss 8.90e-01, Validation-Loss 1.06e+00, Training-Accuracy 6.66e-04, Validation-Accuracy 1.09e-03\n",
      "Epoch 17810, Training-Loss 1.65e+00, Validation-Loss 1.51e+00, Training-Accuracy 1.25e-03, Validation-Accuracy 1.28e-03\n",
      "Epoch 17820, Training-Loss 6.17e-01, Validation-Loss 7.60e-01, Training-Accuracy 6.53e-04, Validation-Accuracy 7.29e-04\n",
      "Epoch 17830, Training-Loss 6.89e-01, Validation-Loss 6.71e-01, Training-Accuracy 7.02e-04, Validation-Accuracy 6.18e-04\n",
      "Epoch 17840, Training-Loss 5.52e-01, Validation-Loss 6.33e-01, Training-Accuracy 6.36e-04, Validation-Accuracy 6.09e-04\n",
      "Epoch 17850, Training-Loss 6.73e-01, Validation-Loss 5.80e-01, Training-Accuracy 6.81e-04, Validation-Accuracy 6.04e-04\n",
      "Epoch 17860, Training-Loss 6.09e-01, Validation-Loss 5.93e-01, Training-Accuracy 6.29e-04, Validation-Accuracy 6.51e-04\n",
      "Epoch 17870, Training-Loss 5.39e-01, Validation-Loss 4.91e-01, Training-Accuracy 6.00e-04, Validation-Accuracy 5.49e-04\n",
      "Epoch 17880, Training-Loss 4.81e-01, Validation-Loss 5.89e-01, Training-Accuracy 5.65e-04, Validation-Accuracy 6.08e-04\n",
      "Epoch 17890, Training-Loss 9.00e-01, Validation-Loss 9.01e-01, Training-Accuracy 8.72e-04, Validation-Accuracy 9.14e-04\n",
      "Epoch 17900, Training-Loss 5.44e-01, Validation-Loss 5.31e-01, Training-Accuracy 5.88e-04, Validation-Accuracy 5.90e-04\n",
      "Epoch 17910, Training-Loss 5.93e-01, Validation-Loss 6.16e-01, Training-Accuracy 5.80e-04, Validation-Accuracy 7.26e-04\n",
      "Epoch 17920, Training-Loss 6.64e-01, Validation-Loss 5.69e-01, Training-Accuracy 6.99e-04, Validation-Accuracy 6.27e-04\n",
      "Epoch 17930, Training-Loss 7.22e-01, Validation-Loss 6.34e-01, Training-Accuracy 7.30e-04, Validation-Accuracy 6.57e-04\n",
      "Epoch 17940, Training-Loss 5.29e-01, Validation-Loss 5.34e-01, Training-Accuracy 6.26e-04, Validation-Accuracy 5.45e-04\n",
      "Epoch 17950, Training-Loss 8.15e-01, Validation-Loss 6.75e-01, Training-Accuracy 8.43e-04, Validation-Accuracy 7.73e-04\n",
      "Epoch 17960, Training-Loss 5.58e-01, Validation-Loss 4.97e-01, Training-Accuracy 6.03e-04, Validation-Accuracy 5.67e-04\n",
      "Epoch 17970, Training-Loss 6.77e-01, Validation-Loss 4.80e-01, Training-Accuracy 5.98e-04, Validation-Accuracy 5.93e-04\n",
      "Epoch 17980, Training-Loss 6.49e-01, Validation-Loss 7.23e-01, Training-Accuracy 5.95e-04, Validation-Accuracy 7.22e-04\n",
      "Epoch 17990, Training-Loss 5.78e-01, Validation-Loss 5.46e-01, Training-Accuracy 7.26e-04, Validation-Accuracy 6.20e-04\n",
      "Epoch 18000, Training-Loss 7.50e-01, Validation-Loss 6.37e-01, Training-Accuracy 8.57e-04, Validation-Accuracy 6.60e-04\n",
      "Epoch 18010, Training-Loss 5.02e-01, Validation-Loss 6.38e-01, Training-Accuracy 5.92e-04, Validation-Accuracy 6.32e-04\n",
      "Epoch 18020, Training-Loss 7.72e-01, Validation-Loss 5.22e-01, Training-Accuracy 7.67e-04, Validation-Accuracy 5.21e-04\n",
      "Epoch 18030, Training-Loss 4.79e-01, Validation-Loss 5.81e-01, Training-Accuracy 5.38e-04, Validation-Accuracy 6.87e-04\n",
      "Epoch 18040, Training-Loss 6.97e-01, Validation-Loss 7.14e-01, Training-Accuracy 6.84e-04, Validation-Accuracy 7.77e-04\n",
      "Epoch 18050, Training-Loss 4.27e-01, Validation-Loss 5.38e-01, Training-Accuracy 5.35e-04, Validation-Accuracy 5.73e-04\n",
      "Epoch 18060, Training-Loss 6.84e-01, Validation-Loss 6.73e-01, Training-Accuracy 7.11e-04, Validation-Accuracy 7.74e-04\n",
      "Epoch 18070, Training-Loss 5.45e-01, Validation-Loss 6.15e-01, Training-Accuracy 5.48e-04, Validation-Accuracy 6.49e-04\n",
      "Epoch 18080, Training-Loss 6.95e-01, Validation-Loss 8.39e-01, Training-Accuracy 7.25e-04, Validation-Accuracy 8.46e-04\n",
      "Epoch 18090, Training-Loss 4.94e-01, Validation-Loss 5.93e-01, Training-Accuracy 5.52e-04, Validation-Accuracy 6.46e-04\n",
      "Epoch 18100, Training-Loss 4.25e-01, Validation-Loss 4.58e-01, Training-Accuracy 5.21e-04, Validation-Accuracy 5.36e-04\n",
      "Epoch 18110, Training-Loss 6.06e-01, Validation-Loss 4.85e-01, Training-Accuracy 5.77e-04, Validation-Accuracy 5.64e-04\n",
      "Epoch 18120, Training-Loss 6.84e-01, Validation-Loss 4.24e-01, Training-Accuracy 7.36e-04, Validation-Accuracy 5.37e-04\n",
      "Epoch 18130, Training-Loss 7.33e-01, Validation-Loss 5.30e-01, Training-Accuracy 7.47e-04, Validation-Accuracy 5.93e-04\n",
      "Epoch 18140, Training-Loss 5.62e-01, Validation-Loss 5.91e-01, Training-Accuracy 5.67e-04, Validation-Accuracy 6.58e-04\n",
      "Epoch 18150, Training-Loss 7.41e-01, Validation-Loss 7.70e-01, Training-Accuracy 8.33e-04, Validation-Accuracy 7.43e-04\n",
      "Epoch 18160, Training-Loss 8.30e-01, Validation-Loss 9.74e-01, Training-Accuracy 8.31e-04, Validation-Accuracy 9.18e-04\n",
      "Epoch 18170, Training-Loss 5.98e-01, Validation-Loss 7.65e-01, Training-Accuracy 6.63e-04, Validation-Accuracy 6.27e-04\n",
      "Epoch 18180, Training-Loss 5.95e-01, Validation-Loss 7.53e-01, Training-Accuracy 6.82e-04, Validation-Accuracy 7.63e-04\n",
      "Epoch 18190, Training-Loss 1.12e+00, Validation-Loss 9.02e-01, Training-Accuracy 1.08e-03, Validation-Accuracy 9.63e-04\n",
      "Epoch 18200, Training-Loss 4.82e-01, Validation-Loss 5.48e-01, Training-Accuracy 5.57e-04, Validation-Accuracy 5.35e-04\n",
      "Epoch 18210, Training-Loss 5.60e-01, Validation-Loss 5.75e-01, Training-Accuracy 6.38e-04, Validation-Accuracy 5.86e-04\n",
      "Epoch 18220, Training-Loss 6.48e-01, Validation-Loss 5.33e-01, Training-Accuracy 6.08e-04, Validation-Accuracy 6.16e-04\n",
      "Epoch 18230, Training-Loss 8.09e-01, Validation-Loss 8.14e-01, Training-Accuracy 7.00e-04, Validation-Accuracy 8.18e-04\n",
      "Epoch 18240, Training-Loss 9.59e-01, Validation-Loss 9.92e-01, Training-Accuracy 8.44e-04, Validation-Accuracy 9.18e-04\n",
      "Epoch 18250, Training-Loss 5.00e-01, Validation-Loss 9.08e-01, Training-Accuracy 5.62e-04, Validation-Accuracy 8.49e-04\n",
      "Epoch 18260, Training-Loss 5.66e-01, Validation-Loss 7.15e-01, Training-Accuracy 5.69e-04, Validation-Accuracy 7.71e-04\n",
      "Epoch 18270, Training-Loss 6.42e-01, Validation-Loss 6.84e-01, Training-Accuracy 6.83e-04, Validation-Accuracy 7.76e-04\n",
      "Epoch 18280, Training-Loss 6.63e-01, Validation-Loss 4.69e-01, Training-Accuracy 5.73e-04, Validation-Accuracy 6.10e-04\n",
      "Epoch 18290, Training-Loss 6.81e-01, Validation-Loss 7.30e-01, Training-Accuracy 7.64e-04, Validation-Accuracy 8.19e-04\n",
      "Epoch 18300, Training-Loss 5.52e-01, Validation-Loss 6.58e-01, Training-Accuracy 6.44e-04, Validation-Accuracy 7.92e-04\n",
      "Epoch 18310, Training-Loss 9.29e-01, Validation-Loss 1.23e+00, Training-Accuracy 8.60e-04, Validation-Accuracy 1.13e-03\n",
      "Epoch 18320, Training-Loss 5.81e-01, Validation-Loss 5.06e-01, Training-Accuracy 5.63e-04, Validation-Accuracy 5.63e-04\n",
      "Epoch 18330, Training-Loss 5.38e-01, Validation-Loss 4.99e-01, Training-Accuracy 5.85e-04, Validation-Accuracy 5.89e-04\n",
      "Epoch 18340, Training-Loss 4.73e-01, Validation-Loss 6.26e-01, Training-Accuracy 5.25e-04, Validation-Accuracy 5.69e-04\n",
      "Epoch 18350, Training-Loss 6.75e-01, Validation-Loss 5.83e-01, Training-Accuracy 6.32e-04, Validation-Accuracy 5.66e-04\n",
      "Epoch 18360, Training-Loss 7.69e-01, Validation-Loss 4.63e-01, Training-Accuracy 8.65e-04, Validation-Accuracy 5.54e-04\n",
      "Epoch 18370, Training-Loss 1.03e+00, Validation-Loss 1.20e+00, Training-Accuracy 1.04e-03, Validation-Accuracy 1.08e-03\n",
      "Epoch 18380, Training-Loss 5.02e-01, Validation-Loss 5.06e-01, Training-Accuracy 5.24e-04, Validation-Accuracy 5.60e-04\n",
      "Epoch 18390, Training-Loss 7.05e-01, Validation-Loss 8.28e-01, Training-Accuracy 7.81e-04, Validation-Accuracy 9.05e-04\n",
      "Epoch 18400, Training-Loss 6.67e-01, Validation-Loss 5.01e-01, Training-Accuracy 8.10e-04, Validation-Accuracy 5.65e-04\n",
      "Epoch 18410, Training-Loss 1.02e+00, Validation-Loss 6.78e-01, Training-Accuracy 8.78e-04, Validation-Accuracy 8.12e-04\n",
      "Epoch 18420, Training-Loss 7.06e-01, Validation-Loss 6.52e-01, Training-Accuracy 6.18e-04, Validation-Accuracy 7.28e-04\n",
      "Epoch 18430, Training-Loss 6.13e-01, Validation-Loss 7.01e-01, Training-Accuracy 6.77e-04, Validation-Accuracy 8.27e-04\n",
      "Epoch 18440, Training-Loss 6.60e-01, Validation-Loss 5.23e-01, Training-Accuracy 7.00e-04, Validation-Accuracy 5.86e-04\n",
      "Epoch 18450, Training-Loss 7.02e-01, Validation-Loss 1.14e+00, Training-Accuracy 6.76e-04, Validation-Accuracy 1.10e-03\n",
      "Epoch 18460, Training-Loss 4.55e-01, Validation-Loss 8.83e-01, Training-Accuracy 4.87e-04, Validation-Accuracy 8.80e-04\n",
      "Epoch 18470, Training-Loss 6.31e-01, Validation-Loss 8.73e-01, Training-Accuracy 6.32e-04, Validation-Accuracy 8.91e-04\n",
      "Epoch 18480, Training-Loss 6.85e-01, Validation-Loss 4.69e-01, Training-Accuracy 7.51e-04, Validation-Accuracy 5.32e-04\n",
      "Epoch 18490, Training-Loss 5.75e-01, Validation-Loss 5.16e-01, Training-Accuracy 6.83e-04, Validation-Accuracy 5.59e-04\n",
      "Epoch 18500, Training-Loss 4.77e-01, Validation-Loss 5.42e-01, Training-Accuracy 6.27e-04, Validation-Accuracy 5.80e-04\n",
      "Epoch 18510, Training-Loss 8.09e-01, Validation-Loss 7.55e-01, Training-Accuracy 8.22e-04, Validation-Accuracy 6.54e-04\n",
      "Epoch 18520, Training-Loss 5.06e-01, Validation-Loss 4.84e-01, Training-Accuracy 5.31e-04, Validation-Accuracy 5.74e-04\n",
      "Epoch 18530, Training-Loss 5.77e-01, Validation-Loss 4.07e-01, Training-Accuracy 7.06e-04, Validation-Accuracy 5.14e-04\n",
      "Epoch 18540, Training-Loss 9.32e-01, Validation-Loss 6.83e-01, Training-Accuracy 8.60e-04, Validation-Accuracy 7.91e-04\n",
      "Epoch 18550, Training-Loss 5.25e-01, Validation-Loss 8.11e-01, Training-Accuracy 5.52e-04, Validation-Accuracy 8.00e-04\n",
      "Epoch 18560, Training-Loss 4.73e-01, Validation-Loss 5.33e-01, Training-Accuracy 5.72e-04, Validation-Accuracy 5.79e-04\n",
      "Epoch 18570, Training-Loss 4.90e-01, Validation-Loss 5.85e-01, Training-Accuracy 5.93e-04, Validation-Accuracy 6.71e-04\n",
      "Epoch 18580, Training-Loss 1.03e+00, Validation-Loss 6.98e-01, Training-Accuracy 9.88e-04, Validation-Accuracy 8.79e-04\n",
      "Epoch 18590, Training-Loss 6.91e-01, Validation-Loss 4.24e-01, Training-Accuracy 7.26e-04, Validation-Accuracy 5.25e-04\n",
      "Epoch 18600, Training-Loss 5.25e-01, Validation-Loss 5.72e-01, Training-Accuracy 5.93e-04, Validation-Accuracy 7.00e-04\n",
      "Epoch 18610, Training-Loss 6.03e-01, Validation-Loss 6.68e-01, Training-Accuracy 6.34e-04, Validation-Accuracy 7.58e-04\n",
      "Epoch 18620, Training-Loss 6.61e-01, Validation-Loss 5.23e-01, Training-Accuracy 8.06e-04, Validation-Accuracy 5.58e-04\n",
      "Epoch 18630, Training-Loss 5.90e-01, Validation-Loss 4.68e-01, Training-Accuracy 6.62e-04, Validation-Accuracy 5.46e-04\n",
      "Epoch 18640, Training-Loss 5.87e-01, Validation-Loss 5.50e-01, Training-Accuracy 6.04e-04, Validation-Accuracy 6.18e-04\n",
      "Epoch 18650, Training-Loss 5.23e-01, Validation-Loss 5.28e-01, Training-Accuracy 6.15e-04, Validation-Accuracy 5.77e-04\n",
      "Epoch 18660, Training-Loss 1.02e+00, Validation-Loss 6.49e-01, Training-Accuracy 1.06e-03, Validation-Accuracy 7.15e-04\n",
      "Epoch 18670, Training-Loss 4.74e-01, Validation-Loss 5.08e-01, Training-Accuracy 6.27e-04, Validation-Accuracy 5.95e-04\n",
      "Epoch 18680, Training-Loss 5.17e-01, Validation-Loss 7.52e-01, Training-Accuracy 5.77e-04, Validation-Accuracy 6.95e-04\n",
      "Epoch 18690, Training-Loss 5.31e-01, Validation-Loss 6.11e-01, Training-Accuracy 6.00e-04, Validation-Accuracy 6.59e-04\n",
      "Epoch 18700, Training-Loss 5.70e-01, Validation-Loss 4.65e-01, Training-Accuracy 6.11e-04, Validation-Accuracy 5.84e-04\n",
      "Epoch 18710, Training-Loss 4.78e-01, Validation-Loss 5.51e-01, Training-Accuracy 5.99e-04, Validation-Accuracy 5.97e-04\n",
      "Epoch 18720, Training-Loss 6.10e-01, Validation-Loss 5.10e-01, Training-Accuracy 6.32e-04, Validation-Accuracy 5.64e-04\n",
      "Epoch 18730, Training-Loss 4.80e-01, Validation-Loss 6.05e-01, Training-Accuracy 5.27e-04, Validation-Accuracy 5.93e-04\n",
      "Epoch 18740, Training-Loss 6.54e-01, Validation-Loss 9.48e-01, Training-Accuracy 7.32e-04, Validation-Accuracy 9.97e-04\n",
      "Epoch 18750, Training-Loss 7.37e-01, Validation-Loss 1.28e+00, Training-Accuracy 7.83e-04, Validation-Accuracy 1.22e-03\n",
      "Epoch 18760, Training-Loss 5.58e-01, Validation-Loss 6.02e-01, Training-Accuracy 5.93e-04, Validation-Accuracy 6.49e-04\n",
      "Epoch 18770, Training-Loss 5.59e-01, Validation-Loss 7.55e-01, Training-Accuracy 6.58e-04, Validation-Accuracy 7.58e-04\n",
      "Epoch 18780, Training-Loss 5.75e-01, Validation-Loss 4.84e-01, Training-Accuracy 6.57e-04, Validation-Accuracy 5.59e-04\n",
      "Epoch 18790, Training-Loss 4.29e-01, Validation-Loss 4.92e-01, Training-Accuracy 5.29e-04, Validation-Accuracy 6.12e-04\n",
      "Epoch 18800, Training-Loss 5.11e-01, Validation-Loss 4.89e-01, Training-Accuracy 5.02e-04, Validation-Accuracy 5.32e-04\n",
      "Epoch 18810, Training-Loss 5.86e-01, Validation-Loss 4.08e-01, Training-Accuracy 6.20e-04, Validation-Accuracy 4.96e-04\n",
      "Epoch 18820, Training-Loss 6.39e-01, Validation-Loss 4.89e-01, Training-Accuracy 6.82e-04, Validation-Accuracy 5.59e-04\n",
      "Epoch 18830, Training-Loss 7.24e-01, Validation-Loss 4.56e-01, Training-Accuracy 7.55e-04, Validation-Accuracy 5.53e-04\n",
      "Epoch 18840, Training-Loss 6.52e-01, Validation-Loss 4.95e-01, Training-Accuracy 6.24e-04, Validation-Accuracy 5.96e-04\n",
      "Epoch 18850, Training-Loss 5.78e-01, Validation-Loss 5.45e-01, Training-Accuracy 5.41e-04, Validation-Accuracy 5.89e-04\n",
      "Epoch 18860, Training-Loss 7.50e-01, Validation-Loss 9.83e-01, Training-Accuracy 8.17e-04, Validation-Accuracy 8.93e-04\n",
      "Epoch 18870, Training-Loss 5.07e-01, Validation-Loss 4.06e-01, Training-Accuracy 5.70e-04, Validation-Accuracy 5.51e-04\n",
      "Epoch 18880, Training-Loss 6.82e-01, Validation-Loss 6.36e-01, Training-Accuracy 7.59e-04, Validation-Accuracy 6.52e-04\n",
      "Epoch 18890, Training-Loss 5.43e-01, Validation-Loss 5.64e-01, Training-Accuracy 6.70e-04, Validation-Accuracy 6.15e-04\n",
      "Epoch 18900, Training-Loss 4.60e-01, Validation-Loss 5.15e-01, Training-Accuracy 5.12e-04, Validation-Accuracy 5.68e-04\n",
      "Epoch 18910, Training-Loss 6.34e-01, Validation-Loss 5.42e-01, Training-Accuracy 6.48e-04, Validation-Accuracy 5.87e-04\n",
      "Epoch 18920, Training-Loss 5.98e-01, Validation-Loss 5.60e-01, Training-Accuracy 6.52e-04, Validation-Accuracy 5.76e-04\n",
      "Epoch 18930, Training-Loss 6.01e-01, Validation-Loss 4.76e-01, Training-Accuracy 6.12e-04, Validation-Accuracy 5.71e-04\n",
      "Epoch 18940, Training-Loss 5.92e-01, Validation-Loss 5.79e-01, Training-Accuracy 5.40e-04, Validation-Accuracy 6.27e-04\n",
      "Epoch 18950, Training-Loss 5.18e-01, Validation-Loss 5.69e-01, Training-Accuracy 6.49e-04, Validation-Accuracy 6.83e-04\n",
      "Epoch 18960, Training-Loss 5.82e-01, Validation-Loss 7.99e-01, Training-Accuracy 6.13e-04, Validation-Accuracy 8.94e-04\n",
      "Epoch 18970, Training-Loss 5.24e-01, Validation-Loss 8.00e-01, Training-Accuracy 5.19e-04, Validation-Accuracy 8.66e-04\n",
      "Epoch 18980, Training-Loss 5.73e-01, Validation-Loss 5.50e-01, Training-Accuracy 5.72e-04, Validation-Accuracy 5.88e-04\n",
      "Epoch 18990, Training-Loss 4.88e-01, Validation-Loss 4.72e-01, Training-Accuracy 5.93e-04, Validation-Accuracy 5.70e-04\n",
      "Epoch 19000, Training-Loss 4.90e-01, Validation-Loss 5.98e-01, Training-Accuracy 6.05e-04, Validation-Accuracy 6.16e-04\n",
      "Epoch 19010, Training-Loss 8.10e-01, Validation-Loss 1.02e+00, Training-Accuracy 8.48e-04, Validation-Accuracy 9.35e-04\n",
      "Epoch 19020, Training-Loss 5.83e-01, Validation-Loss 7.67e-01, Training-Accuracy 6.67e-04, Validation-Accuracy 8.36e-04\n",
      "Epoch 19030, Training-Loss 7.06e-01, Validation-Loss 8.93e-01, Training-Accuracy 8.10e-04, Validation-Accuracy 9.67e-04\n",
      "Epoch 19040, Training-Loss 3.91e-01, Validation-Loss 6.52e-01, Training-Accuracy 5.25e-04, Validation-Accuracy 7.79e-04\n",
      "Epoch 19050, Training-Loss 5.16e-01, Validation-Loss 4.89e-01, Training-Accuracy 6.18e-04, Validation-Accuracy 6.30e-04\n",
      "Epoch 19060, Training-Loss 4.48e-01, Validation-Loss 5.93e-01, Training-Accuracy 6.30e-04, Validation-Accuracy 5.88e-04\n",
      "Epoch 19070, Training-Loss 4.59e-01, Validation-Loss 6.68e-01, Training-Accuracy 5.37e-04, Validation-Accuracy 7.39e-04\n",
      "Epoch 19080, Training-Loss 5.57e-01, Validation-Loss 5.83e-01, Training-Accuracy 5.36e-04, Validation-Accuracy 6.05e-04\n",
      "Epoch 19090, Training-Loss 8.19e-01, Validation-Loss 4.56e-01, Training-Accuracy 7.58e-04, Validation-Accuracy 5.61e-04\n",
      "Epoch 19100, Training-Loss 5.61e-01, Validation-Loss 5.85e-01, Training-Accuracy 6.78e-04, Validation-Accuracy 5.74e-04\n",
      "Epoch 19110, Training-Loss 4.56e-01, Validation-Loss 4.83e-01, Training-Accuracy 5.25e-04, Validation-Accuracy 5.24e-04\n",
      "Epoch 19120, Training-Loss 6.60e-01, Validation-Loss 4.54e-01, Training-Accuracy 7.27e-04, Validation-Accuracy 5.43e-04\n",
      "Epoch 19130, Training-Loss 5.61e-01, Validation-Loss 4.65e-01, Training-Accuracy 6.42e-04, Validation-Accuracy 5.34e-04\n",
      "Epoch 19140, Training-Loss 5.99e-01, Validation-Loss 4.72e-01, Training-Accuracy 6.82e-04, Validation-Accuracy 6.44e-04\n",
      "Epoch 19150, Training-Loss 5.03e-01, Validation-Loss 4.78e-01, Training-Accuracy 5.21e-04, Validation-Accuracy 5.45e-04\n",
      "Epoch 19160, Training-Loss 4.28e-01, Validation-Loss 6.03e-01, Training-Accuracy 5.17e-04, Validation-Accuracy 6.53e-04\n",
      "Epoch 19170, Training-Loss 4.77e-01, Validation-Loss 4.87e-01, Training-Accuracy 5.60e-04, Validation-Accuracy 6.15e-04\n",
      "Epoch 19180, Training-Loss 5.33e-01, Validation-Loss 1.04e+00, Training-Accuracy 6.56e-04, Validation-Accuracy 9.66e-04\n",
      "Epoch 19190, Training-Loss 1.21e+00, Validation-Loss 2.03e+00, Training-Accuracy 1.23e-03, Validation-Accuracy 1.48e-03\n",
      "Epoch 19200, Training-Loss 1.27e+00, Validation-Loss 1.27e+00, Training-Accuracy 1.12e-03, Validation-Accuracy 1.22e-03\n",
      "Epoch 19210, Training-Loss 1.16e+00, Validation-Loss 4.25e-01, Training-Accuracy 1.13e-03, Validation-Accuracy 5.63e-04\n",
      "Epoch 19220, Training-Loss 9.99e-01, Validation-Loss 1.13e+00, Training-Accuracy 1.00e-03, Validation-Accuracy 9.29e-04\n",
      "Epoch 19230, Training-Loss 5.37e-01, Validation-Loss 5.99e-01, Training-Accuracy 5.71e-04, Validation-Accuracy 7.59e-04\n",
      "Epoch 19240, Training-Loss 5.78e-01, Validation-Loss 6.28e-01, Training-Accuracy 6.82e-04, Validation-Accuracy 7.13e-04\n",
      "Epoch 19250, Training-Loss 4.66e-01, Validation-Loss 4.10e-01, Training-Accuracy 5.47e-04, Validation-Accuracy 5.16e-04\n",
      "Epoch 19260, Training-Loss 6.23e-01, Validation-Loss 7.85e-01, Training-Accuracy 6.87e-04, Validation-Accuracy 8.95e-04\n",
      "Epoch 19270, Training-Loss 4.54e-01, Validation-Loss 5.59e-01, Training-Accuracy 5.48e-04, Validation-Accuracy 6.28e-04\n",
      "Epoch 19280, Training-Loss 6.33e-01, Validation-Loss 5.17e-01, Training-Accuracy 6.09e-04, Validation-Accuracy 6.05e-04\n",
      "Epoch 19290, Training-Loss 5.81e-01, Validation-Loss 6.33e-01, Training-Accuracy 6.61e-04, Validation-Accuracy 7.13e-04\n",
      "Epoch 19300, Training-Loss 6.89e-01, Validation-Loss 5.73e-01, Training-Accuracy 7.83e-04, Validation-Accuracy 6.96e-04\n",
      "Epoch 19310, Training-Loss 5.11e-01, Validation-Loss 4.45e-01, Training-Accuracy 5.73e-04, Validation-Accuracy 5.49e-04\n",
      "Epoch 19320, Training-Loss 4.47e-01, Validation-Loss 4.78e-01, Training-Accuracy 5.38e-04, Validation-Accuracy 5.32e-04\n",
      "Epoch 19330, Training-Loss 5.46e-01, Validation-Loss 5.08e-01, Training-Accuracy 6.43e-04, Validation-Accuracy 5.79e-04\n",
      "Epoch 19340, Training-Loss 4.77e-01, Validation-Loss 6.30e-01, Training-Accuracy 5.59e-04, Validation-Accuracy 7.22e-04\n",
      "Epoch 19350, Training-Loss 5.26e-01, Validation-Loss 4.41e-01, Training-Accuracy 5.54e-04, Validation-Accuracy 5.32e-04\n",
      "Epoch 19360, Training-Loss 4.70e-01, Validation-Loss 7.00e-01, Training-Accuracy 5.75e-04, Validation-Accuracy 7.29e-04\n",
      "Epoch 19370, Training-Loss 7.00e-01, Validation-Loss 7.97e-01, Training-Accuracy 7.65e-04, Validation-Accuracy 8.96e-04\n",
      "Epoch 19380, Training-Loss 4.62e-01, Validation-Loss 5.91e-01, Training-Accuracy 5.46e-04, Validation-Accuracy 7.54e-04\n",
      "Epoch 19390, Training-Loss 6.98e-01, Validation-Loss 3.85e-01, Training-Accuracy 8.09e-04, Validation-Accuracy 4.89e-04\n",
      "Epoch 19400, Training-Loss 6.36e-01, Validation-Loss 3.88e-01, Training-Accuracy 7.08e-04, Validation-Accuracy 5.07e-04\n",
      "Epoch 19410, Training-Loss 4.08e-01, Validation-Loss 3.83e-01, Training-Accuracy 4.93e-04, Validation-Accuracy 5.40e-04\n",
      "Epoch 19420, Training-Loss 4.53e-01, Validation-Loss 9.09e-01, Training-Accuracy 5.06e-04, Validation-Accuracy 9.09e-04\n",
      "Epoch 19430, Training-Loss 9.05e-01, Validation-Loss 8.73e-01, Training-Accuracy 9.37e-04, Validation-Accuracy 9.38e-04\n",
      "Epoch 19440, Training-Loss 4.82e-01, Validation-Loss 9.95e-01, Training-Accuracy 5.21e-04, Validation-Accuracy 8.35e-04\n",
      "Epoch 19450, Training-Loss 8.23e-01, Validation-Loss 5.11e-01, Training-Accuracy 9.63e-04, Validation-Accuracy 6.06e-04\n",
      "Epoch 19460, Training-Loss 5.63e-01, Validation-Loss 4.88e-01, Training-Accuracy 5.98e-04, Validation-Accuracy 5.82e-04\n",
      "Epoch 19470, Training-Loss 5.43e-01, Validation-Loss 9.01e-01, Training-Accuracy 6.22e-04, Validation-Accuracy 8.96e-04\n",
      "Epoch 19480, Training-Loss 9.29e-01, Validation-Loss 9.26e-01, Training-Accuracy 9.73e-04, Validation-Accuracy 9.02e-04\n",
      "Epoch 19490, Training-Loss 6.61e-01, Validation-Loss 4.44e-01, Training-Accuracy 7.46e-04, Validation-Accuracy 6.06e-04\n",
      "Epoch 19500, Training-Loss 4.98e-01, Validation-Loss 5.50e-01, Training-Accuracy 5.70e-04, Validation-Accuracy 6.21e-04\n",
      "Epoch 19510, Training-Loss 4.96e-01, Validation-Loss 6.15e-01, Training-Accuracy 5.98e-04, Validation-Accuracy 7.33e-04\n",
      "Epoch 19520, Training-Loss 4.47e-01, Validation-Loss 4.43e-01, Training-Accuracy 5.51e-04, Validation-Accuracy 5.53e-04\n",
      "Epoch 19530, Training-Loss 5.67e-01, Validation-Loss 5.46e-01, Training-Accuracy 6.92e-04, Validation-Accuracy 6.22e-04\n",
      "Epoch 19540, Training-Loss 5.01e-01, Validation-Loss 4.40e-01, Training-Accuracy 6.39e-04, Validation-Accuracy 5.30e-04\n",
      "Epoch 19550, Training-Loss 4.29e-01, Validation-Loss 4.72e-01, Training-Accuracy 4.94e-04, Validation-Accuracy 5.72e-04\n",
      "Epoch 19560, Training-Loss 4.49e-01, Validation-Loss 4.45e-01, Training-Accuracy 5.16e-04, Validation-Accuracy 5.35e-04\n",
      "Epoch 19570, Training-Loss 6.89e-01, Validation-Loss 5.38e-01, Training-Accuracy 7.84e-04, Validation-Accuracy 6.55e-04\n",
      "Epoch 19580, Training-Loss 4.67e-01, Validation-Loss 4.19e-01, Training-Accuracy 5.19e-04, Validation-Accuracy 5.58e-04\n",
      "Epoch 19590, Training-Loss 4.22e-01, Validation-Loss 6.20e-01, Training-Accuracy 5.24e-04, Validation-Accuracy 7.49e-04\n",
      "Epoch 19600, Training-Loss 4.29e-01, Validation-Loss 4.45e-01, Training-Accuracy 5.17e-04, Validation-Accuracy 5.40e-04\n",
      "Epoch 19610, Training-Loss 3.39e-01, Validation-Loss 4.54e-01, Training-Accuracy 4.71e-04, Validation-Accuracy 6.07e-04\n",
      "Epoch 19620, Training-Loss 4.89e-01, Validation-Loss 5.26e-01, Training-Accuracy 4.89e-04, Validation-Accuracy 5.87e-04\n",
      "Epoch 19630, Training-Loss 1.36e+00, Validation-Loss 8.17e-01, Training-Accuracy 1.11e-03, Validation-Accuracy 9.20e-04\n",
      "Epoch 19640, Training-Loss 7.67e-01, Validation-Loss 5.97e-01, Training-Accuracy 7.77e-04, Validation-Accuracy 6.35e-04\n",
      "Epoch 19650, Training-Loss 4.43e-01, Validation-Loss 5.09e-01, Training-Accuracy 5.46e-04, Validation-Accuracy 5.24e-04\n",
      "Epoch 19660, Training-Loss 3.97e-01, Validation-Loss 4.49e-01, Training-Accuracy 5.78e-04, Validation-Accuracy 5.16e-04\n",
      "Epoch 19670, Training-Loss 4.52e-01, Validation-Loss 5.66e-01, Training-Accuracy 5.56e-04, Validation-Accuracy 5.79e-04\n",
      "Epoch 19680, Training-Loss 6.86e-01, Validation-Loss 6.01e-01, Training-Accuracy 6.83e-04, Validation-Accuracy 7.57e-04\n",
      "Epoch 19690, Training-Loss 4.07e-01, Validation-Loss 5.84e-01, Training-Accuracy 5.76e-04, Validation-Accuracy 5.61e-04\n",
      "Epoch 19700, Training-Loss 5.51e-01, Validation-Loss 3.82e-01, Training-Accuracy 6.59e-04, Validation-Accuracy 4.87e-04\n",
      "Epoch 19710, Training-Loss 4.49e-01, Validation-Loss 3.39e-01, Training-Accuracy 5.07e-04, Validation-Accuracy 4.75e-04\n",
      "Epoch 19720, Training-Loss 4.23e-01, Validation-Loss 6.09e-01, Training-Accuracy 5.49e-04, Validation-Accuracy 6.84e-04\n",
      "Epoch 19730, Training-Loss 4.63e-01, Validation-Loss 5.00e-01, Training-Accuracy 5.35e-04, Validation-Accuracy 5.35e-04\n",
      "Epoch 19740, Training-Loss 3.73e-01, Validation-Loss 4.03e-01, Training-Accuracy 4.63e-04, Validation-Accuracy 5.46e-04\n",
      "Epoch 19750, Training-Loss 4.42e-01, Validation-Loss 6.72e-01, Training-Accuracy 5.50e-04, Validation-Accuracy 7.95e-04\n",
      "Epoch 19760, Training-Loss 4.13e-01, Validation-Loss 5.60e-01, Training-Accuracy 5.32e-04, Validation-Accuracy 6.01e-04\n",
      "Epoch 19770, Training-Loss 4.25e-01, Validation-Loss 5.25e-01, Training-Accuracy 5.98e-04, Validation-Accuracy 6.47e-04\n",
      "Epoch 19780, Training-Loss 5.17e-01, Validation-Loss 3.95e-01, Training-Accuracy 5.56e-04, Validation-Accuracy 4.91e-04\n",
      "Epoch 19790, Training-Loss 3.58e-01, Validation-Loss 5.11e-01, Training-Accuracy 4.62e-04, Validation-Accuracy 5.79e-04\n",
      "Epoch 19800, Training-Loss 3.92e-01, Validation-Loss 4.69e-01, Training-Accuracy 4.82e-04, Validation-Accuracy 5.93e-04\n",
      "Epoch 19810, Training-Loss 4.40e-01, Validation-Loss 3.54e-01, Training-Accuracy 4.99e-04, Validation-Accuracy 4.96e-04\n",
      "Epoch 19820, Training-Loss 4.01e-01, Validation-Loss 4.30e-01, Training-Accuracy 5.10e-04, Validation-Accuracy 5.16e-04\n",
      "Epoch 19830, Training-Loss 5.75e-01, Validation-Loss 5.78e-01, Training-Accuracy 6.10e-04, Validation-Accuracy 6.65e-04\n",
      "Epoch 19840, Training-Loss 4.53e-01, Validation-Loss 6.23e-01, Training-Accuracy 5.55e-04, Validation-Accuracy 7.06e-04\n",
      "Epoch 19850, Training-Loss 4.84e-01, Validation-Loss 6.28e-01, Training-Accuracy 5.62e-04, Validation-Accuracy 7.20e-04\n",
      "Epoch 19860, Training-Loss 6.25e-01, Validation-Loss 7.74e-01, Training-Accuracy 6.50e-04, Validation-Accuracy 8.65e-04\n",
      "Epoch 19870, Training-Loss 1.11e+00, Validation-Loss 1.38e+00, Training-Accuracy 1.04e-03, Validation-Accuracy 1.19e-03\n",
      "Epoch 19880, Training-Loss 9.25e-01, Validation-Loss 1.18e+00, Training-Accuracy 9.82e-04, Validation-Accuracy 1.07e-03\n",
      "Epoch 19890, Training-Loss 5.20e-01, Validation-Loss 3.86e-01, Training-Accuracy 5.28e-04, Validation-Accuracy 5.05e-04\n",
      "Epoch 19900, Training-Loss 3.84e-01, Validation-Loss 3.83e-01, Training-Accuracy 4.98e-04, Validation-Accuracy 4.66e-04\n",
      "Epoch 19910, Training-Loss 4.06e-01, Validation-Loss 4.31e-01, Training-Accuracy 4.84e-04, Validation-Accuracy 5.01e-04\n",
      "Epoch 19920, Training-Loss 4.67e-01, Validation-Loss 4.96e-01, Training-Accuracy 5.59e-04, Validation-Accuracy 6.57e-04\n",
      "Epoch 19930, Training-Loss 4.65e-01, Validation-Loss 6.34e-01, Training-Accuracy 5.69e-04, Validation-Accuracy 8.02e-04\n",
      "Epoch 19940, Training-Loss 9.52e-01, Validation-Loss 1.30e+00, Training-Accuracy 8.87e-04, Validation-Accuracy 1.09e-03\n",
      "Epoch 19950, Training-Loss 8.21e-01, Validation-Loss 6.56e-01, Training-Accuracy 9.14e-04, Validation-Accuracy 7.11e-04\n",
      "Epoch 19960, Training-Loss 4.09e-01, Validation-Loss 4.38e-01, Training-Accuracy 4.92e-04, Validation-Accuracy 5.69e-04\n",
      "Epoch 19970, Training-Loss 4.61e-01, Validation-Loss 4.80e-01, Training-Accuracy 5.63e-04, Validation-Accuracy 5.88e-04\n",
      "Epoch 19980, Training-Loss 6.47e-01, Validation-Loss 6.10e-01, Training-Accuracy 7.86e-04, Validation-Accuracy 6.97e-04\n",
      "Epoch 19990, Training-Loss 6.98e-01, Validation-Loss 5.38e-01, Training-Accuracy 7.34e-04, Validation-Accuracy 6.27e-04\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses, val_losses = training_loop(epochs, model, loss_fn_data, optimizer,train_loader, val_loader)  # Train the model\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20000, Test-Loss 1.42e+02,Test-Accuracy 6.68e-04\n"
     ]
    }
   ],
   "source": [
    "test_losses = test_loop(epochs, model, loss_fn_data, optimizer, train_loader, test_loader)  # Test the model\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp_nn = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).cpu().detach().numpy() # Get the predictions from the model\n",
    "\n",
    "temp_nn = temp_nn.reshape(num_steps+1, num_points+2) # Reshape the predictions to a 2D array\n",
    "\n",
    "plt.figure\n",
    "plt.plot(time_ss, temp_nn[:,num_points//2], label='Predicted Temperature')\n",
    "plt.plot(time_ss, temperature_history_1[:,num_points//2], label='Actual Temperature')\n",
    "plt.xlabel('Time(s)')\n",
    "plt.ylabel('Temperature (K)')\n",
    "plt.title('Predicted vs Actual Temperature at x = 7.5mm')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
