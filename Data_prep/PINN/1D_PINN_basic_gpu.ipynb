{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha ia 0.000615151925594835\n",
      "dx is 0.00020408163265306123\n",
      "dt is  3.3852867115765984e-05\n",
      "num_steps is 147699\n",
      "cfl is 3.3852867115765984e-05\n",
      "stability criteria satisfied\n"
     ]
    }
   ],
   "source": [
    "# Geometry\n",
    "length = 1e-2                 # Length of the rod\n",
    "\n",
    "# Material properties\n",
    "rho = 2760.0                      # Density of AL380 (kg/m^3)\n",
    "rho_l = rho\n",
    "rho_s = rho *1.5\n",
    "rho_m = rho\n",
    "k = 1090.0 *1.5                         # W/m-K\n",
    "k_l = k                        # W/m-K\n",
    "k_s = k                        # W/m-K\n",
    "k_m = k                        # W/m-K\n",
    "cp = 963.0                        # Specific heat of aluminum (J/kg-K)\n",
    "cp_l = cp\n",
    "cp_s = cp\n",
    "cp_m = cp\n",
    "alpha = k / (rho * cp)            # Thermal diffusivity\n",
    "alpha_l = alpha\n",
    "alpha_s = alpha\n",
    "aplha_m = alpha\n",
    "print('alpha ia',alpha)\n",
    "#L_fusion = 3.9e3                 # J/kg\n",
    "L_fusion = 389e3               # J/kg\n",
    "\n",
    "# Spatial discretization\n",
    "\n",
    "num_points = 50                  # Number of spatial points\n",
    "dx = length / (num_points - 1)\n",
    "print('dx is',dx)\n",
    "                                   #dt = time_end/num_steps\n",
    "#num_steps = 200000               # Number of time steps\n",
    "                                  # num_steps = round(time_end/dt)\n",
    "                                                              \n",
    "# Time Discretization  \n",
    "# \n",
    "time_end = 5                 # seconds                         \n",
    "#num_steps = 10000\n",
    "# dt = time_end/num_steps\n",
    "dt = abs(0.5 *(dx**2/alpha))\n",
    "print('dt is ',dt)\n",
    "num_steps = round(time_end/dt) +1\n",
    "print('num_steps is',num_steps)\n",
    "cfl = 0.5 *(dx**2/alpha)\n",
    "print('cfl is',cfl)\n",
    "#dt = time_end / num_steps\n",
    "time_steps = np.linspace(0, time_end, num_steps + 1)\n",
    "\n",
    "if dt <= cfl:\n",
    "    print('stability criteria satisfied')\n",
    "else:\n",
    "    print('stability criteria not satisfied')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Conditions\n",
    "T_L = 866.0                       #  K -Liquidus Temperature (593 c)\n",
    "T_S = 811.0                       # K- Solidus Temperature (538 C)\n",
    "\n",
    "\n",
    "# Initial temperature and phase fields\n",
    "init_temp = 870.0\n",
    "temperature = np.full(num_points, init_temp)\n",
    "phase = np.zeros(num_points)*1.0\n",
    "\n",
    "# Set boundary conditions\n",
    "temperature[-1] = 313.0 #(40 C)\n",
    "phase[-1] = 1.0\n",
    "\n",
    "temperature[0] = 313.0 #(40 C)\n",
    "phase[0] = 1.0\n",
    "\n",
    "# Store initial state in history\n",
    "temperature_history = [temperature.copy()]\n",
    "phi_history = [phase.copy()]\n",
    "\n",
    "#print(temperature_history,phi_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finite difference method\n",
    "\n",
    "for m in range(1, num_steps+1):                  # time loop\n",
    "    for n in range(1,num_points-1):              # space loop, adjusted range\n",
    "        #print(f\"Step {m}, point {n},Temperature: {temperature}, Phase: {phase}\")\n",
    "        if temperature[n] >= T_L:\n",
    "            temperature[n] = temperature[n] + ((alpha * dt )/ dx**2) * (temperature[n+1] - 2.0 * temperature[n] + temperature[n-1])\n",
    "            phase[n] = 0\n",
    "         \n",
    "            #print(m,n,temperature[n],phase[n])\n",
    "        elif T_S < temperature[n] < T_L:\n",
    "            #temperature[n] = temperature[n] - (((k * dt) / (rho*(T_L-T_S)*(cp*(T_L-T_S)-L_fusion)*(dx**2))) * (temperature[n+1] - 2 * temperature[n] + temperature[n-1]))\n",
    "            temperature[n] = temperature[n] - ((k/(rho*(cp-(L_fusion/(T_L-T_S)))))* (temperature[n+1] - 2 * temperature[n] + temperature[n-1]))\n",
    "            phase[n] = (T_L - temperature[n]) / (T_L - T_S)\n",
    "            #print(m,n,temperature[n],phase[n])\n",
    "         \n",
    "        elif temperature[n]<T_S:\n",
    "            temperature[n] = temperature[n] + ((alpha * dt )/ dx**2) * (temperature[n+1] - 2.0 * temperature[n] + temperature[n-1])\n",
    "            phase[n] = 1\n",
    "            \n",
    "        else:\n",
    "            print(\"ERROR: should not be here\")\n",
    "         \n",
    "           # print(m,n,temperature[n],phase[n])\n",
    "    \n",
    "    temperature_history.append(temperature.copy())\n",
    "    phi_history.append(phase.copy())\n",
    "    #Print for debugging\n",
    "    #print(f\"Step {m}, space{n},Temperature: {temperature}, Phase: {phase}\")\n",
    "\n",
    "\n",
    "#print(temperature_history)\n",
    "#print(phi_history)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_lamda = 0.1    #material constant\n",
    "\n",
    "del_Pcr = 25     # difference between P liquidus and P at critical pressure\n",
    "u_l = 450000         # dynamic viscosity of liquid\n",
    "beta = (rho_s - rho_l)/ rho_l \n",
    "del_Tf = T_L - T_S\n",
    "   \n",
    "\n",
    "\n",
    "g = np.array(np.gradient(temperature_history, axis=1))\n",
    "t_dot = np.array(np.gradient(temperature_history, axis=0))\n",
    "t_dot = np.where(t_dot <= 0, 1e-10, t_dot)\n",
    "Main= g /(t_dot**(5/6))\n",
    "Pcr = (del_Pcr/u_l*beta*del_Tf)**(1/2)\n",
    "Niyama = c_lamda *(Pcr)\n",
    "#print(Main)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check for gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeatPINN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=75, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=75, out_features=75, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=75, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the neural network architecture\n",
    "class HeatPINN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size): # This is the constructor\n",
    "        super(HeatPINN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )                                                    \n",
    "\n",
    "    def forward(self, x, t):                               # This is the forward pass\n",
    "        input_features = torch.cat([x, t], dim=1)          # Concatenate the input features\n",
    "        return self.net(input_features)                     # Return the output of the network\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 75\n",
    "learning_rate = 1e-3\n",
    "epochs = 1000\n",
    "alpha = 0.01  # Adjust this value based on your problem\n",
    "\n",
    "# Initialize the model\n",
    "model = HeatPINN(input_size=2, hidden_size=hidden_size, output_size=1)\n",
    "print(model)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function for data \n",
    "\n",
    "def loss_fn_data(u_pred, u_true):\n",
    "    return nn.MSELoss()(u_pred, u_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for Aye Eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7385000, 2])\n",
      "torch.Size([7385000, 1])\n",
      "torch.Size([7385000, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training data preparation\n",
    "x = torch.linspace(0, 1, num_points).view(-1, 1).requires_grad_(True) # Spatial dimension of length l\n",
    "t = torch.linspace(0, 5, num_steps+1).view(-1, 1).requires_grad_(True) # Temporal dimension of length T\n",
    "X, T = torch.meshgrid(x.squeeze(), t.squeeze(), indexing='ij') # Create a meshgrid of X and T\n",
    "X = X.reshape(-1, 1)   # Reshape X to a column vector\n",
    "T = T.reshape(-1, 1)       # Reshape T to a column vector\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs = torch.cat([X, T], dim=1)\n",
    "\n",
    "temp_t = torch.tensor(np.array(temperature_history)) # Convert the temperature history to a tensor\n",
    "phase_t = torch.tensor(np.array(phi_history))          # Convert the phase history to a tensor\n",
    "temp_inp = temp_t.reshape(-1,1)\n",
    "phase_inp = phase_t.reshape(-1,1)\n",
    "print(inputs.shape)\n",
    "print(T.shape)\n",
    "print(temp_inp.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the custom data set for the training from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class  SpatiotemporalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, temp_inp, phase_inp):\n",
    "        self.inputs = inputs\n",
    "        self.temp_inp = temp_inp\n",
    "        self.phase_inp = phase_inp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.inputs[index], self.temp_inp[index], self.phase_inp[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "# Create the dataset and dataloader   \n",
    "dataset = SpatiotemporalDataset(inputs, temp_inp, phase_inp)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pde_loss tensor(0.0122, grad_fn=<MeanBackward0>) bc_loss tensor(561667.3750, grad_fn=<AddBackward0>) ic_loss tensor(757132.3750, grad_fn=<MeanBackward0>) data_loss tensor(549277.1404, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0083, grad_fn=<MeanBackward0>) bc_loss tensor(529588.2500, grad_fn=<AddBackward0>) ic_loss tensor(757081.7500, grad_fn=<MeanBackward0>) data_loss tensor(549173.4612, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0050, grad_fn=<MeanBackward0>) bc_loss tensor(499034.5625, grad_fn=<AddBackward0>) ic_loss tensor(757033.2500, grad_fn=<MeanBackward0>) data_loss tensor(549068.8875, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0026, grad_fn=<MeanBackward0>) bc_loss tensor(470331.4062, grad_fn=<AddBackward0>) ic_loss tensor(756985.8750, grad_fn=<MeanBackward0>) data_loss tensor(548965.0136, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0010, grad_fn=<MeanBackward0>) bc_loss tensor(444431.8125, grad_fn=<AddBackward0>) ic_loss tensor(756940.3125, grad_fn=<MeanBackward0>) data_loss tensor(548863.8248, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 5, Loss: 1750235.9497991225\n",
      "pde_loss tensor(0.0004, grad_fn=<MeanBackward0>) bc_loss tensor(419861.2500, grad_fn=<AddBackward0>) ic_loss tensor(756894.3750, grad_fn=<MeanBackward0>) data_loss tensor(548768.1905, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0005, grad_fn=<MeanBackward0>) bc_loss tensor(397096.2188, grad_fn=<AddBackward0>) ic_loss tensor(756848.1875, grad_fn=<MeanBackward0>) data_loss tensor(548676.0256, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0012, grad_fn=<MeanBackward0>) bc_loss tensor(375360.1562, grad_fn=<AddBackward0>) ic_loss tensor(756801.6250, grad_fn=<MeanBackward0>) data_loss tensor(548585.1748, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0026, grad_fn=<MeanBackward0>) bc_loss tensor(354513.1250, grad_fn=<AddBackward0>) ic_loss tensor(756754.7500, grad_fn=<MeanBackward0>) data_loss tensor(548495.7034, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0046, grad_fn=<MeanBackward0>) bc_loss tensor(334085.5312, grad_fn=<AddBackward0>) ic_loss tensor(756707.3750, grad_fn=<MeanBackward0>) data_loss tensor(548406.4730, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 10, Loss: 1639199.3480004703\n",
      "pde_loss tensor(0.0073, grad_fn=<MeanBackward0>) bc_loss tensor(313866.8438, grad_fn=<AddBackward0>) ic_loss tensor(756659.0625, grad_fn=<MeanBackward0>) data_loss tensor(548315.0168, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0110, grad_fn=<MeanBackward0>) bc_loss tensor(293864.6875, grad_fn=<AddBackward0>) ic_loss tensor(756609.6875, grad_fn=<MeanBackward0>) data_loss tensor(548220.0213, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0158, grad_fn=<MeanBackward0>) bc_loss tensor(275004.9688, grad_fn=<AddBackward0>) ic_loss tensor(756559.8125, grad_fn=<MeanBackward0>) data_loss tensor(548119.6395, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0219, grad_fn=<MeanBackward0>) bc_loss tensor(256412.4062, grad_fn=<AddBackward0>) ic_loss tensor(756509.1250, grad_fn=<MeanBackward0>) data_loss tensor(548013.9620, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0291, grad_fn=<MeanBackward0>) bc_loss tensor(237786.3438, grad_fn=<AddBackward0>) ic_loss tensor(756458.1875, grad_fn=<MeanBackward0>) data_loss tensor(547906.1880, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 15, Loss: 1542150.7505195402\n",
      "pde_loss tensor(0.0374, grad_fn=<MeanBackward0>) bc_loss tensor(219299., grad_fn=<AddBackward0>) ic_loss tensor(756407., grad_fn=<MeanBackward0>) data_loss tensor(547796.2988, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0469, grad_fn=<MeanBackward0>) bc_loss tensor(201229.4375, grad_fn=<AddBackward0>) ic_loss tensor(756355.0625, grad_fn=<MeanBackward0>) data_loss tensor(547683.4384, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0577, grad_fn=<MeanBackward0>) bc_loss tensor(183537.8906, grad_fn=<AddBackward0>) ic_loss tensor(756303.2500, grad_fn=<MeanBackward0>) data_loss tensor(547569.3738, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0702, grad_fn=<MeanBackward0>) bc_loss tensor(166040.0781, grad_fn=<AddBackward0>) ic_loss tensor(756249.5000, grad_fn=<MeanBackward0>) data_loss tensor(547451.5536, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.0844, grad_fn=<MeanBackward0>) bc_loss tensor(148492.5156, grad_fn=<AddBackward0>) ic_loss tensor(756193.9375, grad_fn=<MeanBackward0>) data_loss tensor(547330.6533, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 20, Loss: 1452017.1532876133\n",
      "pde_loss tensor(0.0999, grad_fn=<MeanBackward0>) bc_loss tensor(131488.8438, grad_fn=<AddBackward0>) ic_loss tensor(756136.8125, grad_fn=<MeanBackward0>) data_loss tensor(547208.0139, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.1171, grad_fn=<MeanBackward0>) bc_loss tensor(115151.1875, grad_fn=<AddBackward0>) ic_loss tensor(756076.8750, grad_fn=<MeanBackward0>) data_loss tensor(547081.0729, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.1363, grad_fn=<MeanBackward0>) bc_loss tensor(99428.9141, grad_fn=<AddBackward0>) ic_loss tensor(756014.3750, grad_fn=<MeanBackward0>) data_loss tensor(546950.1171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.1570, grad_fn=<MeanBackward0>) bc_loss tensor(84752.5156, grad_fn=<AddBackward0>) ic_loss tensor(755949.5625, grad_fn=<MeanBackward0>) data_loss tensor(546816.1649, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.1796, grad_fn=<MeanBackward0>) bc_loss tensor(70879.9062, grad_fn=<AddBackward0>) ic_loss tensor(755881.4375, grad_fn=<MeanBackward0>) data_loss tensor(546677.5948, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 25, Loss: 1373439.094816451\n",
      "pde_loss tensor(0.2045, grad_fn=<MeanBackward0>) bc_loss tensor(57966.4766, grad_fn=<AddBackward0>) ic_loss tensor(755810.8750, grad_fn=<MeanBackward0>) data_loss tensor(546534.2588, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.2317, grad_fn=<MeanBackward0>) bc_loss tensor(46147.4844, grad_fn=<AddBackward0>) ic_loss tensor(755738.1875, grad_fn=<MeanBackward0>) data_loss tensor(546386.3151, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.2614, grad_fn=<MeanBackward0>) bc_loss tensor(35541.4414, grad_fn=<AddBackward0>) ic_loss tensor(755662.9375, grad_fn=<MeanBackward0>) data_loss tensor(546233.9171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.2938, grad_fn=<MeanBackward0>) bc_loss tensor(26248.1855, grad_fn=<AddBackward0>) ic_loss tensor(755585.5625, grad_fn=<MeanBackward0>) data_loss tensor(546077.3463, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.3290, grad_fn=<MeanBackward0>) bc_loss tensor(18312.9746, grad_fn=<AddBackward0>) ic_loss tensor(755506.2500, grad_fn=<MeanBackward0>) data_loss tensor(545917.2408, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 30, Loss: 1319736.8032693616\n",
      "pde_loss tensor(0.3667, grad_fn=<MeanBackward0>) bc_loss tensor(11825.7051, grad_fn=<AddBackward0>) ic_loss tensor(755424.7500, grad_fn=<MeanBackward0>) data_loss tensor(545754.5027, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.4069, grad_fn=<MeanBackward0>) bc_loss tensor(6799.3340, grad_fn=<AddBackward0>) ic_loss tensor(755342., grad_fn=<MeanBackward0>) data_loss tensor(545589.5153, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.4495, grad_fn=<MeanBackward0>) bc_loss tensor(3222.2373, grad_fn=<AddBackward0>) ic_loss tensor(755258.5625, grad_fn=<MeanBackward0>) data_loss tensor(545423.2301, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.4942, grad_fn=<MeanBackward0>) bc_loss tensor(1016.5261, grad_fn=<AddBackward0>) ic_loss tensor(755175.3750, grad_fn=<MeanBackward0>) data_loss tensor(545256.7047, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.5406, grad_fn=<MeanBackward0>) bc_loss tensor(66.0961, grad_fn=<AddBackward0>) ic_loss tensor(755092.8125, grad_fn=<MeanBackward0>) data_loss tensor(545091.1650, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 35, Loss: 1300250.6024571776\n",
      "pde_loss tensor(0.5884, grad_fn=<MeanBackward0>) bc_loss tensor(200.3882, grad_fn=<AddBackward0>) ic_loss tensor(755011.6250, grad_fn=<MeanBackward0>) data_loss tensor(544928.0410, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.6368, grad_fn=<MeanBackward0>) bc_loss tensor(1196.7891, grad_fn=<AddBackward0>) ic_loss tensor(754933.0625, grad_fn=<MeanBackward0>) data_loss tensor(544769.0352, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.6852, grad_fn=<MeanBackward0>) bc_loss tensor(2787.5764, grad_fn=<AddBackward0>) ic_loss tensor(754857.8750, grad_fn=<MeanBackward0>) data_loss tensor(544616.1127, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.7329, grad_fn=<MeanBackward0>) bc_loss tensor(4683.7983, grad_fn=<AddBackward0>) ic_loss tensor(754787.1250, grad_fn=<MeanBackward0>) data_loss tensor(544471.0563, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.7790, grad_fn=<MeanBackward0>) bc_loss tensor(6607.1704, grad_fn=<AddBackward0>) ic_loss tensor(754721.7500, grad_fn=<MeanBackward0>) data_loss tensor(544335.2418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 40, Loss: 1305664.929280663\n",
      "pde_loss tensor(0.8230, grad_fn=<MeanBackward0>) bc_loss tensor(8320.8594, grad_fn=<AddBackward0>) ic_loss tensor(754661.7500, grad_fn=<MeanBackward0>) data_loss tensor(544209.5489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.8644, grad_fn=<MeanBackward0>) bc_loss tensor(9649.8770, grad_fn=<AddBackward0>) ic_loss tensor(754607.7500, grad_fn=<MeanBackward0>) data_loss tensor(544094.4671, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.9030, grad_fn=<MeanBackward0>) bc_loss tensor(10489.9219, grad_fn=<AddBackward0>) ic_loss tensor(754559.4375, grad_fn=<MeanBackward0>) data_loss tensor(543989.9415, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.9386, grad_fn=<MeanBackward0>) bc_loss tensor(10806.9102, grad_fn=<AddBackward0>) ic_loss tensor(754516.6875, grad_fn=<MeanBackward0>) data_loss tensor(543895.5518, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(0.9713, grad_fn=<MeanBackward0>) bc_loss tensor(10625.0195, grad_fn=<AddBackward0>) ic_loss tensor(754478.7500, grad_fn=<MeanBackward0>) data_loss tensor(543810.6040, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 45, Loss: 1308915.3540439846\n",
      "pde_loss tensor(1.0011, grad_fn=<MeanBackward0>) bc_loss tensor(10013.1455, grad_fn=<AddBackward0>) ic_loss tensor(754445.4375, grad_fn=<MeanBackward0>) data_loss tensor(543734.1947, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.0284, grad_fn=<MeanBackward0>) bc_loss tensor(9068.2266, grad_fn=<AddBackward0>) ic_loss tensor(754416., grad_fn=<MeanBackward0>) data_loss tensor(543665.2605, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.0535, grad_fn=<MeanBackward0>) bc_loss tensor(7899.7773, grad_fn=<AddBackward0>) ic_loss tensor(754389.9375, grad_fn=<MeanBackward0>) data_loss tensor(543602.6325, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.0768, grad_fn=<MeanBackward0>) bc_loss tensor(6616.4805, grad_fn=<AddBackward0>) ic_loss tensor(754366.3750, grad_fn=<MeanBackward0>) data_loss tensor(543545.0851, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.0988, grad_fn=<MeanBackward0>) bc_loss tensor(5339.4556, grad_fn=<AddBackward0>) ic_loss tensor(754344.7500, grad_fn=<MeanBackward0>) data_loss tensor(543491.3668, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 50, Loss: 1303176.67930111\n",
      "pde_loss tensor(1.1197, grad_fn=<MeanBackward0>) bc_loss tensor(4126.7349, grad_fn=<AddBackward0>) ic_loss tensor(754324.4375, grad_fn=<MeanBackward0>) data_loss tensor(543440.2406, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.1402, grad_fn=<MeanBackward0>) bc_loss tensor(3030.2703, grad_fn=<AddBackward0>) ic_loss tensor(754304.7500, grad_fn=<MeanBackward0>) data_loss tensor(543390.5880, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.1606, grad_fn=<MeanBackward0>) bc_loss tensor(2091.6699, grad_fn=<AddBackward0>) ic_loss tensor(754285.3125, grad_fn=<MeanBackward0>) data_loss tensor(543341.3359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.1814, grad_fn=<MeanBackward0>) bc_loss tensor(1333.2241, grad_fn=<AddBackward0>) ic_loss tensor(754265.4375, grad_fn=<MeanBackward0>) data_loss tensor(543291.4821, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.2029, grad_fn=<MeanBackward0>) bc_loss tensor(759.8994, grad_fn=<AddBackward0>) ic_loss tensor(754244.8125, grad_fn=<MeanBackward0>) data_loss tensor(543240.1005, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 55, Loss: 1298246.0380061385\n",
      "pde_loss tensor(1.2255, grad_fn=<MeanBackward0>) bc_loss tensor(362.7963, grad_fn=<AddBackward0>) ic_loss tensor(754223.1250, grad_fn=<MeanBackward0>) data_loss tensor(543186.3535, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.2497, grad_fn=<MeanBackward0>) bc_loss tensor(122.8639, grad_fn=<AddBackward0>) ic_loss tensor(754199.6250, grad_fn=<MeanBackward0>) data_loss tensor(543129.4863, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.2756, grad_fn=<MeanBackward0>) bc_loss tensor(14.3342, grad_fn=<AddBackward0>) ic_loss tensor(754174.3125, grad_fn=<MeanBackward0>) data_loss tensor(543068.8455, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.3036, grad_fn=<MeanBackward0>) bc_loss tensor(8.0025, grad_fn=<AddBackward0>) ic_loss tensor(754146.9375, grad_fn=<MeanBackward0>) data_loss tensor(543003.8813, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.3339, grad_fn=<MeanBackward0>) bc_loss tensor(74.0606, grad_fn=<AddBackward0>) ic_loss tensor(754117.1250, grad_fn=<MeanBackward0>) data_loss tensor(542934.1454, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 60, Loss: 1297126.6453849697\n",
      "pde_loss tensor(1.3668, grad_fn=<MeanBackward0>) bc_loss tensor(184.1106, grad_fn=<AddBackward0>) ic_loss tensor(754084.6250, grad_fn=<MeanBackward0>) data_loss tensor(542859.2864, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.4026, grad_fn=<MeanBackward0>) bc_loss tensor(309.6396, grad_fn=<AddBackward0>) ic_loss tensor(754049.5000, grad_fn=<MeanBackward0>) data_loss tensor(542779.0515, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.4412, grad_fn=<MeanBackward0>) bc_loss tensor(431.9463, grad_fn=<AddBackward0>) ic_loss tensor(754011.8125, grad_fn=<MeanBackward0>) data_loss tensor(542693.2838, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.4830, grad_fn=<MeanBackward0>) bc_loss tensor(534.8530, grad_fn=<AddBackward0>) ic_loss tensor(753971.3125, grad_fn=<MeanBackward0>) data_loss tensor(542601.8736, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.5279, grad_fn=<MeanBackward0>) bc_loss tensor(607.5362, grad_fn=<AddBackward0>) ic_loss tensor(753928.1250, grad_fn=<MeanBackward0>) data_loss tensor(542504.8148, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 65, Loss: 1297042.0022833557\n",
      "pde_loss tensor(1.5762, grad_fn=<MeanBackward0>) bc_loss tensor(643.9659, grad_fn=<AddBackward0>) ic_loss tensor(753882.0625, grad_fn=<MeanBackward0>) data_loss tensor(542402.1505, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.6279, grad_fn=<MeanBackward0>) bc_loss tensor(642.7593, grad_fn=<AddBackward0>) ic_loss tensor(753833.4375, grad_fn=<MeanBackward0>) data_loss tensor(542294.0005, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.6829, grad_fn=<MeanBackward0>) bc_loss tensor(607.4359, grad_fn=<AddBackward0>) ic_loss tensor(753782.2500, grad_fn=<MeanBackward0>) data_loss tensor(542180.5503, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.7414, grad_fn=<MeanBackward0>) bc_loss tensor(543.7391, grad_fn=<AddBackward0>) ic_loss tensor(753728.6250, grad_fn=<MeanBackward0>) data_loss tensor(542062.0317, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.8033, grad_fn=<MeanBackward0>) bc_loss tensor(460.5144, grad_fn=<AddBackward0>) ic_loss tensor(753672.9375, grad_fn=<MeanBackward0>) data_loss tensor(541938.7194, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 70, Loss: 1296073.9694452793\n",
      "pde_loss tensor(1.8685, grad_fn=<MeanBackward0>) bc_loss tensor(366.9332, grad_fn=<AddBackward0>) ic_loss tensor(753615.0625, grad_fn=<MeanBackward0>) data_loss tensor(541810.9318, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1.9371, grad_fn=<MeanBackward0>) bc_loss tensor(271.8554, grad_fn=<AddBackward0>) ic_loss tensor(753555.1875, grad_fn=<MeanBackward0>) data_loss tensor(541679.0250, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2.0089, grad_fn=<MeanBackward0>) bc_loss tensor(183.3824, grad_fn=<AddBackward0>) ic_loss tensor(753493.8125, grad_fn=<MeanBackward0>) data_loss tensor(541543.3854, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2.0837, grad_fn=<MeanBackward0>) bc_loss tensor(108.2374, grad_fn=<AddBackward0>) ic_loss tensor(753430.8750, grad_fn=<MeanBackward0>) data_loss tensor(541404.4264, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2.1616, grad_fn=<MeanBackward0>) bc_loss tensor(51.3178, grad_fn=<AddBackward0>) ic_loss tensor(753366.8125, grad_fn=<MeanBackward0>) data_loss tensor(541262.5815, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 75, Loss: 1294682.89395642\n",
      "pde_loss tensor(2.2422, grad_fn=<MeanBackward0>) bc_loss tensor(15.4239, grad_fn=<AddBackward0>) ic_loss tensor(753301.6875, grad_fn=<MeanBackward0>) data_loss tensor(541118.2922, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2.3255, grad_fn=<MeanBackward0>) bc_loss tensor(1.1677, grad_fn=<AddBackward0>) ic_loss tensor(753235.5625, grad_fn=<MeanBackward0>) data_loss tensor(540971.9936, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2.4112, grad_fn=<MeanBackward0>) bc_loss tensor(7.0776, grad_fn=<AddBackward0>) ic_loss tensor(753168.6875, grad_fn=<MeanBackward0>) data_loss tensor(540824.1121, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2.4990, grad_fn=<MeanBackward0>) bc_loss tensor(29.8726, grad_fn=<AddBackward0>) ic_loss tensor(753101.5625, grad_fn=<MeanBackward0>) data_loss tensor(540675.0517, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2.5889, grad_fn=<MeanBackward0>) bc_loss tensor(65.2139, grad_fn=<AddBackward0>) ic_loss tensor(753034.1875, grad_fn=<MeanBackward0>) data_loss tensor(540525.1759, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 80, Loss: 1293627.1759282253\n",
      "pde_loss tensor(2.6807, grad_fn=<MeanBackward0>) bc_loss tensor(108.5885, grad_fn=<AddBackward0>) ic_loss tensor(752966.7500, grad_fn=<MeanBackward0>) data_loss tensor(540374.8705, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2.7743, grad_fn=<MeanBackward0>) bc_loss tensor(151.0166, grad_fn=<AddBackward0>) ic_loss tensor(752899.6875, grad_fn=<MeanBackward0>) data_loss tensor(540224.4457, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2.8695, grad_fn=<MeanBackward0>) bc_loss tensor(189.0722, grad_fn=<AddBackward0>) ic_loss tensor(752833.0625, grad_fn=<MeanBackward0>) data_loss tensor(540074.0004, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2.9663, grad_fn=<MeanBackward0>) bc_loss tensor(220.2740, grad_fn=<AddBackward0>) ic_loss tensor(752766.6875, grad_fn=<MeanBackward0>) data_loss tensor(539923.5871, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3.0647, grad_fn=<MeanBackward0>) bc_loss tensor(241.2487, grad_fn=<AddBackward0>) ic_loss tensor(752700.6250, grad_fn=<MeanBackward0>) data_loss tensor(539773.1897, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 85, Loss: 1292718.1272443498\n",
      "pde_loss tensor(3.1647, grad_fn=<MeanBackward0>) bc_loss tensor(250.6193, grad_fn=<AddBackward0>) ic_loss tensor(752634.9375, grad_fn=<MeanBackward0>) data_loss tensor(539622.7356, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3.2664, grad_fn=<MeanBackward0>) bc_loss tensor(248.5272, grad_fn=<AddBackward0>) ic_loss tensor(752569.3125, grad_fn=<MeanBackward0>) data_loss tensor(539472.0711, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3.3698, grad_fn=<MeanBackward0>) bc_loss tensor(236.6606, grad_fn=<AddBackward0>) ic_loss tensor(752503.8750, grad_fn=<MeanBackward0>) data_loss tensor(539320.9722, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3.4751, grad_fn=<MeanBackward0>) bc_loss tensor(216.4568, grad_fn=<AddBackward0>) ic_loss tensor(752438.2500, grad_fn=<MeanBackward0>) data_loss tensor(539169.3216, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3.5827, grad_fn=<MeanBackward0>) bc_loss tensor(189.7682, grad_fn=<AddBackward0>) ic_loss tensor(752372.6875, grad_fn=<MeanBackward0>) data_loss tensor(539016.7220, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 90, Loss: 1291582.784536085\n",
      "pde_loss tensor(3.6927, grad_fn=<MeanBackward0>) bc_loss tensor(160.0431, grad_fn=<AddBackward0>) ic_loss tensor(752306.6875, grad_fn=<MeanBackward0>) data_loss tensor(538862.7289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3.8056, grad_fn=<MeanBackward0>) bc_loss tensor(129.9496, grad_fn=<AddBackward0>) ic_loss tensor(752240.1875, grad_fn=<MeanBackward0>) data_loss tensor(538706.8848, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3.9218, grad_fn=<MeanBackward0>) bc_loss tensor(101.2594, grad_fn=<AddBackward0>) ic_loss tensor(752173., grad_fn=<MeanBackward0>) data_loss tensor(538548.7591, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4.0414, grad_fn=<MeanBackward0>) bc_loss tensor(75.5210, grad_fn=<AddBackward0>) ic_loss tensor(752104.8125, grad_fn=<MeanBackward0>) data_loss tensor(538387.9340, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4.1659, grad_fn=<MeanBackward0>) bc_loss tensor(53.7319, grad_fn=<AddBackward0>) ic_loss tensor(752035.5625, grad_fn=<MeanBackward0>) data_loss tensor(538224.0278, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 95, Loss: 1290317.465297032\n",
      "pde_loss tensor(4.2963, grad_fn=<MeanBackward0>) bc_loss tensor(36.3371, grad_fn=<AddBackward0>) ic_loss tensor(751965.0625, grad_fn=<MeanBackward0>) data_loss tensor(538056.7464, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4.4318, grad_fn=<MeanBackward0>) bc_loss tensor(23.2904, grad_fn=<AddBackward0>) ic_loss tensor(751893.2500, grad_fn=<MeanBackward0>) data_loss tensor(537885.8118, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4.5725, grad_fn=<MeanBackward0>) bc_loss tensor(14.1666, grad_fn=<AddBackward0>) ic_loss tensor(751819.8125, grad_fn=<MeanBackward0>) data_loss tensor(537710.9235, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4.7187, grad_fn=<MeanBackward0>) bc_loss tensor(8.2886, grad_fn=<AddBackward0>) ic_loss tensor(751744.8750, grad_fn=<MeanBackward0>) data_loss tensor(537531.8076, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4.8707, grad_fn=<MeanBackward0>) bc_loss tensor(4.8664, grad_fn=<AddBackward0>) ic_loss tensor(751668., grad_fn=<MeanBackward0>) data_loss tensor(537348.2317, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 100, Loss: 1289025.981650383\n",
      "pde_loss tensor(5.0286, grad_fn=<MeanBackward0>) bc_loss tensor(3.0977, grad_fn=<AddBackward0>) ic_loss tensor(751589.3750, grad_fn=<MeanBackward0>) data_loss tensor(537160.0157, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5.1927, grad_fn=<MeanBackward0>) bc_loss tensor(2.3570, grad_fn=<AddBackward0>) ic_loss tensor(751508.6250, grad_fn=<MeanBackward0>) data_loss tensor(536967.0382, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5.3633, grad_fn=<MeanBackward0>) bc_loss tensor(2.1894, grad_fn=<AddBackward0>) ic_loss tensor(751426.1250, grad_fn=<MeanBackward0>) data_loss tensor(536769.2359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5.5405, grad_fn=<MeanBackward0>) bc_loss tensor(2.2839, grad_fn=<AddBackward0>) ic_loss tensor(751341.6250, grad_fn=<MeanBackward0>) data_loss tensor(536566.5815, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5.7250, grad_fn=<MeanBackward0>) bc_loss tensor(2.7120, grad_fn=<AddBackward0>) ic_loss tensor(751255.3750, grad_fn=<MeanBackward0>) data_loss tensor(536359.1113, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 105, Loss: 1287622.923834417\n",
      "pde_loss tensor(5.9201, grad_fn=<MeanBackward0>) bc_loss tensor(3.6460, grad_fn=<AddBackward0>) ic_loss tensor(751167.0625, grad_fn=<MeanBackward0>) data_loss tensor(536146.9475, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6.1234, grad_fn=<MeanBackward0>) bc_loss tensor(5.2676, grad_fn=<AddBackward0>) ic_loss tensor(751077., grad_fn=<MeanBackward0>) data_loss tensor(535930.2658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6.3347, grad_fn=<MeanBackward0>) bc_loss tensor(8.4523, grad_fn=<AddBackward0>) ic_loss tensor(750985.3125, grad_fn=<MeanBackward0>) data_loss tensor(535709.2267, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6.5543, grad_fn=<MeanBackward0>) bc_loss tensor(13.5164, grad_fn=<AddBackward0>) ic_loss tensor(750891.8750, grad_fn=<MeanBackward0>) data_loss tensor(535484.1543, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6.7823, grad_fn=<MeanBackward0>) bc_loss tensor(18.6700, grad_fn=<AddBackward0>) ic_loss tensor(750797.2500, grad_fn=<MeanBackward0>) data_loss tensor(535255.3291, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 110, Loss: 1286078.0165836771\n",
      "pde_loss tensor(7.0184, grad_fn=<MeanBackward0>) bc_loss tensor(23.0462, grad_fn=<AddBackward0>) ic_loss tensor(750701.3125, grad_fn=<MeanBackward0>) data_loss tensor(535022.9331, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7.2623, grad_fn=<MeanBackward0>) bc_loss tensor(26.2098, grad_fn=<AddBackward0>) ic_loss tensor(750604.0625, grad_fn=<MeanBackward0>) data_loss tensor(534787.0170, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7.5142, grad_fn=<MeanBackward0>) bc_loss tensor(28.0125, grad_fn=<AddBackward0>) ic_loss tensor(750505.4375, grad_fn=<MeanBackward0>) data_loss tensor(534547.5458, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7.7743, grad_fn=<MeanBackward0>) bc_loss tensor(28.3600, grad_fn=<AddBackward0>) ic_loss tensor(750405.6250, grad_fn=<MeanBackward0>) data_loss tensor(534304.4395, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8.0429, grad_fn=<MeanBackward0>) bc_loss tensor(31.6681, grad_fn=<AddBackward0>) ic_loss tensor(750304.2500, grad_fn=<MeanBackward0>) data_loss tensor(534057.6027, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 115, Loss: 1284401.540222933\n",
      "pde_loss tensor(8.3203, grad_fn=<MeanBackward0>) bc_loss tensor(38.4521, grad_fn=<AddBackward0>) ic_loss tensor(750201.6875, grad_fn=<MeanBackward0>) data_loss tensor(533807.0945, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8.6065, grad_fn=<MeanBackward0>) bc_loss tensor(42.8033, grad_fn=<AddBackward0>) ic_loss tensor(750097.7500, grad_fn=<MeanBackward0>) data_loss tensor(533552.9551, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8.9017, grad_fn=<MeanBackward0>) bc_loss tensor(44.9870, grad_fn=<AddBackward0>) ic_loss tensor(749992.4375, grad_fn=<MeanBackward0>) data_loss tensor(533295.1507, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9.2063, grad_fn=<MeanBackward0>) bc_loss tensor(47.1589, grad_fn=<AddBackward0>) ic_loss tensor(749885.8750, grad_fn=<MeanBackward0>) data_loss tensor(533033.6332, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9.5205, grad_fn=<MeanBackward0>) bc_loss tensor(50.8616, grad_fn=<AddBackward0>) ic_loss tensor(749777.6875, grad_fn=<MeanBackward0>) data_loss tensor(532768.2867, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 120, Loss: 1282606.3492150286\n",
      "pde_loss tensor(9.8445, grad_fn=<MeanBackward0>) bc_loss tensor(53.2463, grad_fn=<AddBackward0>) ic_loss tensor(749668.0625, grad_fn=<MeanBackward0>) data_loss tensor(532499.0942, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10.1788, grad_fn=<MeanBackward0>) bc_loss tensor(54.0755, grad_fn=<AddBackward0>) ic_loss tensor(749557.0625, grad_fn=<MeanBackward0>) data_loss tensor(532226.0154, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10.5235, grad_fn=<MeanBackward0>) bc_loss tensor(53.6124, grad_fn=<AddBackward0>) ic_loss tensor(749444.3125, grad_fn=<MeanBackward0>) data_loss tensor(531948.9684, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10.8792, grad_fn=<MeanBackward0>) bc_loss tensor(52.3362, grad_fn=<AddBackward0>) ic_loss tensor(749330.0625, grad_fn=<MeanBackward0>) data_loss tensor(531667.8269, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11.2462, grad_fn=<MeanBackward0>) bc_loss tensor(50.8720, grad_fn=<AddBackward0>) ic_loss tensor(749213.8750, grad_fn=<MeanBackward0>) data_loss tensor(531382.4292, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 125, Loss: 1280658.4291517858\n",
      "pde_loss tensor(11.6249, grad_fn=<MeanBackward0>) bc_loss tensor(49.7358, grad_fn=<AddBackward0>) ic_loss tensor(749096., grad_fn=<MeanBackward0>) data_loss tensor(531092.5954, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12.0160, grad_fn=<MeanBackward0>) bc_loss tensor(49.0798, grad_fn=<AddBackward0>) ic_loss tensor(748976.1875, grad_fn=<MeanBackward0>) data_loss tensor(530798.1441, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12.4198, grad_fn=<MeanBackward0>) bc_loss tensor(48.7511, grad_fn=<AddBackward0>) ic_loss tensor(748854.3750, grad_fn=<MeanBackward0>) data_loss tensor(530498.9084, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12.8367, grad_fn=<MeanBackward0>) bc_loss tensor(48.4722, grad_fn=<AddBackward0>) ic_loss tensor(748730.3750, grad_fn=<MeanBackward0>) data_loss tensor(530194.7682, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13.2675, grad_fn=<MeanBackward0>) bc_loss tensor(49.1069, grad_fn=<AddBackward0>) ic_loss tensor(748604.3125, grad_fn=<MeanBackward0>) data_loss tensor(529885.6227, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 130, Loss: 1278552.3102412499\n",
      "pde_loss tensor(13.7124, grad_fn=<MeanBackward0>) bc_loss tensor(54.4616, grad_fn=<AddBackward0>) ic_loss tensor(748476.0625, grad_fn=<MeanBackward0>) data_loss tensor(529571.3859, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14.1719, grad_fn=<MeanBackward0>) bc_loss tensor(57.5005, grad_fn=<AddBackward0>) ic_loss tensor(748346.2500, grad_fn=<MeanBackward0>) data_loss tensor(529252.3627, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14.6462, grad_fn=<MeanBackward0>) bc_loss tensor(60.1204, grad_fn=<AddBackward0>) ic_loss tensor(748214.3750, grad_fn=<MeanBackward0>) data_loss tensor(528928.5226, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15.1359, grad_fn=<MeanBackward0>) bc_loss tensor(61.7226, grad_fn=<AddBackward0>) ic_loss tensor(748081., grad_fn=<MeanBackward0>) data_loss tensor(528600.1119, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15.6413, grad_fn=<MeanBackward0>) bc_loss tensor(61.5646, grad_fn=<AddBackward0>) ic_loss tensor(747945.8750, grad_fn=<MeanBackward0>) data_loss tensor(528266.8203, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 135, Loss: 1276289.8827684359\n",
      "pde_loss tensor(16.1631, grad_fn=<MeanBackward0>) bc_loss tensor(62.5670, grad_fn=<AddBackward0>) ic_loss tensor(747808.5625, grad_fn=<MeanBackward0>) data_loss tensor(527928.3071, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16.7020, grad_fn=<MeanBackward0>) bc_loss tensor(65.4464, grad_fn=<AddBackward0>) ic_loss tensor(747669.0625, grad_fn=<MeanBackward0>) data_loss tensor(527584.3460, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17.2582, grad_fn=<MeanBackward0>) bc_loss tensor(70.2616, grad_fn=<AddBackward0>) ic_loss tensor(747527.3750, grad_fn=<MeanBackward0>) data_loss tensor(527235.1254, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17.8325, grad_fn=<MeanBackward0>) bc_loss tensor(70.7639, grad_fn=<AddBackward0>) ic_loss tensor(747383.7500, grad_fn=<MeanBackward0>) data_loss tensor(526880.7470, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18.4253, grad_fn=<MeanBackward0>) bc_loss tensor(68.3680, grad_fn=<AddBackward0>) ic_loss tensor(747238.1875, grad_fn=<MeanBackward0>) data_loss tensor(526520.9825, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 140, Loss: 1273845.9824618592\n",
      "pde_loss tensor(19.0375, grad_fn=<MeanBackward0>) bc_loss tensor(66.0100, grad_fn=<AddBackward0>) ic_loss tensor(747090.1250, grad_fn=<MeanBackward0>) data_loss tensor(526155.5658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19.6699, grad_fn=<MeanBackward0>) bc_loss tensor(65.0706, grad_fn=<AddBackward0>) ic_loss tensor(746939.6875, grad_fn=<MeanBackward0>) data_loss tensor(525784.2122, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20.3233, grad_fn=<MeanBackward0>) bc_loss tensor(64.3295, grad_fn=<AddBackward0>) ic_loss tensor(746786.1875, grad_fn=<MeanBackward0>) data_loss tensor(525406.7284, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20.9987, grad_fn=<MeanBackward0>) bc_loss tensor(64.9737, grad_fn=<AddBackward0>) ic_loss tensor(746630.0625, grad_fn=<MeanBackward0>) data_loss tensor(525023.0489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21.6970, grad_fn=<MeanBackward0>) bc_loss tensor(66.9118, grad_fn=<AddBackward0>) ic_loss tensor(746471.3125, grad_fn=<MeanBackward0>) data_loss tensor(524633.1752, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 145, Loss: 1271193.112715887\n",
      "pde_loss tensor(22.4192, grad_fn=<MeanBackward0>) bc_loss tensor(65.4699, grad_fn=<AddBackward0>) ic_loss tensor(746310.0625, grad_fn=<MeanBackward0>) data_loss tensor(524237.1430, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23.1658, grad_fn=<MeanBackward0>) bc_loss tensor(61.6430, grad_fn=<AddBackward0>) ic_loss tensor(746146.3125, grad_fn=<MeanBackward0>) data_loss tensor(523834.7411, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23.9378, grad_fn=<MeanBackward0>) bc_loss tensor(59.5697, grad_fn=<AddBackward0>) ic_loss tensor(745979.7500, grad_fn=<MeanBackward0>) data_loss tensor(523425.4969, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24.7359, grad_fn=<MeanBackward0>) bc_loss tensor(64.8082, grad_fn=<AddBackward0>) ic_loss tensor(745810.1250, grad_fn=<MeanBackward0>) data_loss tensor(523009.1922, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25.5594, grad_fn=<MeanBackward0>) bc_loss tensor(69.6131, grad_fn=<AddBackward0>) ic_loss tensor(745637.8750, grad_fn=<MeanBackward0>) data_loss tensor(522586.6603, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 150, Loss: 1268319.7227829061\n",
      "pde_loss tensor(26.4089, grad_fn=<MeanBackward0>) bc_loss tensor(71.1023, grad_fn=<AddBackward0>) ic_loss tensor(745463.1875, grad_fn=<MeanBackward0>) data_loss tensor(522157.9706, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27.2852, grad_fn=<MeanBackward0>) bc_loss tensor(71.6831, grad_fn=<AddBackward0>) ic_loss tensor(745285.8125, grad_fn=<MeanBackward0>) data_loss tensor(521722.8941, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(28.1893, grad_fn=<MeanBackward0>) bc_loss tensor(69.9509, grad_fn=<AddBackward0>) ic_loss tensor(745105.6875, grad_fn=<MeanBackward0>) data_loss tensor(521281.3658, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(29.1222, grad_fn=<MeanBackward0>) bc_loss tensor(67.2057, grad_fn=<AddBackward0>) ic_loss tensor(744922.8125, grad_fn=<MeanBackward0>) data_loss tensor(520833.2123, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(30.0853, grad_fn=<MeanBackward0>) bc_loss tensor(65.9500, grad_fn=<AddBackward0>) ic_loss tensor(744737.5000, grad_fn=<MeanBackward0>) data_loss tensor(520378.3898, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 155, Loss: 1265211.9523310827\n",
      "pde_loss tensor(31.0796, grad_fn=<MeanBackward0>) bc_loss tensor(74.0358, grad_fn=<AddBackward0>) ic_loss tensor(744549.8750, grad_fn=<MeanBackward0>) data_loss tensor(519917.1462, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(32.1067, grad_fn=<MeanBackward0>) bc_loss tensor(72.0473, grad_fn=<AddBackward0>) ic_loss tensor(744360.8125, grad_fn=<MeanBackward0>) data_loss tensor(519449.5654, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(33.1683, grad_fn=<MeanBackward0>) bc_loss tensor(63.8089, grad_fn=<AddBackward0>) ic_loss tensor(744168.8750, grad_fn=<MeanBackward0>) data_loss tensor(518974.6680, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(34.2663, grad_fn=<MeanBackward0>) bc_loss tensor(57.4072, grad_fn=<AddBackward0>) ic_loss tensor(743973.8750, grad_fn=<MeanBackward0>) data_loss tensor(518491.9217, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(35.4021, grad_fn=<MeanBackward0>) bc_loss tensor(56.5057, grad_fn=<AddBackward0>) ic_loss tensor(743775.3750, grad_fn=<MeanBackward0>) data_loss tensor(518000.7057, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 160, Loss: 1261868.0182331724\n",
      "pde_loss tensor(36.5771, grad_fn=<MeanBackward0>) bc_loss tensor(60.1535, grad_fn=<AddBackward0>) ic_loss tensor(743572.9375, grad_fn=<MeanBackward0>) data_loss tensor(517500.8666, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(37.7912, grad_fn=<MeanBackward0>) bc_loss tensor(61.5322, grad_fn=<AddBackward0>) ic_loss tensor(743366.5625, grad_fn=<MeanBackward0>) data_loss tensor(516993.0825, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(39.0464, grad_fn=<MeanBackward0>) bc_loss tensor(60.8987, grad_fn=<AddBackward0>) ic_loss tensor(743156.0625, grad_fn=<MeanBackward0>) data_loss tensor(516476.6897, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(40.3441, grad_fn=<MeanBackward0>) bc_loss tensor(61.5790, grad_fn=<AddBackward0>) ic_loss tensor(742941.3750, grad_fn=<MeanBackward0>) data_loss tensor(515951.3144, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(41.6847, grad_fn=<MeanBackward0>) bc_loss tensor(63.1383, grad_fn=<AddBackward0>) ic_loss tensor(742720.9375, grad_fn=<MeanBackward0>) data_loss tensor(515416.6606, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 165, Loss: 1258242.4106113482\n",
      "pde_loss tensor(43.0695, grad_fn=<MeanBackward0>) bc_loss tensor(67.6357, grad_fn=<AddBackward0>) ic_loss tensor(742495.3125, grad_fn=<MeanBackward0>) data_loss tensor(514872.7980, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(44.4973, grad_fn=<MeanBackward0>) bc_loss tensor(71.3476, grad_fn=<AddBackward0>) ic_loss tensor(742265.7500, grad_fn=<MeanBackward0>) data_loss tensor(514321.1183, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(45.9696, grad_fn=<MeanBackward0>) bc_loss tensor(72.1212, grad_fn=<AddBackward0>) ic_loss tensor(742032.1250, grad_fn=<MeanBackward0>) data_loss tensor(513761.2678, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(47.4883, grad_fn=<MeanBackward0>) bc_loss tensor(70.0591, grad_fn=<AddBackward0>) ic_loss tensor(741794.8125, grad_fn=<MeanBackward0>) data_loss tensor(513192.9809, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(49.0578, grad_fn=<MeanBackward0>) bc_loss tensor(70.2485, grad_fn=<AddBackward0>) ic_loss tensor(741552.9375, grad_fn=<MeanBackward0>) data_loss tensor(512615.0130, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 170, Loss: 1254287.2630461252\n",
      "pde_loss tensor(50.6803, grad_fn=<MeanBackward0>) bc_loss tensor(71.2188, grad_fn=<AddBackward0>) ic_loss tensor(741306.2500, grad_fn=<MeanBackward0>) data_loss tensor(512026.9852, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(52.3573, grad_fn=<MeanBackward0>) bc_loss tensor(72.9055, grad_fn=<AddBackward0>) ic_loss tensor(741054.8750, grad_fn=<MeanBackward0>) data_loss tensor(511428.9655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(54.0906, grad_fn=<MeanBackward0>) bc_loss tensor(75.8858, grad_fn=<AddBackward0>) ic_loss tensor(740799.2500, grad_fn=<MeanBackward0>) data_loss tensor(510821.1863, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(55.8805, grad_fn=<MeanBackward0>) bc_loss tensor(79.9744, grad_fn=<AddBackward0>) ic_loss tensor(740540.3125, grad_fn=<MeanBackward0>) data_loss tensor(510204.4446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(57.7263, grad_fn=<MeanBackward0>) bc_loss tensor(83.3152, grad_fn=<AddBackward0>) ic_loss tensor(740278.6250, grad_fn=<MeanBackward0>) data_loss tensor(509579.8021, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 175, Loss: 1249999.489607445\n",
      "pde_loss tensor(59.6321, grad_fn=<MeanBackward0>) bc_loss tensor(85.9935, grad_fn=<AddBackward0>) ic_loss tensor(740014.8125, grad_fn=<MeanBackward0>) data_loss tensor(508947.1171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(61.6013, grad_fn=<MeanBackward0>) bc_loss tensor(91.0980, grad_fn=<AddBackward0>) ic_loss tensor(739747.4375, grad_fn=<MeanBackward0>) data_loss tensor(508304.8165, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(63.6357, grad_fn=<MeanBackward0>) bc_loss tensor(90.8276, grad_fn=<AddBackward0>) ic_loss tensor(739476., grad_fn=<MeanBackward0>) data_loss tensor(507652.6712, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(65.7372, grad_fn=<MeanBackward0>) bc_loss tensor(87.3297, grad_fn=<AddBackward0>) ic_loss tensor(739200.5000, grad_fn=<MeanBackward0>) data_loss tensor(506990.3724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(67.9084, grad_fn=<MeanBackward0>) bc_loss tensor(84.8734, grad_fn=<AddBackward0>) ic_loss tensor(738920.1250, grad_fn=<MeanBackward0>) data_loss tensor(506317.3659, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 180, Loss: 1245390.3034448915\n",
      "pde_loss tensor(70.1504, grad_fn=<MeanBackward0>) bc_loss tensor(82.7345, grad_fn=<AddBackward0>) ic_loss tensor(738635.0625, grad_fn=<MeanBackward0>) data_loss tensor(505633.5875, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(72.4641, grad_fn=<MeanBackward0>) bc_loss tensor(80.1276, grad_fn=<AddBackward0>) ic_loss tensor(738345.3125, grad_fn=<MeanBackward0>) data_loss tensor(504939.4078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(74.8541, grad_fn=<MeanBackward0>) bc_loss tensor(80.3500, grad_fn=<AddBackward0>) ic_loss tensor(738050.6250, grad_fn=<MeanBackward0>) data_loss tensor(504234.0998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(77.3248, grad_fn=<MeanBackward0>) bc_loss tensor(80.9419, grad_fn=<AddBackward0>) ic_loss tensor(737751.1250, grad_fn=<MeanBackward0>) data_loss tensor(503517.1665, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(79.8785, grad_fn=<MeanBackward0>) bc_loss tensor(81.2124, grad_fn=<AddBackward0>) ic_loss tensor(737446.6875, grad_fn=<MeanBackward0>) data_loss tensor(502788.7370, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 185, Loss: 1240396.487045037\n",
      "pde_loss tensor(82.5170, grad_fn=<MeanBackward0>) bc_loss tensor(79.8980, grad_fn=<AddBackward0>) ic_loss tensor(737137.8125, grad_fn=<MeanBackward0>) data_loss tensor(502049.0463, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(85.2416, grad_fn=<MeanBackward0>) bc_loss tensor(75.7621, grad_fn=<AddBackward0>) ic_loss tensor(736824.6250, grad_fn=<MeanBackward0>) data_loss tensor(501298.3041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(88.0557, grad_fn=<MeanBackward0>) bc_loss tensor(72.9709, grad_fn=<AddBackward0>) ic_loss tensor(736506., grad_fn=<MeanBackward0>) data_loss tensor(500535.5505, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(90.9637, grad_fn=<MeanBackward0>) bc_loss tensor(70.8679, grad_fn=<AddBackward0>) ic_loss tensor(736181.6875, grad_fn=<MeanBackward0>) data_loss tensor(499760.1189, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(93.9699, grad_fn=<MeanBackward0>) bc_loss tensor(69.4362, grad_fn=<AddBackward0>) ic_loss tensor(735851.3125, grad_fn=<MeanBackward0>) data_loss tensor(498971.5236, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 190, Loss: 1234986.2111314046\n",
      "pde_loss tensor(97.0774, grad_fn=<MeanBackward0>) bc_loss tensor(69.6856, grad_fn=<AddBackward0>) ic_loss tensor(735515., grad_fn=<MeanBackward0>) data_loss tensor(498169.6548, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(100.2880, grad_fn=<MeanBackward0>) bc_loss tensor(72.2885, grad_fn=<AddBackward0>) ic_loss tensor(735173.2500, grad_fn=<MeanBackward0>) data_loss tensor(497354.9957, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(103.6041, grad_fn=<MeanBackward0>) bc_loss tensor(71.6995, grad_fn=<AddBackward0>) ic_loss tensor(734826.7500, grad_fn=<MeanBackward0>) data_loss tensor(496528.2381, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(107.0298, grad_fn=<MeanBackward0>) bc_loss tensor(71.1189, grad_fn=<AddBackward0>) ic_loss tensor(734475.2500, grad_fn=<MeanBackward0>) data_loss tensor(495689.0041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(110.5696, grad_fn=<MeanBackward0>) bc_loss tensor(69.2181, grad_fn=<AddBackward0>) ic_loss tensor(734118.9375, grad_fn=<MeanBackward0>) data_loss tensor(494836.9621, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 195, Loss: 1229135.7120935114\n",
      "pde_loss tensor(114.2272, grad_fn=<MeanBackward0>) bc_loss tensor(68.1273, grad_fn=<AddBackward0>) ic_loss tensor(733757.0625, grad_fn=<MeanBackward0>) data_loss tensor(493971.5811, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(118.0061, grad_fn=<MeanBackward0>) bc_loss tensor(68.2701, grad_fn=<AddBackward0>) ic_loss tensor(733389.1875, grad_fn=<MeanBackward0>) data_loss tensor(493092.4124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(121.9093, grad_fn=<MeanBackward0>) bc_loss tensor(67.9341, grad_fn=<AddBackward0>) ic_loss tensor(733015.1875, grad_fn=<MeanBackward0>) data_loss tensor(492199.3197, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(125.9399, grad_fn=<MeanBackward0>) bc_loss tensor(68.4897, grad_fn=<AddBackward0>) ic_loss tensor(732635.3750, grad_fn=<MeanBackward0>) data_loss tensor(491292.6271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(130.1017, grad_fn=<MeanBackward0>) bc_loss tensor(79.1935, grad_fn=<AddBackward0>) ic_loss tensor(732250.3750, grad_fn=<MeanBackward0>) data_loss tensor(490372.6998, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 200, Loss: 1222832.3872742988\n",
      "pde_loss tensor(134.3967, grad_fn=<MeanBackward0>) bc_loss tensor(80.4123, grad_fn=<AddBackward0>) ic_loss tensor(731861.3750, grad_fn=<MeanBackward0>) data_loss tensor(489440.5429, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(138.8303, grad_fn=<MeanBackward0>) bc_loss tensor(73.0407, grad_fn=<AddBackward0>) ic_loss tensor(731467.3750, grad_fn=<MeanBackward0>) data_loss tensor(488495.1850, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(143.4124, grad_fn=<MeanBackward0>) bc_loss tensor(67.5244, grad_fn=<AddBackward0>) ic_loss tensor(731067.1875, grad_fn=<MeanBackward0>) data_loss tensor(487534.7539, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(148.1499, grad_fn=<MeanBackward0>) bc_loss tensor(67.4459, grad_fn=<AddBackward0>) ic_loss tensor(730659.9375, grad_fn=<MeanBackward0>) data_loss tensor(486558.2428, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(153.0477, grad_fn=<MeanBackward0>) bc_loss tensor(68.3808, grad_fn=<AddBackward0>) ic_loss tensor(730245.7500, grad_fn=<MeanBackward0>) data_loss tensor(485565.7203, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 205, Loss: 1216032.9078014414\n",
      "pde_loss tensor(158.1102, grad_fn=<MeanBackward0>) bc_loss tensor(70.7454, grad_fn=<AddBackward0>) ic_loss tensor(729825.2500, grad_fn=<MeanBackward0>) data_loss tensor(484557.7340, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(163.3413, grad_fn=<MeanBackward0>) bc_loss tensor(72.7050, grad_fn=<AddBackward0>) ic_loss tensor(729398.5000, grad_fn=<MeanBackward0>) data_loss tensor(483534.6942, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(168.7459, grad_fn=<MeanBackward0>) bc_loss tensor(72.0662, grad_fn=<AddBackward0>) ic_loss tensor(728965.1875, grad_fn=<MeanBackward0>) data_loss tensor(482496.4552, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(174.3327, grad_fn=<MeanBackward0>) bc_loss tensor(71.2663, grad_fn=<AddBackward0>) ic_loss tensor(728525.2500, grad_fn=<MeanBackward0>) data_loss tensor(481442.1902, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(180.1109, grad_fn=<MeanBackward0>) bc_loss tensor(71.1363, grad_fn=<AddBackward0>) ic_loss tensor(728077.8750, grad_fn=<MeanBackward0>) data_loss tensor(480371.0787, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 210, Loss: 1208700.2037053977\n",
      "pde_loss tensor(186.0880, grad_fn=<MeanBackward0>) bc_loss tensor(70.6564, grad_fn=<AddBackward0>) ic_loss tensor(727622.8125, grad_fn=<MeanBackward0>) data_loss tensor(479282.6357, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(192.2705, grad_fn=<MeanBackward0>) bc_loss tensor(71.2883, grad_fn=<AddBackward0>) ic_loss tensor(727159.7500, grad_fn=<MeanBackward0>) data_loss tensor(478176.5953, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(198.6609, grad_fn=<MeanBackward0>) bc_loss tensor(73.0801, grad_fn=<AddBackward0>) ic_loss tensor(726689.1250, grad_fn=<MeanBackward0>) data_loss tensor(477053.4880, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(205.2621, grad_fn=<MeanBackward0>) bc_loss tensor(73.5967, grad_fn=<AddBackward0>) ic_loss tensor(726211.1250, grad_fn=<MeanBackward0>) data_loss tensor(475913.8596, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(212.0813, grad_fn=<MeanBackward0>) bc_loss tensor(71.7259, grad_fn=<AddBackward0>) ic_loss tensor(725726.1250, grad_fn=<MeanBackward0>) data_loss tensor(474757.8406, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 215, Loss: 1200767.7781205417\n",
      "pde_loss tensor(219.1291, grad_fn=<MeanBackward0>) bc_loss tensor(70.3684, grad_fn=<AddBackward0>) ic_loss tensor(725234., grad_fn=<MeanBackward0>) data_loss tensor(473584.9107, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(226.4171, grad_fn=<MeanBackward0>) bc_loss tensor(69.2965, grad_fn=<AddBackward0>) ic_loss tensor(724734.6875, grad_fn=<MeanBackward0>) data_loss tensor(472394.5251, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(233.9543, grad_fn=<MeanBackward0>) bc_loss tensor(69.7891, grad_fn=<AddBackward0>) ic_loss tensor(724227.9375, grad_fn=<MeanBackward0>) data_loss tensor(471186.1898, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(241.7447, grad_fn=<MeanBackward0>) bc_loss tensor(68.6764, grad_fn=<AddBackward0>) ic_loss tensor(723713.5000, grad_fn=<MeanBackward0>) data_loss tensor(469960.6571, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(249.7977, grad_fn=<MeanBackward0>) bc_loss tensor(66.3702, grad_fn=<AddBackward0>) ic_loss tensor(723191.2500, grad_fn=<MeanBackward0>) data_loss tensor(468717.4245, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 220, Loss: 1192224.8619692235\n",
      "pde_loss tensor(258.1246, grad_fn=<MeanBackward0>) bc_loss tensor(66.4202, grad_fn=<AddBackward0>) ic_loss tensor(722660.5000, grad_fn=<MeanBackward0>) data_loss tensor(467455.5367, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(266.7326, grad_fn=<MeanBackward0>) bc_loss tensor(68.9820, grad_fn=<AddBackward0>) ic_loss tensor(722121., grad_fn=<MeanBackward0>) data_loss tensor(466174.7696, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(275.6296, grad_fn=<MeanBackward0>) bc_loss tensor(72.1176, grad_fn=<AddBackward0>) ic_loss tensor(721573., grad_fn=<MeanBackward0>) data_loss tensor(464875.2124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(284.8166, grad_fn=<MeanBackward0>) bc_loss tensor(73.5013, grad_fn=<AddBackward0>) ic_loss tensor(721016.8750, grad_fn=<MeanBackward0>) data_loss tensor(463557.0443, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(294.3005, grad_fn=<MeanBackward0>) bc_loss tensor(72.9728, grad_fn=<AddBackward0>) ic_loss tensor(720452.3750, grad_fn=<MeanBackward0>) data_loss tensor(462219.8336, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 225, Loss: 1183039.4586188786\n",
      "pde_loss tensor(304.0909, grad_fn=<MeanBackward0>) bc_loss tensor(71.5785, grad_fn=<AddBackward0>) ic_loss tensor(719878.8750, grad_fn=<MeanBackward0>) data_loss tensor(460862.9949, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(314.2539, grad_fn=<MeanBackward0>) bc_loss tensor(71.1077, grad_fn=<AddBackward0>) ic_loss tensor(719296.3125, grad_fn=<MeanBackward0>) data_loss tensor(459486.2025, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(324.8735, grad_fn=<MeanBackward0>) bc_loss tensor(76.1487, grad_fn=<AddBackward0>) ic_loss tensor(718704.5000, grad_fn=<MeanBackward0>) data_loss tensor(458090.1714, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(335.8382, grad_fn=<MeanBackward0>) bc_loss tensor(81.9195, grad_fn=<AddBackward0>) ic_loss tensor(718104.3750, grad_fn=<MeanBackward0>) data_loss tensor(456675.5605, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(347.1553, grad_fn=<MeanBackward0>) bc_loss tensor(86.5164, grad_fn=<AddBackward0>) ic_loss tensor(717495.6250, grad_fn=<MeanBackward0>) data_loss tensor(455242.6178, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 230, Loss: 1173171.9302769622\n",
      "pde_loss tensor(358.8356, grad_fn=<MeanBackward0>) bc_loss tensor(93.0111, grad_fn=<AddBackward0>) ic_loss tensor(716878.3750, grad_fn=<MeanBackward0>) data_loss tensor(453791.0949, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(370.8901, grad_fn=<MeanBackward0>) bc_loss tensor(100.2623, grad_fn=<AddBackward0>) ic_loss tensor(716251.8125, grad_fn=<MeanBackward0>) data_loss tensor(452320.6336, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(383.3300, grad_fn=<MeanBackward0>) bc_loss tensor(107.6717, grad_fn=<AddBackward0>) ic_loss tensor(715615.6875, grad_fn=<MeanBackward0>) data_loss tensor(450830.8757, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(396.1654, grad_fn=<MeanBackward0>) bc_loss tensor(113.2365, grad_fn=<AddBackward0>) ic_loss tensor(714970.5000, grad_fn=<MeanBackward0>) data_loss tensor(449322.1615, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(409.4035, grad_fn=<MeanBackward0>) bc_loss tensor(116.3448, grad_fn=<AddBackward0>) ic_loss tensor(714316.4375, grad_fn=<MeanBackward0>) data_loss tensor(447795.1370, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 235, Loss: 1162637.3244838938\n",
      "pde_loss tensor(423.0514, grad_fn=<MeanBackward0>) bc_loss tensor(116.6525, grad_fn=<AddBackward0>) ic_loss tensor(713653.0625, grad_fn=<MeanBackward0>) data_loss tensor(446249.9177, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(437.1194, grad_fn=<MeanBackward0>) bc_loss tensor(115.8567, grad_fn=<AddBackward0>) ic_loss tensor(712980.1250, grad_fn=<MeanBackward0>) data_loss tensor(444686.3205, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(451.6149, grad_fn=<MeanBackward0>) bc_loss tensor(114.0760, grad_fn=<AddBackward0>) ic_loss tensor(712297.9375, grad_fn=<MeanBackward0>) data_loss tensor(443104.6351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(466.5507, grad_fn=<MeanBackward0>) bc_loss tensor(108.6735, grad_fn=<AddBackward0>) ic_loss tensor(711606.5000, grad_fn=<MeanBackward0>) data_loss tensor(441504.8841, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(481.9441, grad_fn=<MeanBackward0>) bc_loss tensor(100.1710, grad_fn=<AddBackward0>) ic_loss tensor(710905.6250, grad_fn=<MeanBackward0>) data_loss tensor(439886.3318, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 240, Loss: 1151374.0818258058\n",
      "pde_loss tensor(497.8125, grad_fn=<MeanBackward0>) bc_loss tensor(90.0451, grad_fn=<AddBackward0>) ic_loss tensor(710195.2500, grad_fn=<MeanBackward0>) data_loss tensor(438248.5173, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(514.1767, grad_fn=<MeanBackward0>) bc_loss tensor(80.2793, grad_fn=<AddBackward0>) ic_loss tensor(709475.0625, grad_fn=<MeanBackward0>) data_loss tensor(436590.8215, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(531.0547, grad_fn=<MeanBackward0>) bc_loss tensor(72.2741, grad_fn=<AddBackward0>) ic_loss tensor(708744.5625, grad_fn=<MeanBackward0>) data_loss tensor(434912.5944, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(548.4675, grad_fn=<MeanBackward0>) bc_loss tensor(66.2869, grad_fn=<AddBackward0>) ic_loss tensor(708003.5000, grad_fn=<MeanBackward0>) data_loss tensor(433213.3539, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(566.4313, grad_fn=<MeanBackward0>) bc_loss tensor(62.8378, grad_fn=<AddBackward0>) ic_loss tensor(707251.7500, grad_fn=<MeanBackward0>) data_loss tensor(431492.7858, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 245, Loss: 1139373.7858048722\n",
      "pde_loss tensor(584.9586, grad_fn=<MeanBackward0>) bc_loss tensor(62.3621, grad_fn=<AddBackward0>) ic_loss tensor(706489.1875, grad_fn=<MeanBackward0>) data_loss tensor(429751.1459, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(604.0598, grad_fn=<MeanBackward0>) bc_loss tensor(62.0474, grad_fn=<AddBackward0>) ic_loss tensor(705716.1875, grad_fn=<MeanBackward0>) data_loss tensor(427989.2536, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(623.7492, grad_fn=<MeanBackward0>) bc_loss tensor(60.9260, grad_fn=<AddBackward0>) ic_loss tensor(704932.9375, grad_fn=<MeanBackward0>) data_loss tensor(426207.1340, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(644.0427, grad_fn=<MeanBackward0>) bc_loss tensor(60.4132, grad_fn=<AddBackward0>) ic_loss tensor(704139.0625, grad_fn=<MeanBackward0>) data_loss tensor(424404.7123, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(664.9540, grad_fn=<MeanBackward0>) bc_loss tensor(63.4577, grad_fn=<AddBackward0>) ic_loss tensor(703334.0625, grad_fn=<MeanBackward0>) data_loss tensor(422581.8602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 250, Loss: 1126644.360186988\n",
      "pde_loss tensor(686.4950, grad_fn=<MeanBackward0>) bc_loss tensor(86.6135, grad_fn=<AddBackward0>) ic_loss tensor(702519.4375, grad_fn=<MeanBackward0>) data_loss tensor(420739.3362, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(708.6555, grad_fn=<MeanBackward0>) bc_loss tensor(86.5575, grad_fn=<AddBackward0>) ic_loss tensor(701698.9375, grad_fn=<MeanBackward0>) data_loss tensor(418881.9218, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(731.4515, grad_fn=<MeanBackward0>) bc_loss tensor(74.1272, grad_fn=<AddBackward0>) ic_loss tensor(700869.1250, grad_fn=<MeanBackward0>) data_loss tensor(417006.8099, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(754.9156, grad_fn=<MeanBackward0>) bc_loss tensor(62.7662, grad_fn=<AddBackward0>) ic_loss tensor(700029., grad_fn=<MeanBackward0>) data_loss tensor(415112.6677, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(779.0833, grad_fn=<MeanBackward0>) bc_loss tensor(63.4205, grad_fn=<AddBackward0>) ic_loss tensor(699177.1875, grad_fn=<MeanBackward0>) data_loss tensor(413197.7565, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 255, Loss: 1113217.444001461\n",
      "pde_loss tensor(803.9697, grad_fn=<MeanBackward0>) bc_loss tensor(64.5295, grad_fn=<AddBackward0>) ic_loss tensor(698314.6250, grad_fn=<MeanBackward0>) data_loss tensor(411263.0994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(829.5856, grad_fn=<MeanBackward0>) bc_loss tensor(63.2208, grad_fn=<AddBackward0>) ic_loss tensor(697442.0625, grad_fn=<MeanBackward0>) data_loss tensor(409309.4209, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(855.9446, grad_fn=<MeanBackward0>) bc_loss tensor(66.1519, grad_fn=<AddBackward0>) ic_loss tensor(696558.4375, grad_fn=<MeanBackward0>) data_loss tensor(407336.4567, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(883.0574, grad_fn=<MeanBackward0>) bc_loss tensor(82.2320, grad_fn=<AddBackward0>) ic_loss tensor(695664.4375, grad_fn=<MeanBackward0>) data_loss tensor(405345.0150, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(910.9193, grad_fn=<MeanBackward0>) bc_loss tensor(73.3865, grad_fn=<AddBackward0>) ic_loss tensor(694765.5625, grad_fn=<MeanBackward0>) data_loss tensor(403340.1228, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 260, Loss: 1099089.9978265578\n",
      "pde_loss tensor(939.5605, grad_fn=<MeanBackward0>) bc_loss tensor(59.5158, grad_fn=<AddBackward0>) ic_loss tensor(693855.5000, grad_fn=<MeanBackward0>) data_loss tensor(401316.8341, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(969.0324, grad_fn=<MeanBackward0>) bc_loss tensor(52.1835, grad_fn=<AddBackward0>) ic_loss tensor(692932.6875, grad_fn=<MeanBackward0>) data_loss tensor(399272.6806, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(999.3812, grad_fn=<MeanBackward0>) bc_loss tensor(53.6959, grad_fn=<AddBackward0>) ic_loss tensor(691997., grad_fn=<MeanBackward0>) data_loss tensor(397206.8701, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1030.6250, grad_fn=<MeanBackward0>) bc_loss tensor(60.3076, grad_fn=<AddBackward0>) ic_loss tensor(691048.9375, grad_fn=<MeanBackward0>) data_loss tensor(395120.0060, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1062.7733, grad_fn=<MeanBackward0>) bc_loss tensor(69.6889, grad_fn=<AddBackward0>) ic_loss tensor(690089., grad_fn=<MeanBackward0>) data_loss tensor(393013.2008, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 265, Loss: 1084234.638337676\n",
      "pde_loss tensor(1095.8224, grad_fn=<MeanBackward0>) bc_loss tensor(78.2480, grad_fn=<AddBackward0>) ic_loss tensor(689118., grad_fn=<MeanBackward0>) data_loss tensor(390887.9852, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1129.7703, grad_fn=<MeanBackward0>) bc_loss tensor(85.5677, grad_fn=<AddBackward0>) ic_loss tensor(688135.2500, grad_fn=<MeanBackward0>) data_loss tensor(388745.1321, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1164.6219, grad_fn=<MeanBackward0>) bc_loss tensor(96.4209, grad_fn=<AddBackward0>) ic_loss tensor(687141.9375, grad_fn=<MeanBackward0>) data_loss tensor(386585.9631, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1200.3844, grad_fn=<MeanBackward0>) bc_loss tensor(91.7216, grad_fn=<AddBackward0>) ic_loss tensor(686142.0625, grad_fn=<MeanBackward0>) data_loss tensor(384414.2179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1237.0831, grad_fn=<MeanBackward0>) bc_loss tensor(87.0293, grad_fn=<AddBackward0>) ic_loss tensor(685131.3125, grad_fn=<MeanBackward0>) data_loss tensor(382226.8773, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 270, Loss: 1068682.3148055524\n",
      "pde_loss tensor(1274.7529, grad_fn=<MeanBackward0>) bc_loss tensor(79.6439, grad_fn=<AddBackward0>) ic_loss tensor(684109.4375, grad_fn=<MeanBackward0>) data_loss tensor(380023.2224, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1313.4453, grad_fn=<MeanBackward0>) bc_loss tensor(73.0696, grad_fn=<AddBackward0>) ic_loss tensor(683075.9375, grad_fn=<MeanBackward0>) data_loss tensor(377801.9940, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1353.1938, grad_fn=<MeanBackward0>) bc_loss tensor(68.4538, grad_fn=<AddBackward0>) ic_loss tensor(682029.9375, grad_fn=<MeanBackward0>) data_loss tensor(375562.6710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1394.0383, grad_fn=<MeanBackward0>) bc_loss tensor(77.2981, grad_fn=<AddBackward0>) ic_loss tensor(680971.8125, grad_fn=<MeanBackward0>) data_loss tensor(373304.8336, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1435.9640, grad_fn=<MeanBackward0>) bc_loss tensor(73.8092, grad_fn=<AddBackward0>) ic_loss tensor(679905.5625, grad_fn=<MeanBackward0>) data_loss tensor(371033.1245, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 275, Loss: 1052448.4369711399\n",
      "pde_loss tensor(1479.0078, grad_fn=<MeanBackward0>) bc_loss tensor(77.7338, grad_fn=<AddBackward0>) ic_loss tensor(678827.8750, grad_fn=<MeanBackward0>) data_loss tensor(368744.6936, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1523.1631, grad_fn=<MeanBackward0>) bc_loss tensor(79.0976, grad_fn=<AddBackward0>) ic_loss tensor(677738.8750, grad_fn=<MeanBackward0>) data_loss tensor(366441.6289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1568.4717, grad_fn=<MeanBackward0>) bc_loss tensor(83.7943, grad_fn=<AddBackward0>) ic_loss tensor(676638.9375, grad_fn=<MeanBackward0>) data_loss tensor(364122.8016, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1614.9316, grad_fn=<MeanBackward0>) bc_loss tensor(91.2354, grad_fn=<AddBackward0>) ic_loss tensor(675528.3125, grad_fn=<MeanBackward0>) data_loss tensor(361790.0997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1662.5555, grad_fn=<MeanBackward0>) bc_loss tensor(98.4372, grad_fn=<AddBackward0>) ic_loss tensor(674406.6250, grad_fn=<MeanBackward0>) data_loss tensor(359443.8842, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 280, Loss: 1035611.5091681313\n",
      "pde_loss tensor(1711.3756, grad_fn=<MeanBackward0>) bc_loss tensor(100.6078, grad_fn=<AddBackward0>) ic_loss tensor(673273.9375, grad_fn=<MeanBackward0>) data_loss tensor(357083.6592, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1761.4146, grad_fn=<MeanBackward0>) bc_loss tensor(99.2576, grad_fn=<AddBackward0>) ic_loss tensor(672130.1875, grad_fn=<MeanBackward0>) data_loss tensor(354710.0898, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1812.7196, grad_fn=<MeanBackward0>) bc_loss tensor(97.0595, grad_fn=<AddBackward0>) ic_loss tensor(670974.1250, grad_fn=<MeanBackward0>) data_loss tensor(352321.7828, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1865.3234, grad_fn=<MeanBackward0>) bc_loss tensor(94.2612, grad_fn=<AddBackward0>) ic_loss tensor(669806.1875, grad_fn=<MeanBackward0>) data_loss tensor(349918.9801, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1919.2208, grad_fn=<MeanBackward0>) bc_loss tensor(92.4626, grad_fn=<AddBackward0>) ic_loss tensor(668629.0625, grad_fn=<MeanBackward0>) data_loss tensor(347504.8721, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 285, Loss: 1018145.6220541336\n",
      "pde_loss tensor(1974.4385, grad_fn=<MeanBackward0>) bc_loss tensor(83.9619, grad_fn=<AddBackward0>) ic_loss tensor(667442.0625, grad_fn=<MeanBackward0>) data_loss tensor(345078.9327, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2031.0170, grad_fn=<MeanBackward0>) bc_loss tensor(81.1996, grad_fn=<AddBackward0>) ic_loss tensor(666242., grad_fn=<MeanBackward0>) data_loss tensor(342638.8548, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2088.9592, grad_fn=<MeanBackward0>) bc_loss tensor(73.7777, grad_fn=<AddBackward0>) ic_loss tensor(665031.8125, grad_fn=<MeanBackward0>) data_loss tensor(340188.1260, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2148.3179, grad_fn=<MeanBackward0>) bc_loss tensor(66.9515, grad_fn=<AddBackward0>) ic_loss tensor(663810.7500, grad_fn=<MeanBackward0>) data_loss tensor(337725.3125, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2209.1567, grad_fn=<MeanBackward0>) bc_loss tensor(73.6774, grad_fn=<AddBackward0>) ic_loss tensor(662576.2500, grad_fn=<MeanBackward0>) data_loss tensor(335247.8275, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 290, Loss: 1000106.8899690732\n",
      "pde_loss tensor(2271.4702, grad_fn=<MeanBackward0>) bc_loss tensor(97.4643, grad_fn=<AddBackward0>) ic_loss tensor(661332.2500, grad_fn=<MeanBackward0>) data_loss tensor(332760.1794, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2335.2158, grad_fn=<MeanBackward0>) bc_loss tensor(98.3174, grad_fn=<AddBackward0>) ic_loss tensor(660084.4375, grad_fn=<MeanBackward0>) data_loss tensor(330267.7584, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2400.4382, grad_fn=<MeanBackward0>) bc_loss tensor(93.4182, grad_fn=<AddBackward0>) ic_loss tensor(658823.8750, grad_fn=<MeanBackward0>) data_loss tensor(327764.3049, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2467.1758, grad_fn=<MeanBackward0>) bc_loss tensor(84.2075, grad_fn=<AddBackward0>) ic_loss tensor(657552.4375, grad_fn=<MeanBackward0>) data_loss tensor(325251.1764, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2535.4717, grad_fn=<MeanBackward0>) bc_loss tensor(73.4748, grad_fn=<AddBackward0>) ic_loss tensor(656271.7500, grad_fn=<MeanBackward0>) data_loss tensor(322730.1363, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 295, Loss: 981610.823778882\n",
      "pde_loss tensor(2605.4082, grad_fn=<MeanBackward0>) bc_loss tensor(72.5834, grad_fn=<AddBackward0>) ic_loss tensor(654977.1875, grad_fn=<MeanBackward0>) data_loss tensor(320196.5591, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2677.0042, grad_fn=<MeanBackward0>) bc_loss tensor(79.8215, grad_fn=<AddBackward0>) ic_loss tensor(653669.8750, grad_fn=<MeanBackward0>) data_loss tensor(317651.6842, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2750.2104, grad_fn=<MeanBackward0>) bc_loss tensor(78.9363, grad_fn=<AddBackward0>) ic_loss tensor(652354.1250, grad_fn=<MeanBackward0>) data_loss tensor(315101.3740, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2825.0142, grad_fn=<MeanBackward0>) bc_loss tensor(76.6212, grad_fn=<AddBackward0>) ic_loss tensor(651026.0625, grad_fn=<MeanBackward0>) data_loss tensor(312544.3050, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2901.4492, grad_fn=<MeanBackward0>) bc_loss tensor(76.3060, grad_fn=<AddBackward0>) ic_loss tensor(649684.0625, grad_fn=<MeanBackward0>) data_loss tensor(309978.9359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 300, Loss: 962640.7484472839\n",
      "pde_loss tensor(2979.5256, grad_fn=<MeanBackward0>) bc_loss tensor(74.3836, grad_fn=<AddBackward0>) ic_loss tensor(648333.2500, grad_fn=<MeanBackward0>) data_loss tensor(307409.2418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3059.2871, grad_fn=<MeanBackward0>) bc_loss tensor(69.8997, grad_fn=<AddBackward0>) ic_loss tensor(646971.3125, grad_fn=<MeanBackward0>) data_loss tensor(304834.0758, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3140.7917, grad_fn=<MeanBackward0>) bc_loss tensor(70.1081, grad_fn=<AddBackward0>) ic_loss tensor(645594.2500, grad_fn=<MeanBackward0>) data_loss tensor(302250.2083, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3224.0659, grad_fn=<MeanBackward0>) bc_loss tensor(67.9625, grad_fn=<AddBackward0>) ic_loss tensor(644207.6875, grad_fn=<MeanBackward0>) data_loss tensor(299661.7700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3309.1372, grad_fn=<MeanBackward0>) bc_loss tensor(72.6819, grad_fn=<AddBackward0>) ic_loss tensor(642813.7500, grad_fn=<MeanBackward0>) data_loss tensor(297070.5201, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 305, Loss: 943266.0826019009\n",
      "pde_loss tensor(3396.0286, grad_fn=<MeanBackward0>) bc_loss tensor(91.2494, grad_fn=<AddBackward0>) ic_loss tensor(641411.1250, grad_fn=<MeanBackward0>) data_loss tensor(294476.1650, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3484.6838, grad_fn=<MeanBackward0>) bc_loss tensor(95.3963, grad_fn=<AddBackward0>) ic_loss tensor(639999.7500, grad_fn=<MeanBackward0>) data_loss tensor(291880.2369, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3575.1267, grad_fn=<MeanBackward0>) bc_loss tensor(95.3982, grad_fn=<AddBackward0>) ic_loss tensor(638583.1875, grad_fn=<MeanBackward0>) data_loss tensor(289285.7342, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3667.4238, grad_fn=<MeanBackward0>) bc_loss tensor(92.8299, grad_fn=<AddBackward0>) ic_loss tensor(637159.6875, grad_fn=<MeanBackward0>) data_loss tensor(286690.9200, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3761.6265, grad_fn=<MeanBackward0>) bc_loss tensor(82.8005, grad_fn=<AddBackward0>) ic_loss tensor(635727.5625, grad_fn=<MeanBackward0>) data_loss tensor(284094.6793, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 310, Loss: 923666.6792557503\n",
      "pde_loss tensor(3857.8032, grad_fn=<MeanBackward0>) bc_loss tensor(72.1780, grad_fn=<AddBackward0>) ic_loss tensor(634288.1875, grad_fn=<MeanBackward0>) data_loss tensor(281497.7977, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3956.0266, grad_fn=<MeanBackward0>) bc_loss tensor(62.7710, grad_fn=<AddBackward0>) ic_loss tensor(632838.2500, grad_fn=<MeanBackward0>) data_loss tensor(278897.8049, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4056.3245, grad_fn=<MeanBackward0>) bc_loss tensor(62.6100, grad_fn=<AddBackward0>) ic_loss tensor(631374.0625, grad_fn=<MeanBackward0>) data_loss tensor(276293.0757, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4158.6636, grad_fn=<MeanBackward0>) bc_loss tensor(69.2383, grad_fn=<AddBackward0>) ic_loss tensor(629899.3125, grad_fn=<MeanBackward0>) data_loss tensor(273687.5290, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4262.9966, grad_fn=<MeanBackward0>) bc_loss tensor(71.6049, grad_fn=<AddBackward0>) ic_loss tensor(628418., grad_fn=<MeanBackward0>) data_loss tensor(271084.7508, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 315, Loss: 903837.3758161259\n",
      "pde_loss tensor(4369.2842, grad_fn=<MeanBackward0>) bc_loss tensor(75.0833, grad_fn=<AddBackward0>) ic_loss tensor(626923.9375, grad_fn=<MeanBackward0>) data_loss tensor(268482.6311, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4477.4912, grad_fn=<MeanBackward0>) bc_loss tensor(81.5407, grad_fn=<AddBackward0>) ic_loss tensor(625417.2500, grad_fn=<MeanBackward0>) data_loss tensor(265882.9406, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4587.6348, grad_fn=<MeanBackward0>) bc_loss tensor(75.5051, grad_fn=<AddBackward0>) ic_loss tensor(623906.4375, grad_fn=<MeanBackward0>) data_loss tensor(263290.7877, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4699.8013, grad_fn=<MeanBackward0>) bc_loss tensor(71.2562, grad_fn=<AddBackward0>) ic_loss tensor(622383., grad_fn=<MeanBackward0>) data_loss tensor(260700.4468, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4814.0278, grad_fn=<MeanBackward0>) bc_loss tensor(63.7867, grad_fn=<AddBackward0>) ic_loss tensor(620850.0625, grad_fn=<MeanBackward0>) data_loss tensor(258114.4050, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 320, Loss: 883842.2800023066\n",
      "pde_loss tensor(4930.4004, grad_fn=<MeanBackward0>) bc_loss tensor(54.7761, grad_fn=<AddBackward0>) ic_loss tensor(619309.5625, grad_fn=<MeanBackward0>) data_loss tensor(255532.9811, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5048.9849, grad_fn=<MeanBackward0>) bc_loss tensor(52.9901, grad_fn=<AddBackward0>) ic_loss tensor(617755.8750, grad_fn=<MeanBackward0>) data_loss tensor(252952.7891, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5169.7563, grad_fn=<MeanBackward0>) bc_loss tensor(62.7145, grad_fn=<AddBackward0>) ic_loss tensor(616190.8125, grad_fn=<MeanBackward0>) data_loss tensor(250376.5916, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5292.6655, grad_fn=<MeanBackward0>) bc_loss tensor(65.3317, grad_fn=<AddBackward0>) ic_loss tensor(614627., grad_fn=<MeanBackward0>) data_loss tensor(247812.3079, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5417.6958, grad_fn=<MeanBackward0>) bc_loss tensor(78.3173, grad_fn=<AddBackward0>) ic_loss tensor(613049.0625, grad_fn=<MeanBackward0>) data_loss tensor(245252.2889, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 325, Loss: 863797.351432854\n",
      "pde_loss tensor(5544.7544, grad_fn=<MeanBackward0>) bc_loss tensor(82.0631, grad_fn=<AddBackward0>) ic_loss tensor(611476.3750, grad_fn=<MeanBackward0>) data_loss tensor(242709.2781, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5673.8848, grad_fn=<MeanBackward0>) bc_loss tensor(76.7112, grad_fn=<AddBackward0>) ic_loss tensor(609892.6875, grad_fn=<MeanBackward0>) data_loss tensor(240174.1854, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5805.1616, grad_fn=<MeanBackward0>) bc_loss tensor(58.8340, grad_fn=<AddBackward0>) ic_loss tensor(608307.8125, grad_fn=<MeanBackward0>) data_loss tensor(237652.1660, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5938.7290, grad_fn=<MeanBackward0>) bc_loss tensor(43.8344, grad_fn=<AddBackward0>) ic_loss tensor(606712.8125, grad_fn=<MeanBackward0>) data_loss tensor(235137.1945, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6074.7129, grad_fn=<MeanBackward0>) bc_loss tensor(43.2458, grad_fn=<AddBackward0>) ic_loss tensor(605101.6875, grad_fn=<MeanBackward0>) data_loss tensor(232625.5141, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 330, Loss: 843845.1390568102\n",
      "pde_loss tensor(6213.0908, grad_fn=<MeanBackward0>) bc_loss tensor(44.7795, grad_fn=<AddBackward0>) ic_loss tensor(603484.9375, grad_fn=<MeanBackward0>) data_loss tensor(230123.8093, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6353.8154, grad_fn=<MeanBackward0>) bc_loss tensor(40.9577, grad_fn=<AddBackward0>) ic_loss tensor(601863.1875, grad_fn=<MeanBackward0>) data_loss tensor(227633.3154, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6496.8760, grad_fn=<MeanBackward0>) bc_loss tensor(50.6603, grad_fn=<AddBackward0>) ic_loss tensor(600224.8750, grad_fn=<MeanBackward0>) data_loss tensor(225149.2323, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6642.1104, grad_fn=<MeanBackward0>) bc_loss tensor(65.7195, grad_fn=<AddBackward0>) ic_loss tensor(598577., grad_fn=<MeanBackward0>) data_loss tensor(222677.9843, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6789.4185, grad_fn=<MeanBackward0>) bc_loss tensor(60.3402, grad_fn=<AddBackward0>) ic_loss tensor(596934.8750, grad_fn=<MeanBackward0>) data_loss tensor(220228.4121, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 335, Loss: 824013.0371219816\n",
      "pde_loss tensor(6938.8154, grad_fn=<MeanBackward0>) bc_loss tensor(65.2984, grad_fn=<AddBackward0>) ic_loss tensor(595274.5625, grad_fn=<MeanBackward0>) data_loss tensor(217788.7052, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7090.2847, grad_fn=<MeanBackward0>) bc_loss tensor(62.0464, grad_fn=<AddBackward0>) ic_loss tensor(593613.2500, grad_fn=<MeanBackward0>) data_loss tensor(215368.6228, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7243.9526, grad_fn=<MeanBackward0>) bc_loss tensor(57.8168, grad_fn=<AddBackward0>) ic_loss tensor(591950., grad_fn=<MeanBackward0>) data_loss tensor(212966.2897, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7399.8916, grad_fn=<MeanBackward0>) bc_loss tensor(61.4429, grad_fn=<AddBackward0>) ic_loss tensor(590273.3125, grad_fn=<MeanBackward0>) data_loss tensor(210576.0368, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7558.0767, grad_fn=<MeanBackward0>) bc_loss tensor(64.0852, grad_fn=<AddBackward0>) ic_loss tensor(588594.8750, grad_fn=<MeanBackward0>) data_loss tensor(208204.6299, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 340, Loss: 804421.6924320797\n",
      "pde_loss tensor(7718.5615, grad_fn=<MeanBackward0>) bc_loss tensor(62.3666, grad_fn=<AddBackward0>) ic_loss tensor(586913.3125, grad_fn=<MeanBackward0>) data_loss tensor(205851.0365, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7881.3374, grad_fn=<MeanBackward0>) bc_loss tensor(72.9737, grad_fn=<AddBackward0>) ic_loss tensor(585218.3125, grad_fn=<MeanBackward0>) data_loss tensor(203510.5996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8046.1934, grad_fn=<MeanBackward0>) bc_loss tensor(67.1420, grad_fn=<AddBackward0>) ic_loss tensor(583524.3125, grad_fn=<MeanBackward0>) data_loss tensor(201193.4829, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8213.2383, grad_fn=<MeanBackward0>) bc_loss tensor(59.6398, grad_fn=<AddBackward0>) ic_loss tensor(581826., grad_fn=<MeanBackward0>) data_loss tensor(198895.8690, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8382.4785, grad_fn=<MeanBackward0>) bc_loss tensor(60.9364, grad_fn=<AddBackward0>) ic_loss tensor(580116.5000, grad_fn=<MeanBackward0>) data_loss tensor(196614.4825, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 345, Loss: 785174.4200195025\n",
      "pde_loss tensor(8553.8750, grad_fn=<MeanBackward0>) bc_loss tensor(58.0250, grad_fn=<AddBackward0>) ic_loss tensor(578405.6250, grad_fn=<MeanBackward0>) data_loss tensor(194355.4341, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8727.3750, grad_fn=<MeanBackward0>) bc_loss tensor(50.0305, grad_fn=<AddBackward0>) ic_loss tensor(576693.1875, grad_fn=<MeanBackward0>) data_loss tensor(192119.1921, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8903.0498, grad_fn=<MeanBackward0>) bc_loss tensor(53.8792, grad_fn=<AddBackward0>) ic_loss tensor(574967.8750, grad_fn=<MeanBackward0>) data_loss tensor(189899.7515, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9080.8008, grad_fn=<MeanBackward0>) bc_loss tensor(47.9298, grad_fn=<AddBackward0>) ic_loss tensor(573243., grad_fn=<MeanBackward0>) data_loss tensor(187704.2612, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9260.5625, grad_fn=<MeanBackward0>) bc_loss tensor(44.5355, grad_fn=<AddBackward0>) ic_loss tensor(571510.8750, grad_fn=<MeanBackward0>) data_loss tensor(185530.9063, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 350, Loss: 766346.9063087553\n",
      "pde_loss tensor(9442.3477, grad_fn=<MeanBackward0>) bc_loss tensor(53.0629, grad_fn=<AddBackward0>) ic_loss tensor(569766.6875, grad_fn=<MeanBackward0>) data_loss tensor(183377.4178, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9626.0693, grad_fn=<MeanBackward0>) bc_loss tensor(42.9017, grad_fn=<AddBackward0>) ic_loss tensor(568027.7500, grad_fn=<MeanBackward0>) data_loss tensor(181252.1466, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9811.7793, grad_fn=<MeanBackward0>) bc_loss tensor(40.8372, grad_fn=<AddBackward0>) ic_loss tensor(566275.2500, grad_fn=<MeanBackward0>) data_loss tensor(179146.5143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9999.4141, grad_fn=<MeanBackward0>) bc_loss tensor(57.6107, grad_fn=<AddBackward0>) ic_loss tensor(564512.3125, grad_fn=<MeanBackward0>) data_loss tensor(177063.4509, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10188.7803, grad_fn=<MeanBackward0>) bc_loss tensor(65.5906, grad_fn=<AddBackward0>) ic_loss tensor(562761.6875, grad_fn=<MeanBackward0>) data_loss tensor(175014.0907, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 355, Loss: 748030.1532074033\n",
      "pde_loss tensor(10379.8057, grad_fn=<MeanBackward0>) bc_loss tensor(62.9478, grad_fn=<AddBackward0>) ic_loss tensor(560998.8750, grad_fn=<MeanBackward0>) data_loss tensor(172987.2152, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10572.5771, grad_fn=<MeanBackward0>) bc_loss tensor(46.2864, grad_fn=<AddBackward0>) ic_loss tensor(559233.9375, grad_fn=<MeanBackward0>) data_loss tensor(170986.7636, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10767.3428, grad_fn=<MeanBackward0>) bc_loss tensor(29.1135, grad_fn=<AddBackward0>) ic_loss tensor(557468.8750, grad_fn=<MeanBackward0>) data_loss tensor(169012.0398, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10964.1963, grad_fn=<MeanBackward0>) bc_loss tensor(32.6614, grad_fn=<AddBackward0>) ic_loss tensor(555691., grad_fn=<MeanBackward0>) data_loss tensor(167057.9960, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11163.0322, grad_fn=<MeanBackward0>) bc_loss tensor(37.0726, grad_fn=<AddBackward0>) ic_loss tensor(553912.7500, grad_fn=<MeanBackward0>) data_loss tensor(165130.7006, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 360, Loss: 730243.575568957\n",
      "pde_loss tensor(11363.7617, grad_fn=<MeanBackward0>) bc_loss tensor(31.2580, grad_fn=<AddBackward0>) ic_loss tensor(552139.4375, grad_fn=<MeanBackward0>) data_loss tensor(163232.2522, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11566.3105, grad_fn=<MeanBackward0>) bc_loss tensor(45.7867, grad_fn=<AddBackward0>) ic_loss tensor(550347.3750, grad_fn=<MeanBackward0>) data_loss tensor(161353.6563, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11770.4180, grad_fn=<MeanBackward0>) bc_loss tensor(48.4164, grad_fn=<AddBackward0>) ic_loss tensor(548561.4375, grad_fn=<MeanBackward0>) data_loss tensor(159506.5165, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11975.9121, grad_fn=<MeanBackward0>) bc_loss tensor(47.8955, grad_fn=<AddBackward0>) ic_loss tensor(546777.8125, grad_fn=<MeanBackward0>) data_loss tensor(157690.0255, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12182.6230, grad_fn=<MeanBackward0>) bc_loss tensor(90.8061, grad_fn=<AddBackward0>) ic_loss tensor(544984.5625, grad_fn=<MeanBackward0>) data_loss tensor(155900.9805, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 365, Loss: 713158.9805084701\n",
      "pde_loss tensor(12390.0459, grad_fn=<MeanBackward0>) bc_loss tensor(81.8116, grad_fn=<AddBackward0>) ic_loss tensor(543217.0625, grad_fn=<MeanBackward0>) data_loss tensor(154155.0083, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12598.3643, grad_fn=<MeanBackward0>) bc_loss tensor(70.5550, grad_fn=<AddBackward0>) ic_loss tensor(541426.8750, grad_fn=<MeanBackward0>) data_loss tensor(152431.6777, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12807.8955, grad_fn=<MeanBackward0>) bc_loss tensor(54.3924, grad_fn=<AddBackward0>) ic_loss tensor(539645.5000, grad_fn=<MeanBackward0>) data_loss tensor(150741.4778, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13018.8594, grad_fn=<MeanBackward0>) bc_loss tensor(29.4252, grad_fn=<AddBackward0>) ic_loss tensor(537879.4375, grad_fn=<MeanBackward0>) data_loss tensor(149085.4701, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13231.4912, grad_fn=<MeanBackward0>) bc_loss tensor(34.3568, grad_fn=<AddBackward0>) ic_loss tensor(536091.5625, grad_fn=<MeanBackward0>) data_loss tensor(147448.9271, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 370, Loss: 696806.3646262424\n",
      "pde_loss tensor(13445.6523, grad_fn=<MeanBackward0>) bc_loss tensor(52.1699, grad_fn=<AddBackward0>) ic_loss tensor(534300.1875, grad_fn=<MeanBackward0>) data_loss tensor(145839.1956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13661.0117, grad_fn=<MeanBackward0>) bc_loss tensor(23.5696, grad_fn=<AddBackward0>) ic_loss tensor(532535., grad_fn=<MeanBackward0>) data_loss tensor(144268.5409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13877.7051, grad_fn=<MeanBackward0>) bc_loss tensor(28.0769, grad_fn=<AddBackward0>) ic_loss tensor(530745.9375, grad_fn=<MeanBackward0>) data_loss tensor(142718.7344, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14095.5000, grad_fn=<MeanBackward0>) bc_loss tensor(57.3778, grad_fn=<AddBackward0>) ic_loss tensor(528941.6875, grad_fn=<MeanBackward0>) data_loss tensor(141193.8546, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14313.8438, grad_fn=<MeanBackward0>) bc_loss tensor(40.8683, grad_fn=<AddBackward0>) ic_loss tensor(527164.1875, grad_fn=<MeanBackward0>) data_loss tensor(139710.4405, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 375, Loss: 681229.3154980587\n",
      "pde_loss tensor(14532.8887, grad_fn=<MeanBackward0>) bc_loss tensor(48.0506, grad_fn=<AddBackward0>) ic_loss tensor(525371.9375, grad_fn=<MeanBackward0>) data_loss tensor(138252.6519, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14752.2979, grad_fn=<MeanBackward0>) bc_loss tensor(70.8110, grad_fn=<AddBackward0>) ic_loss tensor(523566.0938, grad_fn=<MeanBackward0>) data_loss tensor(136821.8882, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14971.6514, grad_fn=<MeanBackward0>) bc_loss tensor(52.7143, grad_fn=<AddBackward0>) ic_loss tensor(521779.6562, grad_fn=<MeanBackward0>) data_loss tensor(135430.7345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15191.1494, grad_fn=<MeanBackward0>) bc_loss tensor(46.7763, grad_fn=<AddBackward0>) ic_loss tensor(519983.3125, grad_fn=<MeanBackward0>) data_loss tensor(134067.8677, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15410.9766, grad_fn=<MeanBackward0>) bc_loss tensor(64.9768, grad_fn=<AddBackward0>) ic_loss tensor(518182.7500, grad_fn=<MeanBackward0>) data_loss tensor(132734.0147, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 380, Loss: 666392.7022077423\n",
      "pde_loss tensor(15630.9404, grad_fn=<MeanBackward0>) bc_loss tensor(39.2672, grad_fn=<AddBackward0>) ic_loss tensor(516407.4375, grad_fn=<MeanBackward0>) data_loss tensor(131438.8568, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15851.3359, grad_fn=<MeanBackward0>) bc_loss tensor(33.8252, grad_fn=<AddBackward0>) ic_loss tensor(514619.6875, grad_fn=<MeanBackward0>) data_loss tensor(130169.0280, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16072.1719, grad_fn=<MeanBackward0>) bc_loss tensor(50.1971, grad_fn=<AddBackward0>) ic_loss tensor(512820.6250, grad_fn=<MeanBackward0>) data_loss tensor(128924.5749, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16293.1504, grad_fn=<MeanBackward0>) bc_loss tensor(29.5755, grad_fn=<AddBackward0>) ic_loss tensor(511045.6250, grad_fn=<MeanBackward0>) data_loss tensor(127717.0849, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16514.4746, grad_fn=<MeanBackward0>) bc_loss tensor(25.9470, grad_fn=<AddBackward0>) ic_loss tensor(509263.1875, grad_fn=<MeanBackward0>) data_loss tensor(126535.8589, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 385, Loss: 652339.4839259051\n",
      "pde_loss tensor(16736.0332, grad_fn=<MeanBackward0>) bc_loss tensor(42.3066, grad_fn=<AddBackward0>) ic_loss tensor(507469.2188, grad_fn=<MeanBackward0>) data_loss tensor(125379.6294, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16957.4258, grad_fn=<MeanBackward0>) bc_loss tensor(41.3284, grad_fn=<AddBackward0>) ic_loss tensor(505692., grad_fn=<MeanBackward0>) data_loss tensor(124257.6487, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17178.4727, grad_fn=<MeanBackward0>) bc_loss tensor(32.1141, grad_fn=<AddBackward0>) ic_loss tensor(503922.4062, grad_fn=<MeanBackward0>) data_loss tensor(123166.9994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17399.1348, grad_fn=<MeanBackward0>) bc_loss tensor(41.6369, grad_fn=<AddBackward0>) ic_loss tensor(502141.2500, grad_fn=<MeanBackward0>) data_loss tensor(122101.7207, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17619.0977, grad_fn=<MeanBackward0>) bc_loss tensor(47.1336, grad_fn=<AddBackward0>) ic_loss tensor(500368.9062, grad_fn=<MeanBackward0>) data_loss tensor(121068.0392, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 390, Loss: 639103.1642299165\n",
      "pde_loss tensor(17838.1406, grad_fn=<MeanBackward0>) bc_loss tensor(35.8542, grad_fn=<AddBackward0>) ic_loss tensor(498608.8125, grad_fn=<MeanBackward0>) data_loss tensor(120066.9818, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18056.3574, grad_fn=<MeanBackward0>) bc_loss tensor(41.3843, grad_fn=<AddBackward0>) ic_loss tensor(496838.8125, grad_fn=<MeanBackward0>) data_loss tensor(119091.2887, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18273.6895, grad_fn=<MeanBackward0>) bc_loss tensor(42.7778, grad_fn=<AddBackward0>) ic_loss tensor(495076.6250, grad_fn=<MeanBackward0>) data_loss tensor(118145.4535, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18490.0508, grad_fn=<MeanBackward0>) bc_loss tensor(32.9965, grad_fn=<AddBackward0>) ic_loss tensor(493328.2500, grad_fn=<MeanBackward0>) data_loss tensor(117230.5330, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18705.5938, grad_fn=<MeanBackward0>) bc_loss tensor(33.8170, grad_fn=<AddBackward0>) ic_loss tensor(491574.5000, grad_fn=<MeanBackward0>) data_loss tensor(116340.6547, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 395, Loss: 626654.5609958102\n",
      "pde_loss tensor(18920.1836, grad_fn=<MeanBackward0>) bc_loss tensor(41.4386, grad_fn=<AddBackward0>) ic_loss tensor(489821.1250, grad_fn=<MeanBackward0>) data_loss tensor(115477.0707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19133.6035, grad_fn=<MeanBackward0>) bc_loss tensor(31.9818, grad_fn=<AddBackward0>) ic_loss tensor(488084.3125, grad_fn=<MeanBackward0>) data_loss tensor(114643.7382, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19345.9004, grad_fn=<MeanBackward0>) bc_loss tensor(32.6335, grad_fn=<AddBackward0>) ic_loss tensor(486344.3750, grad_fn=<MeanBackward0>) data_loss tensor(113835.0068, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19556.9277, grad_fn=<MeanBackward0>) bc_loss tensor(42.6303, grad_fn=<AddBackward0>) ic_loss tensor(484604.3125, grad_fn=<MeanBackward0>) data_loss tensor(113051.2428, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19766.3828, grad_fn=<MeanBackward0>) bc_loss tensor(35.1919, grad_fn=<AddBackward0>) ic_loss tensor(482881.7500, grad_fn=<MeanBackward0>) data_loss tensor(112296.6550, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 400, Loss: 614979.9675037749\n",
      "pde_loss tensor(19974.2285, grad_fn=<MeanBackward0>) bc_loss tensor(33.3310, grad_fn=<AddBackward0>) ic_loss tensor(481161.5312, grad_fn=<MeanBackward0>) data_loss tensor(111566.9547, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20180.3633, grad_fn=<MeanBackward0>) bc_loss tensor(38.4028, grad_fn=<AddBackward0>) ic_loss tensor(479440.7188, grad_fn=<MeanBackward0>) data_loss tensor(110860.9507, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20384.6309, grad_fn=<MeanBackward0>) bc_loss tensor(33.5674, grad_fn=<AddBackward0>) ic_loss tensor(477734.6562, grad_fn=<MeanBackward0>) data_loss tensor(110181.8266, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20587.0273, grad_fn=<MeanBackward0>) bc_loss tensor(30.8348, grad_fn=<AddBackward0>) ic_loss tensor(476032.2188, grad_fn=<MeanBackward0>) data_loss tensor(109526.3175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20787.5176, grad_fn=<MeanBackward0>) bc_loss tensor(35.9151, grad_fn=<AddBackward0>) ic_loss tensor(474330.6875, grad_fn=<MeanBackward0>) data_loss tensor(108893.2919, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 405, Loss: 604047.4168605527\n",
      "pde_loss tensor(20985.9297, grad_fn=<MeanBackward0>) bc_loss tensor(32.0024, grad_fn=<AddBackward0>) ic_loss tensor(472642.2500, grad_fn=<MeanBackward0>) data_loss tensor(108284.9710, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21182.2695, grad_fn=<MeanBackward0>) bc_loss tensor(30.6553, grad_fn=<AddBackward0>) ic_loss tensor(470958.5625, grad_fn=<MeanBackward0>) data_loss tensor(107698.9126, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21376.4648, grad_fn=<MeanBackward0>) bc_loss tensor(35.4088, grad_fn=<AddBackward0>) ic_loss tensor(469276.3438, grad_fn=<MeanBackward0>) data_loss tensor(107133.8548, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21568.3438, grad_fn=<MeanBackward0>) bc_loss tensor(31.2683, grad_fn=<AddBackward0>) ic_loss tensor(467607.8125, grad_fn=<MeanBackward0>) data_loss tensor(106591.8989, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21757.8770, grad_fn=<MeanBackward0>) bc_loss tensor(30.2058, grad_fn=<AddBackward0>) ic_loss tensor(465943.8438, grad_fn=<MeanBackward0>) data_loss tensor(106070.5231, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 410, Loss: 593802.460575026\n",
      "pde_loss tensor(21945.0039, grad_fn=<MeanBackward0>) bc_loss tensor(34.7702, grad_fn=<AddBackward0>) ic_loss tensor(464281.7500, grad_fn=<MeanBackward0>) data_loss tensor(105568.6625, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22129.5000, grad_fn=<MeanBackward0>) bc_loss tensor(31.9075, grad_fn=<AddBackward0>) ic_loss tensor(462633.1562, grad_fn=<MeanBackward0>) data_loss tensor(105088.0873, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22311.3340, grad_fn=<MeanBackward0>) bc_loss tensor(31.1367, grad_fn=<AddBackward0>) ic_loss tensor(460989.8750, grad_fn=<MeanBackward0>) data_loss tensor(104626.5838, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22490.4297, grad_fn=<MeanBackward0>) bc_loss tensor(33.9422, grad_fn=<AddBackward0>) ic_loss tensor(459350.6875, grad_fn=<MeanBackward0>) data_loss tensor(104183.3526, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22666.6543, grad_fn=<MeanBackward0>) bc_loss tensor(30.9721, grad_fn=<AddBackward0>) ic_loss tensor(457723.4375, grad_fn=<MeanBackward0>) data_loss tensor(103759.3247, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 415, Loss: 584180.3871820297\n",
      "pde_loss tensor(22839.9688, grad_fn=<MeanBackward0>) bc_loss tensor(30.2679, grad_fn=<AddBackward0>) ic_loss tensor(456101.7500, grad_fn=<MeanBackward0>) data_loss tensor(103352.6386, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23010.3457, grad_fn=<MeanBackward0>) bc_loss tensor(32.5255, grad_fn=<AddBackward0>) ic_loss tensor(454484.8438, grad_fn=<MeanBackward0>) data_loss tensor(102962.6066, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23177.7266, grad_fn=<MeanBackward0>) bc_loss tensor(29.5743, grad_fn=<AddBackward0>) ic_loss tensor(452879.4688, grad_fn=<MeanBackward0>) data_loss tensor(102589.8408, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23342.0703, grad_fn=<MeanBackward0>) bc_loss tensor(29.3936, grad_fn=<AddBackward0>) ic_loss tensor(451279.3125, grad_fn=<MeanBackward0>) data_loss tensor(102232.5892, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23503.3301, grad_fn=<MeanBackward0>) bc_loss tensor(31.4472, grad_fn=<AddBackward0>) ic_loss tensor(449684.5625, grad_fn=<MeanBackward0>) data_loss tensor(101890.3814, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 420, Loss: 575109.7251119878\n",
      "pde_loss tensor(23661.3809, grad_fn=<MeanBackward0>) bc_loss tensor(29.0216, grad_fn=<AddBackward0>) ic_loss tensor(448100.9375, grad_fn=<MeanBackward0>) data_loss tensor(101563.5707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23816.2188, grad_fn=<MeanBackward0>) bc_loss tensor(29.2840, grad_fn=<AddBackward0>) ic_loss tensor(446522.5312, grad_fn=<MeanBackward0>) data_loss tensor(101250.5930, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23967.7754, grad_fn=<MeanBackward0>) bc_loss tensor(30.9980, grad_fn=<AddBackward0>) ic_loss tensor(444950.2188, grad_fn=<MeanBackward0>) data_loss tensor(100951.0671, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24115.9590, grad_fn=<MeanBackward0>) bc_loss tensor(29.0469, grad_fn=<AddBackward0>) ic_loss tensor(443388.5625, grad_fn=<MeanBackward0>) data_loss tensor(100665.1171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24260.7344, grad_fn=<MeanBackward0>) bc_loss tensor(30.0044, grad_fn=<AddBackward0>) ic_loss tensor(441831.8438, grad_fn=<MeanBackward0>) data_loss tensor(100391.3044, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 425, Loss: 566513.89815101\n",
      "pde_loss tensor(24402.0020, grad_fn=<MeanBackward0>) bc_loss tensor(30.9697, grad_fn=<AddBackward0>) ic_loss tensor(440282.7188, grad_fn=<MeanBackward0>) data_loss tensor(100129.4845, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24539.7578, grad_fn=<MeanBackward0>) bc_loss tensor(28.8339, grad_fn=<AddBackward0>) ic_loss tensor(438743.7188, grad_fn=<MeanBackward0>) data_loss tensor(99879.4769, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24673.9863, grad_fn=<MeanBackward0>) bc_loss tensor(29.7365, grad_fn=<AddBackward0>) ic_loss tensor(437209.4062, grad_fn=<MeanBackward0>) data_loss tensor(99639.9407, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24804.6777, grad_fn=<MeanBackward0>) bc_loss tensor(29.8344, grad_fn=<AddBackward0>) ic_loss tensor(435683.1250, grad_fn=<MeanBackward0>) data_loss tensor(99410.8602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24931.8086, grad_fn=<MeanBackward0>) bc_loss tensor(28.0272, grad_fn=<AddBackward0>) ic_loss tensor(434165.9062, grad_fn=<MeanBackward0>) data_loss tensor(99191.8370, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 430, Loss: 558317.5870127789\n",
      "pde_loss tensor(25055.4199, grad_fn=<MeanBackward0>) bc_loss tensor(29.3143, grad_fn=<AddBackward0>) ic_loss tensor(432653.0938, grad_fn=<MeanBackward0>) data_loss tensor(98981.7698, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25175.4629, grad_fn=<MeanBackward0>) bc_loss tensor(29.2393, grad_fn=<AddBackward0>) ic_loss tensor(431148.6250, grad_fn=<MeanBackward0>) data_loss tensor(98780.6809, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25291.8887, grad_fn=<MeanBackward0>) bc_loss tensor(28.0942, grad_fn=<AddBackward0>) ic_loss tensor(429652.2812, grad_fn=<MeanBackward0>) data_loss tensor(98588.0576, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25404.7305, grad_fn=<MeanBackward0>) bc_loss tensor(29.4103, grad_fn=<AddBackward0>) ic_loss tensor(428160.7812, grad_fn=<MeanBackward0>) data_loss tensor(98403.0263, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25513.9082, grad_fn=<MeanBackward0>) bc_loss tensor(28.7888, grad_fn=<AddBackward0>) ic_loss tensor(426678.0312, grad_fn=<MeanBackward0>) data_loss tensor(98225.6022, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 435, Loss: 550446.3209764641\n",
      "pde_loss tensor(25619.4414, grad_fn=<MeanBackward0>) bc_loss tensor(28.1950, grad_fn=<AddBackward0>) ic_loss tensor(425202.2812, grad_fn=<MeanBackward0>) data_loss tensor(98055.1287, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25721.3359, grad_fn=<MeanBackward0>) bc_loss tensor(29.3603, grad_fn=<AddBackward0>) ic_loss tensor(423731.9062, grad_fn=<MeanBackward0>) data_loss tensor(97890.9864, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25819.5508, grad_fn=<MeanBackward0>) bc_loss tensor(28.3718, grad_fn=<AddBackward0>) ic_loss tensor(422270.3125, grad_fn=<MeanBackward0>) data_loss tensor(97733.1398, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25914.1133, grad_fn=<MeanBackward0>) bc_loss tensor(28.1787, grad_fn=<AddBackward0>) ic_loss tensor(420814.8750, grad_fn=<MeanBackward0>) data_loss tensor(97580.8756, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26005.0488, grad_fn=<MeanBackward0>) bc_loss tensor(28.8202, grad_fn=<AddBackward0>) ic_loss tensor(419365.3750, grad_fn=<MeanBackward0>) data_loss tensor(97433.8042, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 440, Loss: 542833.0541737119\n",
      "pde_loss tensor(26092.3398, grad_fn=<MeanBackward0>) bc_loss tensor(27.7478, grad_fn=<AddBackward0>) ic_loss tensor(417924.1562, grad_fn=<MeanBackward0>) data_loss tensor(97291.7736, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26176.0449, grad_fn=<MeanBackward0>) bc_loss tensor(28.1832, grad_fn=<AddBackward0>) ic_loss tensor(416488.0625, grad_fn=<MeanBackward0>) data_loss tensor(97154.1270, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26256.1680, grad_fn=<MeanBackward0>) bc_loss tensor(28.4484, grad_fn=<AddBackward0>) ic_loss tensor(415058.5312, grad_fn=<MeanBackward0>) data_loss tensor(97020.6505, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26332.7246, grad_fn=<MeanBackward0>) bc_loss tensor(27.4966, grad_fn=<AddBackward0>) ic_loss tensor(413636.5000, grad_fn=<MeanBackward0>) data_loss tensor(96891.0877, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26405.7539, grad_fn=<MeanBackward0>) bc_loss tensor(28.1607, grad_fn=<AddBackward0>) ic_loss tensor(412219.2500, grad_fn=<MeanBackward0>) data_loss tensor(96764.8880, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 445, Loss: 535418.0442689004\n",
      "pde_loss tensor(26475.2500, grad_fn=<MeanBackward0>) bc_loss tensor(27.8541, grad_fn=<AddBackward0>) ic_loss tensor(410809.0625, grad_fn=<MeanBackward0>) data_loss tensor(96641.9204, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26541.2637, grad_fn=<MeanBackward0>) bc_loss tensor(27.4156, grad_fn=<AddBackward0>) ic_loss tensor(409405.0938, grad_fn=<MeanBackward0>) data_loss tensor(96521.8260, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26603.8242, grad_fn=<MeanBackward0>) bc_loss tensor(28.1109, grad_fn=<AddBackward0>) ic_loss tensor(408006.0625, grad_fn=<MeanBackward0>) data_loss tensor(96404.2179, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26662.9355, grad_fn=<MeanBackward0>) bc_loss tensor(27.3789, grad_fn=<AddBackward0>) ic_loss tensor(406614.1562, grad_fn=<MeanBackward0>) data_loss tensor(96289.0054, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26718.6680, grad_fn=<MeanBackward0>) bc_loss tensor(27.4096, grad_fn=<AddBackward0>) ic_loss tensor(405227.4062, grad_fn=<MeanBackward0>) data_loss tensor(96175.7590, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 450, Loss: 528149.2589903893\n",
      "pde_loss tensor(26771.0664, grad_fn=<MeanBackward0>) bc_loss tensor(27.6638, grad_fn=<AddBackward0>) ic_loss tensor(403846.0938, grad_fn=<MeanBackward0>) data_loss tensor(96064.2852, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26820.1094, grad_fn=<MeanBackward0>) bc_loss tensor(26.9530, grad_fn=<AddBackward0>) ic_loss tensor(402471.1875, grad_fn=<MeanBackward0>) data_loss tensor(95954.4078, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26865.9082, grad_fn=<MeanBackward0>) bc_loss tensor(27.3972, grad_fn=<AddBackward0>) ic_loss tensor(401100.5312, grad_fn=<MeanBackward0>) data_loss tensor(95845.7696, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26908.4961, grad_fn=<MeanBackward0>) bc_loss tensor(27.1077, grad_fn=<AddBackward0>) ic_loss tensor(399735.8750, grad_fn=<MeanBackward0>) data_loss tensor(95738.2861, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26947.8770, grad_fn=<MeanBackward0>) bc_loss tensor(26.7377, grad_fn=<AddBackward0>) ic_loss tensor(398376.4062, grad_fn=<MeanBackward0>) data_loss tensor(95631.7125, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 455, Loss: 520982.7437091227\n",
      "pde_loss tensor(26984.1387, grad_fn=<MeanBackward0>) bc_loss tensor(27.1425, grad_fn=<AddBackward0>) ic_loss tensor(397021.2188, grad_fn=<MeanBackward0>) data_loss tensor(95525.8071, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27017.3203, grad_fn=<MeanBackward0>) bc_loss tensor(26.5725, grad_fn=<AddBackward0>) ic_loss tensor(395671.9062, grad_fn=<MeanBackward0>) data_loss tensor(95420.4996, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27047.4668, grad_fn=<MeanBackward0>) bc_loss tensor(26.7140, grad_fn=<AddBackward0>) ic_loss tensor(394326.6562, grad_fn=<MeanBackward0>) data_loss tensor(95315.5199, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27074.6035, grad_fn=<MeanBackward0>) bc_loss tensor(26.7111, grad_fn=<AddBackward0>) ic_loss tensor(392986.1875, grad_fn=<MeanBackward0>) data_loss tensor(95210.7624, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27098.7969, grad_fn=<MeanBackward0>) bc_loss tensor(26.2277, grad_fn=<AddBackward0>) ic_loss tensor(391650.8125, grad_fn=<MeanBackward0>) data_loss tensor(95106.0939, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 460, Loss: 513881.93767704506\n",
      "pde_loss tensor(27120.1016, grad_fn=<MeanBackward0>) bc_loss tensor(26.5501, grad_fn=<AddBackward0>) ic_loss tensor(390319.0938, grad_fn=<MeanBackward0>) data_loss tensor(95001.3159, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27138.5684, grad_fn=<MeanBackward0>) bc_loss tensor(26.1406, grad_fn=<AddBackward0>) ic_loss tensor(388992.3750, grad_fn=<MeanBackward0>) data_loss tensor(94896.3863, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27154.2598, grad_fn=<MeanBackward0>) bc_loss tensor(26.0978, grad_fn=<AddBackward0>) ic_loss tensor(387669.5312, grad_fn=<MeanBackward0>) data_loss tensor(94791.1246, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27167.2227, grad_fn=<MeanBackward0>) bc_loss tensor(26.1833, grad_fn=<AddBackward0>) ic_loss tensor(386350.5938, grad_fn=<MeanBackward0>) data_loss tensor(94685.4347, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27177.5137, grad_fn=<MeanBackward0>) bc_loss tensor(25.7496, grad_fn=<AddBackward0>) ic_loss tensor(385036.1562, grad_fn=<MeanBackward0>) data_loss tensor(94579.2534, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 465, Loss: 506818.6596837913\n",
      "pde_loss tensor(27185.1699, grad_fn=<MeanBackward0>) bc_loss tensor(25.9923, grad_fn=<AddBackward0>) ic_loss tensor(383724.9062, grad_fn=<MeanBackward0>) data_loss tensor(94472.4492, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27190.2715, grad_fn=<MeanBackward0>) bc_loss tensor(25.6714, grad_fn=<AddBackward0>) ic_loss tensor(382417.8438, grad_fn=<MeanBackward0>) data_loss tensor(94364.9696, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27192.8672, grad_fn=<MeanBackward0>) bc_loss tensor(25.5856, grad_fn=<AddBackward0>) ic_loss tensor(381114.2188, grad_fn=<MeanBackward0>) data_loss tensor(94256.7326, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27192.9668, grad_fn=<MeanBackward0>) bc_loss tensor(25.6075, grad_fn=<AddBackward0>) ic_loss tensor(379813.9375, grad_fn=<MeanBackward0>) data_loss tensor(94147.6455, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27190.6914, grad_fn=<MeanBackward0>) bc_loss tensor(25.2278, grad_fn=<AddBackward0>) ic_loss tensor(378517.3750, grad_fn=<MeanBackward0>) data_loss tensor(94037.6841, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 470, Loss: 499770.96535588644\n",
      "pde_loss tensor(27186.0273, grad_fn=<MeanBackward0>) bc_loss tensor(25.3885, grad_fn=<AddBackward0>) ic_loss tensor(377223.6250, grad_fn=<MeanBackward0>) data_loss tensor(93926.7419, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27179.0703, grad_fn=<MeanBackward0>) bc_loss tensor(25.0607, grad_fn=<AddBackward0>) ic_loss tensor(375933.4375, grad_fn=<MeanBackward0>) data_loss tensor(93814.8155, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27169.8457, grad_fn=<MeanBackward0>) bc_loss tensor(25.0436, grad_fn=<AddBackward0>) ic_loss tensor(374646.0625, grad_fn=<MeanBackward0>) data_loss tensor(93701.8223, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27158.3887, grad_fn=<MeanBackward0>) bc_loss tensor(24.9641, grad_fn=<AddBackward0>) ic_loss tensor(373361.6562, grad_fn=<MeanBackward0>) data_loss tensor(93587.7441, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27144.8242, grad_fn=<MeanBackward0>) bc_loss tensor(24.7090, grad_fn=<AddBackward0>) ic_loss tensor(372080.2812, grad_fn=<MeanBackward0>) data_loss tensor(93472.5446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 475, Loss: 492722.3570874007\n",
      "pde_loss tensor(27129.1172, grad_fn=<MeanBackward0>) bc_loss tensor(24.8031, grad_fn=<AddBackward0>) ic_loss tensor(370801.3438, grad_fn=<MeanBackward0>) data_loss tensor(93356.1742, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27111.3594, grad_fn=<MeanBackward0>) bc_loss tensor(24.4674, grad_fn=<AddBackward0>) ic_loss tensor(369525.3750, grad_fn=<MeanBackward0>) data_loss tensor(93238.6148, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27091.5898, grad_fn=<MeanBackward0>) bc_loss tensor(24.5282, grad_fn=<AddBackward0>) ic_loss tensor(368251.6875, grad_fn=<MeanBackward0>) data_loss tensor(93119.8445, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27069.8477, grad_fn=<MeanBackward0>) bc_loss tensor(24.2960, grad_fn=<AddBackward0>) ic_loss tensor(366980.6875, grad_fn=<MeanBackward0>) data_loss tensor(92999.8499, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(27046.2031, grad_fn=<MeanBackward0>) bc_loss tensor(24.2009, grad_fn=<AddBackward0>) ic_loss tensor(365711.8438, grad_fn=<MeanBackward0>) data_loss tensor(92878.6141, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 480, Loss: 485660.86413766956\n",
      "pde_loss tensor(27020.6777, grad_fn=<MeanBackward0>) bc_loss tensor(24.1383, grad_fn=<AddBackward0>) ic_loss tensor(364445.2188, grad_fn=<MeanBackward0>) data_loss tensor(92756.1176, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26993.3340, grad_fn=<MeanBackward0>) bc_loss tensor(23.8963, grad_fn=<AddBackward0>) ic_loss tensor(363180.9688, grad_fn=<MeanBackward0>) data_loss tensor(92632.3586, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26964.2188, grad_fn=<MeanBackward0>) bc_loss tensor(23.9380, grad_fn=<AddBackward0>) ic_loss tensor(361918.4688, grad_fn=<MeanBackward0>) data_loss tensor(92507.3259, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26933.3711, grad_fn=<MeanBackward0>) bc_loss tensor(23.6405, grad_fn=<AddBackward0>) ic_loss tensor(360658.2188, grad_fn=<MeanBackward0>) data_loss tensor(92381.0234, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26900.8340, grad_fn=<MeanBackward0>) bc_loss tensor(23.6813, grad_fn=<AddBackward0>) ic_loss tensor(359399.5312, grad_fn=<MeanBackward0>) data_loss tensor(92253.4421, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 485, Loss: 478577.5046044875\n",
      "pde_loss tensor(26866.6445, grad_fn=<MeanBackward0>) bc_loss tensor(23.4149, grad_fn=<AddBackward0>) ic_loss tensor(358142.9688, grad_fn=<MeanBackward0>) data_loss tensor(92124.5880, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26830.8340, grad_fn=<MeanBackward0>) bc_loss tensor(23.3864, grad_fn=<AddBackward0>) ic_loss tensor(356887.8438, grad_fn=<MeanBackward0>) data_loss tensor(91994.4501, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26793.4766, grad_fn=<MeanBackward0>) bc_loss tensor(23.2034, grad_fn=<AddBackward0>) ic_loss tensor(355634.5312, grad_fn=<MeanBackward0>) data_loss tensor(91863.0478, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26754.5723, grad_fn=<MeanBackward0>) bc_loss tensor(23.0826, grad_fn=<AddBackward0>) ic_loss tensor(354382.6875, grad_fn=<MeanBackward0>) data_loss tensor(91730.3793, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26714.1973, grad_fn=<MeanBackward0>) bc_loss tensor(22.9809, grad_fn=<AddBackward0>) ic_loss tensor(353132.3438, grad_fn=<MeanBackward0>) data_loss tensor(91596.4570, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 490, Loss: 471465.988268861\n",
      "pde_loss tensor(26672.3613, grad_fn=<MeanBackward0>) bc_loss tensor(22.7807, grad_fn=<AddBackward0>) ic_loss tensor(351883.4688, grad_fn=<MeanBackward0>) data_loss tensor(91461.2794, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26629.1055, grad_fn=<MeanBackward0>) bc_loss tensor(22.7459, grad_fn=<AddBackward0>) ic_loss tensor(350635.7500, grad_fn=<MeanBackward0>) data_loss tensor(91324.8536, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26584.4727, grad_fn=<MeanBackward0>) bc_loss tensor(22.4888, grad_fn=<AddBackward0>) ic_loss tensor(349389.5312, grad_fn=<MeanBackward0>) data_loss tensor(91187.2007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26538.4824, grad_fn=<MeanBackward0>) bc_loss tensor(22.4981, grad_fn=<AddBackward0>) ic_loss tensor(348144.2812, grad_fn=<MeanBackward0>) data_loss tensor(91048.3362, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26491.1895, grad_fn=<MeanBackward0>) bc_loss tensor(22.1923, grad_fn=<AddBackward0>) ic_loss tensor(346900.4062, grad_fn=<MeanBackward0>) data_loss tensor(90908.2617, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 495, Loss: 464322.0429446815\n",
      "pde_loss tensor(26442.6094, grad_fn=<MeanBackward0>) bc_loss tensor(22.2521, grad_fn=<AddBackward0>) ic_loss tensor(345657.2188, grad_fn=<MeanBackward0>) data_loss tensor(90767.0040, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26392.8418, grad_fn=<MeanBackward0>) bc_loss tensor(21.8968, grad_fn=<AddBackward0>) ic_loss tensor(344415.4375, grad_fn=<MeanBackward0>) data_loss tensor(90624.5621, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26341.9492, grad_fn=<MeanBackward0>) bc_loss tensor(22.0373, grad_fn=<AddBackward0>) ic_loss tensor(343174.0312, grad_fn=<MeanBackward0>) data_loss tensor(90480.9805, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26289.9766, grad_fn=<MeanBackward0>) bc_loss tensor(21.5684, grad_fn=<AddBackward0>) ic_loss tensor(341934.1875, grad_fn=<MeanBackward0>) data_loss tensor(90336.2709, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26236.9492, grad_fn=<MeanBackward0>) bc_loss tensor(21.8653, grad_fn=<AddBackward0>) ic_loss tensor(340694.3125, grad_fn=<MeanBackward0>) data_loss tensor(90190.4500, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 500, Loss: 457143.57497928286\n",
      "pde_loss tensor(26182.8887, grad_fn=<MeanBackward0>) bc_loss tensor(21.1622, grad_fn=<AddBackward0>) ic_loss tensor(339456.1875, grad_fn=<MeanBackward0>) data_loss tensor(90043.5521, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26127.8340, grad_fn=<MeanBackward0>) bc_loss tensor(21.8013, grad_fn=<AddBackward0>) ic_loss tensor(338217.4375, grad_fn=<MeanBackward0>) data_loss tensor(89895.6089, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26071.7969, grad_fn=<MeanBackward0>) bc_loss tensor(20.5978, grad_fn=<AddBackward0>) ic_loss tensor(336981.1562, grad_fn=<MeanBackward0>) data_loss tensor(89746.6008, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(26014.8359, grad_fn=<MeanBackward0>) bc_loss tensor(22.0043, grad_fn=<AddBackward0>) ic_loss tensor(335742.9375, grad_fn=<MeanBackward0>) data_loss tensor(89596.6246, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25956.9062, grad_fn=<MeanBackward0>) bc_loss tensor(19.6501, grad_fn=<AddBackward0>) ic_loss tensor(334509.0625, grad_fn=<MeanBackward0>) data_loss tensor(89445.5994, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 505, Loss: 449931.2244338103\n",
      "pde_loss tensor(25898.1543, grad_fn=<MeanBackward0>) bc_loss tensor(22.9174, grad_fn=<AddBackward0>) ic_loss tensor(333270.0312, grad_fn=<MeanBackward0>) data_loss tensor(89293.7106, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25838.4414, grad_fn=<MeanBackward0>) bc_loss tensor(17.8926, grad_fn=<AddBackward0>) ic_loss tensor(332040.1250, grad_fn=<MeanBackward0>) data_loss tensor(89140.7478, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25778.0215, grad_fn=<MeanBackward0>) bc_loss tensor(25.7882, grad_fn=<AddBackward0>) ic_loss tensor(330797.5312, grad_fn=<MeanBackward0>) data_loss tensor(88987.1055, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25716.5645, grad_fn=<MeanBackward0>) bc_loss tensor(14.9169, grad_fn=<AddBackward0>) ic_loss tensor(329575.8750, grad_fn=<MeanBackward0>) data_loss tensor(88832.1752, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25654.5625, grad_fn=<MeanBackward0>) bc_loss tensor(32.0150, grad_fn=<AddBackward0>) ic_loss tensor(328325.1562, grad_fn=<MeanBackward0>) data_loss tensor(88676.9546, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 510, Loss: 442688.70461460104\n",
      "pde_loss tensor(25591.4004, grad_fn=<MeanBackward0>) bc_loss tensor(12.7636, grad_fn=<AddBackward0>) ic_loss tensor(327116.5000, grad_fn=<MeanBackward0>) data_loss tensor(88519.9817, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25527.8047, grad_fn=<MeanBackward0>) bc_loss tensor(23.0866, grad_fn=<AddBackward0>) ic_loss tensor(325863.9375, grad_fn=<MeanBackward0>) data_loss tensor(88363.1171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25463.3887, grad_fn=<MeanBackward0>) bc_loss tensor(32.1551, grad_fn=<AddBackward0>) ic_loss tensor(324624.8125, grad_fn=<MeanBackward0>) data_loss tensor(88205.1922, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25397.8867, grad_fn=<MeanBackward0>) bc_loss tensor(11.9182, grad_fn=<AddBackward0>) ic_loss tensor(323426.8125, grad_fn=<MeanBackward0>) data_loss tensor(88045.4674, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25332.0410, grad_fn=<MeanBackward0>) bc_loss tensor(13.0950, grad_fn=<AddBackward0>) ic_loss tensor(322181.8750, grad_fn=<MeanBackward0>) data_loss tensor(87886.1291, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 515, Loss: 435413.12911631964\n",
      "pde_loss tensor(25265.7051, grad_fn=<MeanBackward0>) bc_loss tensor(105.8305, grad_fn=<AddBackward0>) ic_loss tensor(320904.2812, grad_fn=<MeanBackward0>) data_loss tensor(87726.9752, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25196.8750, grad_fn=<MeanBackward0>) bc_loss tensor(11.2179, grad_fn=<AddBackward0>) ic_loss tensor(319793.4688, grad_fn=<MeanBackward0>) data_loss tensor(87561.8172, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25127.8242, grad_fn=<MeanBackward0>) bc_loss tensor(11.0729, grad_fn=<AddBackward0>) ic_loss tensor(318634.5312, grad_fn=<MeanBackward0>) data_loss tensor(87397.3755, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(25058.6094, grad_fn=<MeanBackward0>) bc_loss tensor(10.9638, grad_fn=<AddBackward0>) ic_loss tensor(317432.6250, grad_fn=<MeanBackward0>) data_loss tensor(87233.6602, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24989.1504, grad_fn=<MeanBackward0>) bc_loss tensor(10.9845, grad_fn=<AddBackward0>) ic_loss tensor(316192.6562, grad_fn=<MeanBackward0>) data_loss tensor(87070.6338, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 520, Loss: 428263.41504006303\n",
      "pde_loss tensor(24919.3789, grad_fn=<MeanBackward0>) bc_loss tensor(11.5463, grad_fn=<AddBackward0>) ic_loss tensor(314919.1562, grad_fn=<MeanBackward0>) data_loss tensor(86908.1982, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24849.1836, grad_fn=<MeanBackward0>) bc_loss tensor(13.1712, grad_fn=<AddBackward0>) ic_loss tensor(313616.4375, grad_fn=<MeanBackward0>) data_loss tensor(86746.2187, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24778.5059, grad_fn=<MeanBackward0>) bc_loss tensor(76.6303, grad_fn=<AddBackward0>) ic_loss tensor(312289.1875, grad_fn=<MeanBackward0>) data_loss tensor(86584.4979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24706.0254, grad_fn=<MeanBackward0>) bc_loss tensor(68.2840, grad_fn=<AddBackward0>) ic_loss tensor(311055.9375, grad_fn=<MeanBackward0>) data_loss tensor(86418.3934, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24632.0430, grad_fn=<MeanBackward0>) bc_loss tensor(14.0388, grad_fn=<AddBackward0>) ic_loss tensor(309897.4062, grad_fn=<MeanBackward0>) data_loss tensor(86248.4748, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 525, Loss: 420791.97484460514\n",
      "pde_loss tensor(24557.8301, grad_fn=<MeanBackward0>) bc_loss tensor(15.1463, grad_fn=<AddBackward0>) ic_loss tensor(308703.9062, grad_fn=<MeanBackward0>) data_loss tensor(86079.2579, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24483.3691, grad_fn=<MeanBackward0>) bc_loss tensor(15.6623, grad_fn=<AddBackward0>) ic_loss tensor(307476.7500, grad_fn=<MeanBackward0>) data_loss tensor(85910.7777, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24408.5762, grad_fn=<MeanBackward0>) bc_loss tensor(16.2663, grad_fn=<AddBackward0>) ic_loss tensor(306220.4688, grad_fn=<MeanBackward0>) data_loss tensor(85742.9027, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24333.3965, grad_fn=<MeanBackward0>) bc_loss tensor(26.2315, grad_fn=<AddBackward0>) ic_loss tensor(304939.3125, grad_fn=<MeanBackward0>) data_loss tensor(85575.4577, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24257.5176, grad_fn=<MeanBackward0>) bc_loss tensor(145.3487, grad_fn=<AddBackward0>) ic_loss tensor(303660.6562, grad_fn=<MeanBackward0>) data_loss tensor(85407.2084, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 530, Loss: 413470.7396105892\n",
      "pde_loss tensor(24178.7266, grad_fn=<MeanBackward0>) bc_loss tensor(30.3690, grad_fn=<AddBackward0>) ic_loss tensor(302524.9688, grad_fn=<MeanBackward0>) data_loss tensor(85230.2465, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24100.4160, grad_fn=<MeanBackward0>) bc_loss tensor(40.7118, grad_fn=<AddBackward0>) ic_loss tensor(301350.0625, grad_fn=<MeanBackward0>) data_loss tensor(85055.4853, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(24022.4023, grad_fn=<MeanBackward0>) bc_loss tensor(62.3207, grad_fn=<AddBackward0>) ic_loss tensor(300146.0312, grad_fn=<MeanBackward0>) data_loss tensor(84882.3707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23944.0781, grad_fn=<MeanBackward0>) bc_loss tensor(52.1991, grad_fn=<AddBackward0>) ic_loss tensor(298932.8438, grad_fn=<MeanBackward0>) data_loss tensor(84709.3017, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23865.3945, grad_fn=<MeanBackward0>) bc_loss tensor(24.4944, grad_fn=<AddBackward0>) ic_loss tensor(297710.3125, grad_fn=<MeanBackward0>) data_loss tensor(84536.2085, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 535, Loss: 406136.39598272956\n",
      "pde_loss tensor(23786.4980, grad_fn=<MeanBackward0>) bc_loss tensor(14.2594, grad_fn=<AddBackward0>) ic_loss tensor(296468.7188, grad_fn=<MeanBackward0>) data_loss tensor(84363.7624, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23707.2949, grad_fn=<MeanBackward0>) bc_loss tensor(17.6084, grad_fn=<AddBackward0>) ic_loss tensor(295207.3125, grad_fn=<MeanBackward0>) data_loss tensor(84191.8429, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23627.6367, grad_fn=<MeanBackward0>) bc_loss tensor(28.5336, grad_fn=<AddBackward0>) ic_loss tensor(293927.7188, grad_fn=<MeanBackward0>) data_loss tensor(84020.1358, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23547.3672, grad_fn=<MeanBackward0>) bc_loss tensor(56.7232, grad_fn=<AddBackward0>) ic_loss tensor(292635.3125, grad_fn=<MeanBackward0>) data_loss tensor(83848.2622, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23466.1445, grad_fn=<MeanBackward0>) bc_loss tensor(128.8177, grad_fn=<AddBackward0>) ic_loss tensor(291350.3438, grad_fn=<MeanBackward0>) data_loss tensor(83674.9061, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 540, Loss: 398620.21861517115\n",
      "pde_loss tensor(23382.5156, grad_fn=<MeanBackward0>) bc_loss tensor(17.9524, grad_fn=<AddBackward0>) ic_loss tensor(290157.8750, grad_fn=<MeanBackward0>) data_loss tensor(83494.0777, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23298.8320, grad_fn=<MeanBackward0>) bc_loss tensor(22.6049, grad_fn=<AddBackward0>) ic_loss tensor(288948.3125, grad_fn=<MeanBackward0>) data_loss tensor(83314.0976, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23215.2441, grad_fn=<MeanBackward0>) bc_loss tensor(42.7079, grad_fn=<AddBackward0>) ic_loss tensor(287718.7812, grad_fn=<MeanBackward0>) data_loss tensor(83135.3451, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23131.6875, grad_fn=<MeanBackward0>) bc_loss tensor(72.1123, grad_fn=<AddBackward0>) ic_loss tensor(286477.6875, grad_fn=<MeanBackward0>) data_loss tensor(82957.4531, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(23047.8027, grad_fn=<MeanBackward0>) bc_loss tensor(55.5401, grad_fn=<AddBackward0>) ic_loss tensor(285245.6875, grad_fn=<MeanBackward0>) data_loss tensor(82778.9166, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 545, Loss: 391127.9478985383\n",
      "pde_loss tensor(22963.5059, grad_fn=<MeanBackward0>) bc_loss tensor(23.0644, grad_fn=<AddBackward0>) ic_loss tensor(284023.3125, grad_fn=<MeanBackward0>) data_loss tensor(82599.5423, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22879.0234, grad_fn=<MeanBackward0>) bc_loss tensor(19.0978, grad_fn=<AddBackward0>) ic_loss tensor(282788.8438, grad_fn=<MeanBackward0>) data_loss tensor(82420.6537, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22794.2363, grad_fn=<MeanBackward0>) bc_loss tensor(27.7951, grad_fn=<AddBackward0>) ic_loss tensor(281542.0938, grad_fn=<MeanBackward0>) data_loss tensor(82242.0761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22708.9961, grad_fn=<MeanBackward0>) bc_loss tensor(46.5559, grad_fn=<AddBackward0>) ic_loss tensor(280289.2188, grad_fn=<MeanBackward0>) data_loss tensor(82063.3053, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22622.7695, grad_fn=<MeanBackward0>) bc_loss tensor(23.9012, grad_fn=<AddBackward0>) ic_loss tensor(279058.8125, grad_fn=<MeanBackward0>) data_loss tensor(81881.9928, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 550, Loss: 383587.4615264175\n",
      "pde_loss tensor(22536.2773, grad_fn=<MeanBackward0>) bc_loss tensor(24.8466, grad_fn=<AddBackward0>) ic_loss tensor(277822.1562, grad_fn=<MeanBackward0>) data_loss tensor(81700.7649, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22449.7090, grad_fn=<MeanBackward0>) bc_loss tensor(43.1361, grad_fn=<AddBackward0>) ic_loss tensor(276571.2812, grad_fn=<MeanBackward0>) data_loss tensor(81520.4151, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22362.4258, grad_fn=<MeanBackward0>) bc_loss tensor(34.4298, grad_fn=<AddBackward0>) ic_loss tensor(275332.0625, grad_fn=<MeanBackward0>) data_loss tensor(81338.4190, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22274.5859, grad_fn=<MeanBackward0>) bc_loss tensor(18.9262, grad_fn=<AddBackward0>) ic_loss tensor(274098.7500, grad_fn=<MeanBackward0>) data_loss tensor(81155.3533, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22186.6035, grad_fn=<MeanBackward0>) bc_loss tensor(32.3383, grad_fn=<AddBackward0>) ic_loss tensor(272853.9688, grad_fn=<MeanBackward0>) data_loss tensor(80972.9293, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 555, Loss: 376045.8355921641\n",
      "pde_loss tensor(22097.8828, grad_fn=<MeanBackward0>) bc_loss tensor(33.6534, grad_fn=<AddBackward0>) ic_loss tensor(271624.4688, grad_fn=<MeanBackward0>) data_loss tensor(80788.6398, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(22008.5078, grad_fn=<MeanBackward0>) bc_loss tensor(17.1186, grad_fn=<AddBackward0>) ic_loss tensor(270403.3438, grad_fn=<MeanBackward0>) data_loss tensor(80602.9652, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21919.0312, grad_fn=<MeanBackward0>) bc_loss tensor(17.6532, grad_fn=<AddBackward0>) ic_loss tensor(269168.2812, grad_fn=<MeanBackward0>) data_loss tensor(80418.0883, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21829.5957, grad_fn=<MeanBackward0>) bc_loss tensor(26.7694, grad_fn=<AddBackward0>) ic_loss tensor(267915.6250, grad_fn=<MeanBackward0>) data_loss tensor(80234.5915, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21739.8789, grad_fn=<MeanBackward0>) bc_loss tensor(39.4820, grad_fn=<AddBackward0>) ic_loss tensor(266669.1562, grad_fn=<MeanBackward0>) data_loss tensor(80050.4940, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 560, Loss: 368499.02520231693\n",
      "pde_loss tensor(21649.2559, grad_fn=<MeanBackward0>) bc_loss tensor(12.9097, grad_fn=<AddBackward0>) ic_loss tensor(265455.3125, grad_fn=<MeanBackward0>) data_loss tensor(79863.1025, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21558.5625, grad_fn=<MeanBackward0>) bc_loss tensor(13.8494, grad_fn=<AddBackward0>) ic_loss tensor(264223.9688, grad_fn=<MeanBackward0>) data_loss tensor(79676.8681, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21467.7109, grad_fn=<MeanBackward0>) bc_loss tensor(23.7071, grad_fn=<AddBackward0>) ic_loss tensor(262979.2188, grad_fn=<MeanBackward0>) data_loss tensor(79491.3824, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21376.2949, grad_fn=<MeanBackward0>) bc_loss tensor(25.0389, grad_fn=<AddBackward0>) ic_loss tensor(261737.5625, grad_fn=<MeanBackward0>) data_loss tensor(79304.9489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21284.3164, grad_fn=<MeanBackward0>) bc_loss tensor(18.2635, grad_fn=<AddBackward0>) ic_loss tensor(260505., grad_fn=<MeanBackward0>) data_loss tensor(79117.1015, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 565, Loss: 360924.69523960626\n",
      "pde_loss tensor(21192.1465, grad_fn=<MeanBackward0>) bc_loss tensor(20.1757, grad_fn=<AddBackward0>) ic_loss tensor(259266.2969, grad_fn=<MeanBackward0>) data_loss tensor(78929.6103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21099.7266, grad_fn=<MeanBackward0>) bc_loss tensor(22.3218, grad_fn=<AddBackward0>) ic_loss tensor(258027.3594, grad_fn=<MeanBackward0>) data_loss tensor(78741.8382, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(21006.7500, grad_fn=<MeanBackward0>) bc_loss tensor(15.3200, grad_fn=<AddBackward0>) ic_loss tensor(256800.1562, grad_fn=<MeanBackward0>) data_loss tensor(78552.5203, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20913.4570, grad_fn=<MeanBackward0>) bc_loss tensor(14.0907, grad_fn=<AddBackward0>) ic_loss tensor(255568.5156, grad_fn=<MeanBackward0>) data_loss tensor(78363.1413, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20819.8789, grad_fn=<MeanBackward0>) bc_loss tensor(14.7478, grad_fn=<AddBackward0>) ic_loss tensor(254329.6250, grad_fn=<MeanBackward0>) data_loss tensor(78173.9274, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 570, Loss: 353338.17742927605\n",
      "pde_loss tensor(20725.9824, grad_fn=<MeanBackward0>) bc_loss tensor(19.6210, grad_fn=<AddBackward0>) ic_loss tensor(253088.0625, grad_fn=<MeanBackward0>) data_loss tensor(77984.5348, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20631.5820, grad_fn=<MeanBackward0>) bc_loss tensor(16.3353, grad_fn=<AddBackward0>) ic_loss tensor(251856.9844, grad_fn=<MeanBackward0>) data_loss tensor(77793.6699, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20536.8379, grad_fn=<MeanBackward0>) bc_loss tensor(12.0270, grad_fn=<AddBackward0>) ic_loss tensor(250627.2344, grad_fn=<MeanBackward0>) data_loss tensor(77602.2551, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20441.8359, grad_fn=<MeanBackward0>) bc_loss tensor(13.3610, grad_fn=<AddBackward0>) ic_loss tensor(249391.3281, grad_fn=<MeanBackward0>) data_loss tensor(77410.9380, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20346.3672, grad_fn=<MeanBackward0>) bc_loss tensor(14.8568, grad_fn=<AddBackward0>) ic_loss tensor(248155.9688, grad_fn=<MeanBackward0>) data_loss tensor(77218.8966, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 575, Loss: 345736.0841451674\n",
      "pde_loss tensor(20250.3340, grad_fn=<MeanBackward0>) bc_loss tensor(13.8479, grad_fn=<AddBackward0>) ic_loss tensor(246924.9062, grad_fn=<MeanBackward0>) data_loss tensor(77025.6263, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20153.7852, grad_fn=<MeanBackward0>) bc_loss tensor(11.4790, grad_fn=<AddBackward0>) ic_loss tensor(245693.8125, grad_fn=<MeanBackward0>) data_loss tensor(76831.5110, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(20056.9062, grad_fn=<MeanBackward0>) bc_loss tensor(15.5920, grad_fn=<AddBackward0>) ic_loss tensor(244460.2812, grad_fn=<MeanBackward0>) data_loss tensor(76637.1154, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19959.6172, grad_fn=<MeanBackward0>) bc_loss tensor(14.0056, grad_fn=<AddBackward0>) ic_loss tensor(243230.7500, grad_fn=<MeanBackward0>) data_loss tensor(76441.8170, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19861.8770, grad_fn=<MeanBackward0>) bc_loss tensor(10.2516, grad_fn=<AddBackward0>) ic_loss tensor(242006.7656, grad_fn=<MeanBackward0>) data_loss tensor(76245.3817, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 580, Loss: 338124.2723200216\n",
      "pde_loss tensor(19763.8008, grad_fn=<MeanBackward0>) bc_loss tensor(9.8368, grad_fn=<AddBackward0>) ic_loss tensor(240777.4844, grad_fn=<MeanBackward0>) data_loss tensor(76048.8785, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19665.4004, grad_fn=<MeanBackward0>) bc_loss tensor(11.7676, grad_fn=<AddBackward0>) ic_loss tensor(239543.0156, grad_fn=<MeanBackward0>) data_loss tensor(75852.3196, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19566.5977, grad_fn=<MeanBackward0>) bc_loss tensor(13.0337, grad_fn=<AddBackward0>) ic_loss tensor(238311.8750, grad_fn=<MeanBackward0>) data_loss tensor(75654.9127, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19467.2773, grad_fn=<MeanBackward0>) bc_loss tensor(9.3807, grad_fn=<AddBackward0>) ic_loss tensor(237089.6875, grad_fn=<MeanBackward0>) data_loss tensor(75455.9922, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19367.6035, grad_fn=<MeanBackward0>) bc_loss tensor(6.8424, grad_fn=<AddBackward0>) ic_loss tensor(235865.9688, grad_fn=<MeanBackward0>) data_loss tensor(75256.6701, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 585, Loss: 330497.0763447504\n",
      "pde_loss tensor(19267.7637, grad_fn=<MeanBackward0>) bc_loss tensor(8.4675, grad_fn=<AddBackward0>) ic_loss tensor(234636.4219, grad_fn=<MeanBackward0>) data_loss tensor(75057.4521, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19167.7324, grad_fn=<MeanBackward0>) bc_loss tensor(10.7068, grad_fn=<AddBackward0>) ic_loss tensor(233408.1719, grad_fn=<MeanBackward0>) data_loss tensor(74857.5588, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(19067.5039, grad_fn=<MeanBackward0>) bc_loss tensor(7.8715, grad_fn=<AddBackward0>) ic_loss tensor(232187.9375, grad_fn=<MeanBackward0>) data_loss tensor(74656.4262, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18967.3418, grad_fn=<MeanBackward0>) bc_loss tensor(7.9149, grad_fn=<AddBackward0>) ic_loss tensor(230962.7812, grad_fn=<MeanBackward0>) data_loss tensor(74455.6760, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18867.1641, grad_fn=<MeanBackward0>) bc_loss tensor(9.4281, grad_fn=<AddBackward0>) ic_loss tensor(229737.3750, grad_fn=<MeanBackward0>) data_loss tensor(74254.7249, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 590, Loss: 322868.6936299886\n",
      "pde_loss tensor(18766.8438, grad_fn=<MeanBackward0>) bc_loss tensor(4.7596, grad_fn=<AddBackward0>) ic_loss tensor(228518.5781, grad_fn=<MeanBackward0>) data_loss tensor(74052.7093, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18666.6055, grad_fn=<MeanBackward0>) bc_loss tensor(5.7483, grad_fn=<AddBackward0>) ic_loss tensor(227294.9531, grad_fn=<MeanBackward0>) data_loss tensor(73851.1592, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18566.4277, grad_fn=<MeanBackward0>) bc_loss tensor(10.0567, grad_fn=<AddBackward0>) ic_loss tensor(226069.3594, grad_fn=<MeanBackward0>) data_loss tensor(73649.8653, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18466.0488, grad_fn=<MeanBackward0>) bc_loss tensor(5.0818, grad_fn=<AddBackward0>) ic_loss tensor(224853.1562, grad_fn=<MeanBackward0>) data_loss tensor(73447.4034, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18365.5332, grad_fn=<MeanBackward0>) bc_loss tensor(4.7866, grad_fn=<AddBackward0>) ic_loss tensor(223633.7812, grad_fn=<MeanBackward0>) data_loss tensor(73245.3792, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 595, Loss: 315249.4729915938\n",
      "pde_loss tensor(18264.9160, grad_fn=<MeanBackward0>) bc_loss tensor(8.6222, grad_fn=<AddBackward0>) ic_loss tensor(222410.2188, grad_fn=<MeanBackward0>) data_loss tensor(73043.9569, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18164.0020, grad_fn=<MeanBackward0>) bc_loss tensor(5.7684, grad_fn=<AddBackward0>) ic_loss tensor(221194.1250, grad_fn=<MeanBackward0>) data_loss tensor(72841.6856, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(18062.9316, grad_fn=<MeanBackward0>) bc_loss tensor(4.6021, grad_fn=<AddBackward0>) ic_loss tensor(219977.8438, grad_fn=<MeanBackward0>) data_loss tensor(72639.5094, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17961.7578, grad_fn=<MeanBackward0>) bc_loss tensor(7.7058, grad_fn=<AddBackward0>) ic_loss tensor(218757.6250, grad_fn=<MeanBackward0>) data_loss tensor(72437.9268, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17860.3477, grad_fn=<MeanBackward0>) bc_loss tensor(5.4590, grad_fn=<AddBackward0>) ic_loss tensor(217543.7344, grad_fn=<MeanBackward0>) data_loss tensor(72235.6536, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 600, Loss: 307645.20049528196\n",
      "pde_loss tensor(17758.7871, grad_fn=<MeanBackward0>) bc_loss tensor(4.1571, grad_fn=<AddBackward0>) ic_loss tensor(216329.8906, grad_fn=<MeanBackward0>) data_loss tensor(72033.4942, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17657.1387, grad_fn=<MeanBackward0>) bc_loss tensor(6.7475, grad_fn=<AddBackward0>) ic_loss tensor(215112.9688, grad_fn=<MeanBackward0>) data_loss tensor(71831.8655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17555.2773, grad_fn=<MeanBackward0>) bc_loss tensor(5.9628, grad_fn=<AddBackward0>) ic_loss tensor(213900.3438, grad_fn=<MeanBackward0>) data_loss tensor(71629.7531, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17453.2500, grad_fn=<MeanBackward0>) bc_loss tensor(3.6272, grad_fn=<AddBackward0>) ic_loss tensor(212690.5781, grad_fn=<MeanBackward0>) data_loss tensor(71427.3793, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17351.1367, grad_fn=<MeanBackward0>) bc_loss tensor(5.3187, grad_fn=<AddBackward0>) ic_loss tensor(211477.2500, grad_fn=<MeanBackward0>) data_loss tensor(71225.5797, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 605, Loss: 300059.28280740057\n",
      "pde_loss tensor(17248.8672, grad_fn=<MeanBackward0>) bc_loss tensor(6.8414, grad_fn=<AddBackward0>) ic_loss tensor(210265.5469, grad_fn=<MeanBackward0>) data_loss tensor(71023.6653, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17146.3809, grad_fn=<MeanBackward0>) bc_loss tensor(4.1662, grad_fn=<AddBackward0>) ic_loss tensor(209059.5781, grad_fn=<MeanBackward0>) data_loss tensor(70821.0588, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(17043.8223, grad_fn=<MeanBackward0>) bc_loss tensor(5.5766, grad_fn=<AddBackward0>) ic_loss tensor(207850.2969, grad_fn=<MeanBackward0>) data_loss tensor(70619.0030, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16941.0879, grad_fn=<MeanBackward0>) bc_loss tensor(6.2852, grad_fn=<AddBackward0>) ic_loss tensor(206642.8750, grad_fn=<MeanBackward0>) data_loss tensor(70416.7619, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16838.1660, grad_fn=<MeanBackward0>) bc_loss tensor(4.2752, grad_fn=<AddBackward0>) ic_loss tensor(205439.5156, grad_fn=<MeanBackward0>) data_loss tensor(70214.0256, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 610, Loss: 292495.9787471128\n",
      "pde_loss tensor(16735.1797, grad_fn=<MeanBackward0>) bc_loss tensor(5.9946, grad_fn=<AddBackward0>) ic_loss tensor(204233.6562, grad_fn=<MeanBackward0>) data_loss tensor(70011.7611, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16632.0332, grad_fn=<MeanBackward0>) bc_loss tensor(5.4051, grad_fn=<AddBackward0>) ic_loss tensor(203031.5156, grad_fn=<MeanBackward0>) data_loss tensor(69809.1110, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16528.7617, grad_fn=<MeanBackward0>) bc_loss tensor(4.4951, grad_fn=<AddBackward0>) ic_loss tensor(201831.0781, grad_fn=<MeanBackward0>) data_loss tensor(69606.3485, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16425.4062, grad_fn=<MeanBackward0>) bc_loss tensor(5.8770, grad_fn=<AddBackward0>) ic_loss tensor(200629.5000, grad_fn=<MeanBackward0>) data_loss tensor(69403.8585, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16321.9287, grad_fn=<MeanBackward0>) bc_loss tensor(5.5361, grad_fn=<AddBackward0>) ic_loss tensor(199430.9688, grad_fn=<MeanBackward0>) data_loss tensor(69201.0711, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 615, Loss: 284959.5086416631\n",
      "pde_loss tensor(16218.3086, grad_fn=<MeanBackward0>) bc_loss tensor(4.7281, grad_fn=<AddBackward0>) ic_loss tensor(198234.5312, grad_fn=<MeanBackward0>) data_loss tensor(68998.1027, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16114.6045, grad_fn=<MeanBackward0>) bc_loss tensor(5.3102, grad_fn=<AddBackward0>) ic_loss tensor(197038.1250, grad_fn=<MeanBackward0>) data_loss tensor(68795.2263, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(16010.7520, grad_fn=<MeanBackward0>) bc_loss tensor(5.0264, grad_fn=<AddBackward0>) ic_loss tensor(195843.8125, grad_fn=<MeanBackward0>) data_loss tensor(68592.1461, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15906.8008, grad_fn=<MeanBackward0>) bc_loss tensor(5.5686, grad_fn=<AddBackward0>) ic_loss tensor(194650.8281, grad_fn=<MeanBackward0>) data_loss tensor(68389.0133, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15802.7393, grad_fn=<MeanBackward0>) bc_loss tensor(4.8519, grad_fn=<AddBackward0>) ic_loss tensor(193460.4688, grad_fn=<MeanBackward0>) data_loss tensor(68185.6485, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 620, Loss: 277453.7109604564\n",
      "pde_loss tensor(15698.6074, grad_fn=<MeanBackward0>) bc_loss tensor(4.7667, grad_fn=<AddBackward0>) ic_loss tensor(192270.5781, grad_fn=<MeanBackward0>) data_loss tensor(67982.4055, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15594.4043, grad_fn=<MeanBackward0>) bc_loss tensor(5.8885, grad_fn=<AddBackward0>) ic_loss tensor(191081.3125, grad_fn=<MeanBackward0>) data_loss tensor(67779.2378, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15490.0508, grad_fn=<MeanBackward0>) bc_loss tensor(4.4594, grad_fn=<AddBackward0>) ic_loss tensor(189896.5312, grad_fn=<MeanBackward0>) data_loss tensor(67575.5057, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15385.6523, grad_fn=<MeanBackward0>) bc_loss tensor(5.3625, grad_fn=<AddBackward0>) ic_loss tensor(188710.7344, grad_fn=<MeanBackward0>) data_loss tensor(67372.1161, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15281.1562, grad_fn=<MeanBackward0>) bc_loss tensor(5.8185, grad_fn=<AddBackward0>) ic_loss tensor(187527.2344, grad_fn=<MeanBackward0>) data_loss tensor(67168.5490, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 625, Loss: 269982.7521512527\n",
      "pde_loss tensor(15176.5449, grad_fn=<MeanBackward0>) bc_loss tensor(4.8928, grad_fn=<AddBackward0>) ic_loss tensor(186347.6406, grad_fn=<MeanBackward0>) data_loss tensor(66964.5738, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(15071.9082, grad_fn=<MeanBackward0>) bc_loss tensor(5.4693, grad_fn=<AddBackward0>) ic_loss tensor(185167.7969, grad_fn=<MeanBackward0>) data_loss tensor(66760.8524, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14967.1982, grad_fn=<MeanBackward0>) bc_loss tensor(5.9402, grad_fn=<AddBackward0>) ic_loss tensor(183989.6094, grad_fn=<MeanBackward0>) data_loss tensor(66557.1053, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14862.4141, grad_fn=<MeanBackward0>) bc_loss tensor(5.5817, grad_fn=<AddBackward0>) ic_loss tensor(182814.6719, grad_fn=<MeanBackward0>) data_loss tensor(66353.0655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14757.5557, grad_fn=<MeanBackward0>) bc_loss tensor(5.2374, grad_fn=<AddBackward0>) ic_loss tensor(181641.6875, grad_fn=<MeanBackward0>) data_loss tensor(66148.9398, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 630, Loss: 262553.4241747659\n",
      "pde_loss tensor(14652.6758, grad_fn=<MeanBackward0>) bc_loss tensor(5.5647, grad_fn=<AddBackward0>) ic_loss tensor(180469.5625, grad_fn=<MeanBackward0>) data_loss tensor(65944.9604, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14547.7588, grad_fn=<MeanBackward0>) bc_loss tensor(5.8882, grad_fn=<AddBackward0>) ic_loss tensor(179299.3906, grad_fn=<MeanBackward0>) data_loss tensor(65740.9403, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14442.7988, grad_fn=<MeanBackward0>) bc_loss tensor(6.3274, grad_fn=<AddBackward0>) ic_loss tensor(178131.2812, grad_fn=<MeanBackward0>) data_loss tensor(65536.8909, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14337.7725, grad_fn=<MeanBackward0>) bc_loss tensor(5.0854, grad_fn=<AddBackward0>) ic_loss tensor(176967.1875, grad_fn=<MeanBackward0>) data_loss tensor(65332.4780, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14232.7744, grad_fn=<MeanBackward0>) bc_loss tensor(6.4775, grad_fn=<AddBackward0>) ic_loss tensor(175801.6719, grad_fn=<MeanBackward0>) data_loss tensor(65128.6399, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 635, Loss: 255169.5617581202\n",
      "pde_loss tensor(14127.7646, grad_fn=<MeanBackward0>) bc_loss tensor(7.5521, grad_fn=<AddBackward0>) ic_loss tensor(174639.3438, grad_fn=<MeanBackward0>) data_loss tensor(64924.6473, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(14022.6963, grad_fn=<MeanBackward0>) bc_loss tensor(5.5344, grad_fn=<AddBackward0>) ic_loss tensor(173482.3125, grad_fn=<MeanBackward0>) data_loss tensor(64720.1592, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13917.7002, grad_fn=<MeanBackward0>) bc_loss tensor(7.1239, grad_fn=<AddBackward0>) ic_loss tensor(172323.4062, grad_fn=<MeanBackward0>) data_loss tensor(64516.3832, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13812.6709, grad_fn=<MeanBackward0>) bc_loss tensor(6.0980, grad_fn=<AddBackward0>) ic_loss tensor(171169.6094, grad_fn=<MeanBackward0>) data_loss tensor(64312.0569, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13707.7607, grad_fn=<MeanBackward0>) bc_loss tensor(7.4098, grad_fn=<AddBackward0>) ic_loss tensor(170015.2969, grad_fn=<MeanBackward0>) data_loss tensor(64108.2650, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 640, Loss: 247838.73374197935\n",
      "pde_loss tensor(13602.9316, grad_fn=<MeanBackward0>) bc_loss tensor(8.1090, grad_fn=<AddBackward0>) ic_loss tensor(168864.3125, grad_fn=<MeanBackward0>) data_loss tensor(63904.3810, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13498.1074, grad_fn=<MeanBackward0>) bc_loss tensor(5.5598, grad_fn=<AddBackward0>) ic_loss tensor(167719.9688, grad_fn=<MeanBackward0>) data_loss tensor(63699.7926, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13393.4873, grad_fn=<MeanBackward0>) bc_loss tensor(8.6969, grad_fn=<AddBackward0>) ic_loss tensor(166570.0938, grad_fn=<MeanBackward0>) data_loss tensor(63496.6392, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13288.8691, grad_fn=<MeanBackward0>) bc_loss tensor(7.1314, grad_fn=<AddBackward0>) ic_loss tensor(165428.6406, grad_fn=<MeanBackward0>) data_loss tensor(63292.5409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(13184.3779, grad_fn=<MeanBackward0>) bc_loss tensor(7.2384, grad_fn=<AddBackward0>) ic_loss tensor(164287.9062, grad_fn=<MeanBackward0>) data_loss tensor(63088.8450, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 645, Loss: 240568.36064398053\n",
      "pde_loss tensor(13079.9775, grad_fn=<MeanBackward0>) bc_loss tensor(7.8087, grad_fn=<AddBackward0>) ic_loss tensor(163148.9844, grad_fn=<MeanBackward0>) data_loss tensor(62885.3376, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12975.6523, grad_fn=<MeanBackward0>) bc_loss tensor(7.3383, grad_fn=<AddBackward0>) ic_loss tensor(162013.6719, grad_fn=<MeanBackward0>) data_loss tensor(62681.7129, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12871.4697, grad_fn=<MeanBackward0>) bc_loss tensor(8.6214, grad_fn=<AddBackward0>) ic_loss tensor(160879.0625, grad_fn=<MeanBackward0>) data_loss tensor(62478.5726, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12767.3613, grad_fn=<MeanBackward0>) bc_loss tensor(6.1542, grad_fn=<AddBackward0>) ic_loss tensor(159751.6562, grad_fn=<MeanBackward0>) data_loss tensor(62274.8056, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12663.4678, grad_fn=<MeanBackward0>) bc_loss tensor(8.0349, grad_fn=<AddBackward0>) ic_loss tensor(158621.0625, grad_fn=<MeanBackward0>) data_loss tensor(62072.2644, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 650, Loss: 233364.82688664744\n",
      "pde_loss tensor(12559.6338, grad_fn=<MeanBackward0>) bc_loss tensor(7.2501, grad_fn=<AddBackward0>) ic_loss tensor(157496.8906, grad_fn=<MeanBackward0>) data_loss tensor(61869.2041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12455.9453, grad_fn=<MeanBackward0>) bc_loss tensor(7.3124, grad_fn=<AddBackward0>) ic_loss tensor(156374.1250, grad_fn=<MeanBackward0>) data_loss tensor(61666.5571, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12352.3779, grad_fn=<MeanBackward0>) bc_loss tensor(7.5457, grad_fn=<AddBackward0>) ic_loss tensor(155253.9062, grad_fn=<MeanBackward0>) data_loss tensor(61464.1095, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12248.8945, grad_fn=<MeanBackward0>) bc_loss tensor(6.0905, grad_fn=<AddBackward0>) ic_loss tensor(154138.8750, grad_fn=<MeanBackward0>) data_loss tensor(61261.3359, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(12145.5811, grad_fn=<MeanBackward0>) bc_loss tensor(7.0710, grad_fn=<AddBackward0>) ic_loss tensor(153023.5625, grad_fn=<MeanBackward0>) data_loss tensor(61059.3338, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 655, Loss: 226235.5525851804\n",
      "pde_loss tensor(12042.4248, grad_fn=<MeanBackward0>) bc_loss tensor(7.7354, grad_fn=<AddBackward0>) ic_loss tensor(151911.5469, grad_fn=<MeanBackward0>) data_loss tensor(60857.5360, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11939.3613, grad_fn=<MeanBackward0>) bc_loss tensor(5.2518, grad_fn=<AddBackward0>) ic_loss tensor(150806.4844, grad_fn=<MeanBackward0>) data_loss tensor(60655.2128, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11836.5361, grad_fn=<MeanBackward0>) bc_loss tensor(7.2026, grad_fn=<AddBackward0>) ic_loss tensor(149698.6406, grad_fn=<MeanBackward0>) data_loss tensor(60454.2055, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11733.7842, grad_fn=<MeanBackward0>) bc_loss tensor(7.0801, grad_fn=<AddBackward0>) ic_loss tensor(148596.1094, grad_fn=<MeanBackward0>) data_loss tensor(60252.9807, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11631.1338, grad_fn=<MeanBackward0>) bc_loss tensor(5.7388, grad_fn=<AddBackward0>) ic_loss tensor(147498.4219, grad_fn=<MeanBackward0>) data_loss tensor(60051.6966, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 660, Loss: 219186.99343231044\n",
      "pde_loss tensor(11528.6572, grad_fn=<MeanBackward0>) bc_loss tensor(6.5653, grad_fn=<AddBackward0>) ic_loss tensor(146401.1562, grad_fn=<MeanBackward0>) data_loss tensor(59851.1961, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11426.2656, grad_fn=<MeanBackward0>) bc_loss tensor(5.3250, grad_fn=<AddBackward0>) ic_loss tensor(145309.1406, grad_fn=<MeanBackward0>) data_loss tensor(59650.5238, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11324.0586, grad_fn=<MeanBackward0>) bc_loss tensor(7.3447, grad_fn=<AddBackward0>) ic_loss tensor(144216.3281, grad_fn=<MeanBackward0>) data_loss tensor(59450.8727, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11221.9355, grad_fn=<MeanBackward0>) bc_loss tensor(5.1966, grad_fn=<AddBackward0>) ic_loss tensor(143131.7812, grad_fn=<MeanBackward0>) data_loss tensor(59250.5173, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(11120.0371, grad_fn=<MeanBackward0>) bc_loss tensor(5.8011, grad_fn=<AddBackward0>) ic_loss tensor(142046.6250, grad_fn=<MeanBackward0>) data_loss tensor(59051.2436, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 665, Loss: 212223.71230106393\n",
      "pde_loss tensor(11018.3076, grad_fn=<MeanBackward0>) bc_loss tensor(7.1180, grad_fn=<AddBackward0>) ic_loss tensor(140963.8594, grad_fn=<MeanBackward0>) data_loss tensor(58852.4435, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10916.6807, grad_fn=<MeanBackward0>) bc_loss tensor(4.5850, grad_fn=<AddBackward0>) ic_loss tensor(139889.3594, grad_fn=<MeanBackward0>) data_loss tensor(58652.9883, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10815.3145, grad_fn=<MeanBackward0>) bc_loss tensor(6.6732, grad_fn=<AddBackward0>) ic_loss tensor(138811.9219, grad_fn=<MeanBackward0>) data_loss tensor(58455.0834, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10714.0303, grad_fn=<MeanBackward0>) bc_loss tensor(5.1629, grad_fn=<AddBackward0>) ic_loss tensor(137742.2656, grad_fn=<MeanBackward0>) data_loss tensor(58256.5131, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10612.9629, grad_fn=<MeanBackward0>) bc_loss tensor(5.9668, grad_fn=<AddBackward0>) ic_loss tensor(136672.9062, grad_fn=<MeanBackward0>) data_loss tensor(58058.8892, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 670, Loss: 205350.73290822067\n",
      "pde_loss tensor(10512.0732, grad_fn=<MeanBackward0>) bc_loss tensor(6.5198, grad_fn=<AddBackward0>) ic_loss tensor(135607.9375, grad_fn=<MeanBackward0>) data_loss tensor(57861.3843, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10411.2998, grad_fn=<MeanBackward0>) bc_loss tensor(4.2748, grad_fn=<AddBackward0>) ic_loss tensor(134551., grad_fn=<MeanBackward0>) data_loss tensor(57663.2276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10310.8730, grad_fn=<MeanBackward0>) bc_loss tensor(6.8405, grad_fn=<AddBackward0>) ic_loss tensor(133488.5000, grad_fn=<MeanBackward0>) data_loss tensor(57467.3349, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10210.6006, grad_fn=<MeanBackward0>) bc_loss tensor(6.3176, grad_fn=<AddBackward0>) ic_loss tensor(132434.2812, grad_fn=<MeanBackward0>) data_loss tensor(57270.8287, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(10110.4707, grad_fn=<MeanBackward0>) bc_loss tensor(3.8450, grad_fn=<AddBackward0>) ic_loss tensor(131388.2812, grad_fn=<MeanBackward0>) data_loss tensor(57073.6837, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 675, Loss: 198576.27747987857\n",
      "pde_loss tensor(10010.7080, grad_fn=<MeanBackward0>) bc_loss tensor(6.0752, grad_fn=<AddBackward0>) ic_loss tensor(130336.0234, grad_fn=<MeanBackward0>) data_loss tensor(56878.9968, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9911.1553, grad_fn=<MeanBackward0>) bc_loss tensor(9.2390, grad_fn=<AddBackward0>) ic_loss tensor(129289.9141, grad_fn=<MeanBackward0>) data_loss tensor(56684.1964, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9811.6123, grad_fn=<MeanBackward0>) bc_loss tensor(2.9313, grad_fn=<AddBackward0>) ic_loss tensor(128260.4062, grad_fn=<MeanBackward0>) data_loss tensor(56486.8872, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9712.4873, grad_fn=<MeanBackward0>) bc_loss tensor(3.6772, grad_fn=<AddBackward0>) ic_loss tensor(127221.0469, grad_fn=<MeanBackward0>) data_loss tensor(56292.8454, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9613.7217, grad_fn=<MeanBackward0>) bc_loss tensor(14.5392, grad_fn=<AddBackward0>) ic_loss tensor(126178.9688, grad_fn=<MeanBackward0>) data_loss tensor(56100.6533, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 680, Loss: 191907.8877039605\n",
      "pde_loss tensor(9514.9131, grad_fn=<MeanBackward0>) bc_loss tensor(1.5750, grad_fn=<AddBackward0>) ic_loss tensor(125164.2031, grad_fn=<MeanBackward0>) data_loss tensor(55903.8408, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9416.5244, grad_fn=<MeanBackward0>) bc_loss tensor(1.5703, grad_fn=<AddBackward0>) ic_loss tensor(124138.8203, grad_fn=<MeanBackward0>) data_loss tensor(55710.4618, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9318.5410, grad_fn=<MeanBackward0>) bc_loss tensor(11.9891, grad_fn=<AddBackward0>) ic_loss tensor(123104.0938, grad_fn=<MeanBackward0>) data_loss tensor(55520.2882, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9220.5254, grad_fn=<MeanBackward0>) bc_loss tensor(9.4464, grad_fn=<AddBackward0>) ic_loss tensor(122089.3047, grad_fn=<MeanBackward0>) data_loss tensor(55326.7992, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(9122.8867, grad_fn=<MeanBackward0>) bc_loss tensor(6.9272, grad_fn=<AddBackward0>) ic_loss tensor(121078.3516, grad_fn=<MeanBackward0>) data_loss tensor(55134.0143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 685, Loss: 185342.17834968958\n",
      "pde_loss tensor(9025.6973, grad_fn=<MeanBackward0>) bc_loss tensor(10.2343, grad_fn=<AddBackward0>) ic_loss tensor(120064.2734, grad_fn=<MeanBackward0>) data_loss tensor(54943.3972, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8928.6650, grad_fn=<MeanBackward0>) bc_loss tensor(2.9469, grad_fn=<AddBackward0>) ic_loss tensor(119064.1953, grad_fn=<MeanBackward0>) data_loss tensor(54751.0076, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8832.0088, grad_fn=<MeanBackward0>) bc_loss tensor(3.4879, grad_fn=<AddBackward0>) ic_loss tensor(118061.1719, grad_fn=<MeanBackward0>) data_loss tensor(54560.6425, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8735.7930, grad_fn=<MeanBackward0>) bc_loss tensor(10.3635, grad_fn=<AddBackward0>) ic_loss tensor(117054.6250, grad_fn=<MeanBackward0>) data_loss tensor(54372.5438, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8639.7783, grad_fn=<MeanBackward0>) bc_loss tensor(3.9161, grad_fn=<AddBackward0>) ic_loss tensor(116065.6250, grad_fn=<MeanBackward0>) data_loss tensor(54182.0814, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 690, Loss: 178891.4016653504\n",
      "pde_loss tensor(8544.1621, grad_fn=<MeanBackward0>) bc_loss tensor(3.3594, grad_fn=<AddBackward0>) ic_loss tensor(115075.5703, grad_fn=<MeanBackward0>) data_loss tensor(53993.2812, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8448.8691, grad_fn=<MeanBackward0>) bc_loss tensor(7.0730, grad_fn=<AddBackward0>) ic_loss tensor(114084.0391, grad_fn=<MeanBackward0>) data_loss tensor(53806.0948, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8353.7383, grad_fn=<MeanBackward0>) bc_loss tensor(7.0916, grad_fn=<AddBackward0>) ic_loss tensor(113102.2969, grad_fn=<MeanBackward0>) data_loss tensor(53617.8650, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8259.0283, grad_fn=<MeanBackward0>) bc_loss tensor(7.1772, grad_fn=<AddBackward0>) ic_loss tensor(112122.3906, grad_fn=<MeanBackward0>) data_loss tensor(53430.8425, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(8164.6548, grad_fn=<MeanBackward0>) bc_loss tensor(5.9309, grad_fn=<AddBackward0>) ic_loss tensor(111150.9766, grad_fn=<MeanBackward0>) data_loss tensor(53243.4929, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 695, Loss: 172565.05540120127\n",
      "pde_loss tensor(8070.5732, grad_fn=<MeanBackward0>) bc_loss tensor(4.0965, grad_fn=<AddBackward0>) ic_loss tensor(110187.6172, grad_fn=<MeanBackward0>) data_loss tensor(53055.7975, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7977.0640, grad_fn=<MeanBackward0>) bc_loss tensor(9.9260, grad_fn=<AddBackward0>) ic_loss tensor(109211.0703, grad_fn=<MeanBackward0>) data_loss tensor(52872.7147, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7883.7266, grad_fn=<MeanBackward0>) bc_loss tensor(6.5919, grad_fn=<AddBackward0>) ic_loss tensor(108253.3125, grad_fn=<MeanBackward0>) data_loss tensor(52686.8072, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7790.9082, grad_fn=<MeanBackward0>) bc_loss tensor(8.2634, grad_fn=<AddBackward0>) ic_loss tensor(107290.9062, grad_fn=<MeanBackward0>) data_loss tensor(52503.6837, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7698.4189, grad_fn=<MeanBackward0>) bc_loss tensor(3.9884, grad_fn=<AddBackward0>) ic_loss tensor(106342.0391, grad_fn=<MeanBackward0>) data_loss tensor(52319.1571, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 700, Loss: 166363.60243592504\n",
      "pde_loss tensor(7606.3545, grad_fn=<MeanBackward0>) bc_loss tensor(3.5866, grad_fn=<AddBackward0>) ic_loss tensor(105393.6016, grad_fn=<MeanBackward0>) data_loss tensor(52136.1918, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7514.7041, grad_fn=<MeanBackward0>) bc_loss tensor(4.6551, grad_fn=<AddBackward0>) ic_loss tensor(104446.5078, grad_fn=<MeanBackward0>) data_loss tensor(51954.5306, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7423.4526, grad_fn=<MeanBackward0>) bc_loss tensor(6.2322, grad_fn=<AddBackward0>) ic_loss tensor(103504.1406, grad_fn=<MeanBackward0>) data_loss tensor(51773.4494, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7332.6128, grad_fn=<MeanBackward0>) bc_loss tensor(6.7524, grad_fn=<AddBackward0>) ic_loss tensor(102567.1172, grad_fn=<MeanBackward0>) data_loss tensor(51592.8430, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7242.1040, grad_fn=<MeanBackward0>) bc_loss tensor(4.4432, grad_fn=<AddBackward0>) ic_loss tensor(101638.5469, grad_fn=<MeanBackward0>) data_loss tensor(51411.8459, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 705, Loss: 160296.9396841132\n",
      "pde_loss tensor(7152.0674, grad_fn=<MeanBackward0>) bc_loss tensor(8.3647, grad_fn=<AddBackward0>) ic_loss tensor(100708.1094, grad_fn=<MeanBackward0>) data_loss tensor(51232.9972, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(7062.2476, grad_fn=<MeanBackward0>) bc_loss tensor(12.1520, grad_fn=<AddBackward0>) ic_loss tensor(99794.7422, grad_fn=<MeanBackward0>) data_loss tensor(51051.5885, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6973.1479, grad_fn=<MeanBackward0>) bc_loss tensor(15.7170, grad_fn=<AddBackward0>) ic_loss tensor(98862.7969, grad_fn=<MeanBackward0>) data_loss tensor(50876.5889, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6884.2749, grad_fn=<MeanBackward0>) bc_loss tensor(4.1906, grad_fn=<AddBackward0>) ic_loss tensor(97959.2188, grad_fn=<MeanBackward0>) data_loss tensor(50696.6436, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6795.9419, grad_fn=<MeanBackward0>) bc_loss tensor(4.1196, grad_fn=<AddBackward0>) ic_loss tensor(97051.4219, grad_fn=<MeanBackward0>) data_loss tensor(50519.5841, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 710, Loss: 154371.0684437031\n",
      "pde_loss tensor(6708.1660, grad_fn=<MeanBackward0>) bc_loss tensor(8.8556, grad_fn=<AddBackward0>) ic_loss tensor(96138.3203, grad_fn=<MeanBackward0>) data_loss tensor(50345.7398, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6620.7925, grad_fn=<MeanBackward0>) bc_loss tensor(10.0139, grad_fn=<AddBackward0>) ic_loss tensor(95235.7344, grad_fn=<MeanBackward0>) data_loss tensor(50171.2289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6533.8833, grad_fn=<MeanBackward0>) bc_loss tensor(6.5895, grad_fn=<AddBackward0>) ic_loss tensor(94342.0078, grad_fn=<MeanBackward0>) data_loss tensor(49996.5734, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6447.5088, grad_fn=<MeanBackward0>) bc_loss tensor(6.4016, grad_fn=<AddBackward0>) ic_loss tensor(93448.7734, grad_fn=<MeanBackward0>) data_loss tensor(49823.7997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6361.5332, grad_fn=<MeanBackward0>) bc_loss tensor(5.1180, grad_fn=<AddBackward0>) ic_loss tensor(92562.2812, grad_fn=<MeanBackward0>) data_loss tensor(49651.1592, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 715, Loss: 148580.0888740613\n",
      "pde_loss tensor(6275.9971, grad_fn=<MeanBackward0>) bc_loss tensor(4.8869, grad_fn=<AddBackward0>) ic_loss tensor(91680.2969, grad_fn=<MeanBackward0>) data_loss tensor(49479.2650, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6191.0024, grad_fn=<MeanBackward0>) bc_loss tensor(9.7204, grad_fn=<AddBackward0>) ic_loss tensor(90797.7344, grad_fn=<MeanBackward0>) data_loss tensor(49309.5400, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6106.3784, grad_fn=<MeanBackward0>) bc_loss tensor(7.0450, grad_fn=<AddBackward0>) ic_loss tensor(89933.3906, grad_fn=<MeanBackward0>) data_loss tensor(49137.1238, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(6022.5039, grad_fn=<MeanBackward0>) bc_loss tensor(22.0808, grad_fn=<AddBackward0>) ic_loss tensor(89055.3203, grad_fn=<MeanBackward0>) data_loss tensor(48970.2365, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5938.6450, grad_fn=<MeanBackward0>) bc_loss tensor(32.9685, grad_fn=<AddBackward0>) ic_loss tensor(88217.2969, grad_fn=<MeanBackward0>) data_loss tensor(48794.4854, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 720, Loss: 142983.3916012184\n",
      "pde_loss tensor(5855.7515, grad_fn=<MeanBackward0>) bc_loss tensor(3.5459, grad_fn=<AddBackward0>) ic_loss tensor(87351.5156, grad_fn=<MeanBackward0>) data_loss tensor(48628.1449, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5773.5269, grad_fn=<MeanBackward0>) bc_loss tensor(71.5415, grad_fn=<AddBackward0>) ic_loss tensor(86485.3516, grad_fn=<MeanBackward0>) data_loss tensor(48464.1700, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5691.3115, grad_fn=<MeanBackward0>) bc_loss tensor(36.5763, grad_fn=<AddBackward0>) ic_loss tensor(85684.0078, grad_fn=<MeanBackward0>) data_loss tensor(48285.4414, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5610.0942, grad_fn=<MeanBackward0>) bc_loss tensor(37.7185, grad_fn=<AddBackward0>) ic_loss tensor(84856.1406, grad_fn=<MeanBackward0>) data_loss tensor(48116.0361, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5529.8442, grad_fn=<MeanBackward0>) bc_loss tensor(5.2379, grad_fn=<AddBackward0>) ic_loss tensor(84007.1094, grad_fn=<MeanBackward0>) data_loss tensor(47954.7071, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 725, Loss: 137496.89462509385\n",
      "pde_loss tensor(5450.2163, grad_fn=<MeanBackward0>) bc_loss tensor(64.5062, grad_fn=<AddBackward0>) ic_loss tensor(83157.2500, grad_fn=<MeanBackward0>) data_loss tensor(47795.8374, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5370.9141, grad_fn=<MeanBackward0>) bc_loss tensor(13.1504, grad_fn=<AddBackward0>) ic_loss tensor(82328.2109, grad_fn=<MeanBackward0>) data_loss tensor(47633.4959, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5292.2026, grad_fn=<MeanBackward0>) bc_loss tensor(14.2686, grad_fn=<AddBackward0>) ic_loss tensor(81501.5625, grad_fn=<MeanBackward0>) data_loss tensor(47472.7524, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5215.8691, grad_fn=<MeanBackward0>) bc_loss tensor(41.5264, grad_fn=<AddBackward0>) ic_loss tensor(80668.1719, grad_fn=<MeanBackward0>) data_loss tensor(47316.4026, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(5142.0942, grad_fn=<MeanBackward0>) bc_loss tensor(36.9171, grad_fn=<AddBackward0>) ic_loss tensor(79844.0781, grad_fn=<MeanBackward0>) data_loss tensor(47160.5281, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 730, Loss: 132183.61402933858\n",
      "pde_loss tensor(5068.8223, grad_fn=<MeanBackward0>) bc_loss tensor(10.7231, grad_fn=<AddBackward0>) ic_loss tensor(79038.7500, grad_fn=<MeanBackward0>) data_loss tensor(47002.5036, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4996.1079, grad_fn=<MeanBackward0>) bc_loss tensor(30.0876, grad_fn=<AddBackward0>) ic_loss tensor(78238.5469, grad_fn=<MeanBackward0>) data_loss tensor(46845.6505, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4923.8916, grad_fn=<MeanBackward0>) bc_loss tensor(3.4173, grad_fn=<AddBackward0>) ic_loss tensor(77452.3906, grad_fn=<MeanBackward0>) data_loss tensor(46687.4543, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4852.2666, grad_fn=<MeanBackward0>) bc_loss tensor(5.8101, grad_fn=<AddBackward0>) ic_loss tensor(76659.9297, grad_fn=<MeanBackward0>) data_loss tensor(46533.6044, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4781.2300, grad_fn=<MeanBackward0>) bc_loss tensor(5.5884, grad_fn=<AddBackward0>) ic_loss tensor(75860.3047, grad_fn=<MeanBackward0>) data_loss tensor(46384.4418, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 735, Loss: 127031.5667704691\n",
      "pde_loss tensor(4710.7183, grad_fn=<MeanBackward0>) bc_loss tensor(16.0798, grad_fn=<AddBackward0>) ic_loss tensor(75057.4062, grad_fn=<MeanBackward0>) data_loss tensor(46238.7587, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4640.4556, grad_fn=<MeanBackward0>) bc_loss tensor(8.1258, grad_fn=<AddBackward0>) ic_loss tensor(74268.2422, grad_fn=<MeanBackward0>) data_loss tensor(46091.2145, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4570.7383, grad_fn=<MeanBackward0>) bc_loss tensor(25.3451, grad_fn=<AddBackward0>) ic_loss tensor(73479.3594, grad_fn=<MeanBackward0>) data_loss tensor(45946.3420, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4501.4844, grad_fn=<MeanBackward0>) bc_loss tensor(18.9941, grad_fn=<AddBackward0>) ic_loss tensor(72705.7031, grad_fn=<MeanBackward0>) data_loss tensor(45800.1019, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4432.7285, grad_fn=<MeanBackward0>) bc_loss tensor(7.2812, grad_fn=<AddBackward0>) ic_loss tensor(71943.1016, grad_fn=<MeanBackward0>) data_loss tensor(45653.5739, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 740, Loss: 122036.68327981714\n",
      "pde_loss tensor(4364.4795, grad_fn=<MeanBackward0>) bc_loss tensor(8.1001, grad_fn=<AddBackward0>) ic_loss tensor(71186.3438, grad_fn=<MeanBackward0>) data_loss tensor(45508.0154, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4296.7085, grad_fn=<MeanBackward0>) bc_loss tensor(2.6945, grad_fn=<AddBackward0>) ic_loss tensor(70436.1016, grad_fn=<MeanBackward0>) data_loss tensor(45363.1178, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4229.5142, grad_fn=<MeanBackward0>) bc_loss tensor(2.9170, grad_fn=<AddBackward0>) ic_loss tensor(69683.0078, grad_fn=<MeanBackward0>) data_loss tensor(45221.7326, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4162.8774, grad_fn=<MeanBackward0>) bc_loss tensor(4.7328, grad_fn=<AddBackward0>) ic_loss tensor(68931.2109, grad_fn=<MeanBackward0>) data_loss tensor(45082.6885, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(4096.7822, grad_fn=<MeanBackward0>) bc_loss tensor(6.0301, grad_fn=<AddBackward0>) ic_loss tensor(68184.6953, grad_fn=<MeanBackward0>) data_loss tensor(44944.8669, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 745, Loss: 117232.37470127919\n",
      "pde_loss tensor(4031.2217, grad_fn=<MeanBackward0>) bc_loss tensor(11.4911, grad_fn=<AddBackward0>) ic_loss tensor(67444.4531, grad_fn=<MeanBackward0>) data_loss tensor(44807.9828, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3966.1567, grad_fn=<MeanBackward0>) bc_loss tensor(8.7968, grad_fn=<AddBackward0>) ic_loss tensor(66715.4062, grad_fn=<MeanBackward0>) data_loss tensor(44670.5945, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3901.6191, grad_fn=<MeanBackward0>) bc_loss tensor(7.6559, grad_fn=<AddBackward0>) ic_loss tensor(65991.7188, grad_fn=<MeanBackward0>) data_loss tensor(44534.3450, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3837.5698, grad_fn=<MeanBackward0>) bc_loss tensor(3.5096, grad_fn=<AddBackward0>) ic_loss tensor(65277.2188, grad_fn=<MeanBackward0>) data_loss tensor(44397.9289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3774.0920, grad_fn=<MeanBackward0>) bc_loss tensor(3.7636, grad_fn=<AddBackward0>) ic_loss tensor(64561.3164, grad_fn=<MeanBackward0>) data_loss tensor(44264.6143, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 750, Loss: 112603.78622278408\n",
      "pde_loss tensor(3711.1782, grad_fn=<MeanBackward0>) bc_loss tensor(2.8259, grad_fn=<AddBackward0>) ic_loss tensor(63851.7266, grad_fn=<MeanBackward0>) data_loss tensor(44132.1869, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3648.8550, grad_fn=<MeanBackward0>) bc_loss tensor(5.7722, grad_fn=<AddBackward0>) ic_loss tensor(63142.3359, grad_fn=<MeanBackward0>) data_loss tensor(44002.4724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3587.0798, grad_fn=<MeanBackward0>) bc_loss tensor(5.6790, grad_fn=<AddBackward0>) ic_loss tensor(62443.2852, grad_fn=<MeanBackward0>) data_loss tensor(43872.4519, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3525.8879, grad_fn=<MeanBackward0>) bc_loss tensor(13.5406, grad_fn=<AddBackward0>) ic_loss tensor(61742.4258, grad_fn=<MeanBackward0>) data_loss tensor(43745.7222, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3465.1104, grad_fn=<MeanBackward0>) bc_loss tensor(27.6718, grad_fn=<AddBackward0>) ic_loss tensor(61067.8594, grad_fn=<MeanBackward0>) data_loss tensor(43613.6066, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 755, Loss: 108174.24719883298\n",
      "pde_loss tensor(3404.9888, grad_fn=<MeanBackward0>) bc_loss tensor(35.0291, grad_fn=<AddBackward0>) ic_loss tensor(60368.0742, grad_fn=<MeanBackward0>) data_loss tensor(43491.7488, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3345.1243, grad_fn=<MeanBackward0>) bc_loss tensor(23.1003, grad_fn=<AddBackward0>) ic_loss tensor(59714.2773, grad_fn=<MeanBackward0>) data_loss tensor(43358.1872, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3285.9280, grad_fn=<MeanBackward0>) bc_loss tensor(1.0953, grad_fn=<AddBackward0>) ic_loss tensor(59039.7266, grad_fn=<MeanBackward0>) data_loss tensor(43233.6409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3227.3259, grad_fn=<MeanBackward0>) bc_loss tensor(37.4392, grad_fn=<AddBackward0>) ic_loss tensor(58363.4727, grad_fn=<MeanBackward0>) data_loss tensor(43112.3299, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3169.2104, grad_fn=<MeanBackward0>) bc_loss tensor(5.7342, grad_fn=<AddBackward0>) ic_loss tensor(57716.1133, grad_fn=<MeanBackward0>) data_loss tensor(42984.8557, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 760, Loss: 103875.91431889507\n",
      "pde_loss tensor(3111.6936, grad_fn=<MeanBackward0>) bc_loss tensor(11.9287, grad_fn=<AddBackward0>) ic_loss tensor(57060.6016, grad_fn=<MeanBackward0>) data_loss tensor(42862.4889, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(3054.7729, grad_fn=<MeanBackward0>) bc_loss tensor(11.4793, grad_fn=<AddBackward0>) ic_loss tensor(56395.9102, grad_fn=<MeanBackward0>) data_loss tensor(42745.5802, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2998.3640, grad_fn=<MeanBackward0>) bc_loss tensor(21.8227, grad_fn=<AddBackward0>) ic_loss tensor(55744.5117, grad_fn=<MeanBackward0>) data_loss tensor(42627.1399, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2942.4424, grad_fn=<MeanBackward0>) bc_loss tensor(3.9103, grad_fn=<AddBackward0>) ic_loss tensor(55112.4570, grad_fn=<MeanBackward0>) data_loss tensor(42505.2055, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2887.0891, grad_fn=<MeanBackward0>) bc_loss tensor(9.1128, grad_fn=<AddBackward0>) ic_loss tensor(54476.3555, grad_fn=<MeanBackward0>) data_loss tensor(42387.0724, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 765, Loss: 99759.63098791453\n",
      "pde_loss tensor(2832.3035, grad_fn=<MeanBackward0>) bc_loss tensor(6.6148, grad_fn=<AddBackward0>) ic_loss tensor(53833.8477, grad_fn=<MeanBackward0>) data_loss tensor(42273.5374, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2778.0522, grad_fn=<MeanBackward0>) bc_loss tensor(19.3077, grad_fn=<AddBackward0>) ic_loss tensor(53195.9844, grad_fn=<MeanBackward0>) data_loss tensor(42161.1198, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2724.2749, grad_fn=<MeanBackward0>) bc_loss tensor(7.5129, grad_fn=<AddBackward0>) ic_loss tensor(52573.1953, grad_fn=<MeanBackward0>) data_loss tensor(42046.4085, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2671.0269, grad_fn=<MeanBackward0>) bc_loss tensor(9.3986, grad_fn=<AddBackward0>) ic_loss tensor(51955.7148, grad_fn=<MeanBackward0>) data_loss tensor(41932.6097, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2618.3301, grad_fn=<MeanBackward0>) bc_loss tensor(8.6617, grad_fn=<AddBackward0>) ic_loss tensor(51339.1406, grad_fn=<MeanBackward0>) data_loss tensor(41821.1591, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 770, Loss: 95787.29188706476\n",
      "pde_loss tensor(2566.1687, grad_fn=<MeanBackward0>) bc_loss tensor(16.5753, grad_fn=<AddBackward0>) ic_loss tensor(50728.1914, grad_fn=<MeanBackward0>) data_loss tensor(41710.5479, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2514.4980, grad_fn=<MeanBackward0>) bc_loss tensor(7.0313, grad_fn=<AddBackward0>) ic_loss tensor(50138.1797, grad_fn=<MeanBackward0>) data_loss tensor(41595.6355, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2463.4858, grad_fn=<MeanBackward0>) bc_loss tensor(3.4708, grad_fn=<AddBackward0>) ic_loss tensor(49541.1484, grad_fn=<MeanBackward0>) data_loss tensor(41485.7472, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2413.1162, grad_fn=<MeanBackward0>) bc_loss tensor(21.1607, grad_fn=<AddBackward0>) ic_loss tensor(48940.6016, grad_fn=<MeanBackward0>) data_loss tensor(41379.8054, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2363.1179, grad_fn=<MeanBackward0>) bc_loss tensor(15.7270, grad_fn=<AddBackward0>) ic_loss tensor(48362.4453, grad_fn=<MeanBackward0>) data_loss tensor(41268.6651, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 775, Loss: 92009.95412975461\n",
      "pde_loss tensor(2313.8237, grad_fn=<MeanBackward0>) bc_loss tensor(11.1474, grad_fn=<AddBackward0>) ic_loss tensor(47774.8477, grad_fn=<MeanBackward0>) data_loss tensor(41163.6517, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2265.1917, grad_fn=<MeanBackward0>) bc_loss tensor(39.4556, grad_fn=<AddBackward0>) ic_loss tensor(47185.3086, grad_fn=<MeanBackward0>) data_loss tensor(41062.3125, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2217.0852, grad_fn=<MeanBackward0>) bc_loss tensor(2.5180, grad_fn=<AddBackward0>) ic_loss tensor(46632.9297, grad_fn=<MeanBackward0>) data_loss tensor(40951.4842, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2169.6482, grad_fn=<MeanBackward0>) bc_loss tensor(4.7766, grad_fn=<AddBackward0>) ic_loss tensor(46076.7148, grad_fn=<MeanBackward0>) data_loss tensor(40844.9210, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2124.1516, grad_fn=<MeanBackward0>) bc_loss tensor(10.9540, grad_fn=<AddBackward0>) ic_loss tensor(45517.5156, grad_fn=<MeanBackward0>) data_loss tensor(40742.4010, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 780, Loss: 88395.02206457284\n",
      "pde_loss tensor(2080.2183, grad_fn=<MeanBackward0>) bc_loss tensor(4.7264, grad_fn=<AddBackward0>) ic_loss tensor(44965.9609, grad_fn=<MeanBackward0>) data_loss tensor(40640.2520, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(2036.8918, grad_fn=<MeanBackward0>) bc_loss tensor(2.1890, grad_fn=<AddBackward0>) ic_loss tensor(44416.9727, grad_fn=<MeanBackward0>) data_loss tensor(40540.3689, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1994.1884, grad_fn=<MeanBackward0>) bc_loss tensor(2.4904, grad_fn=<AddBackward0>) ic_loss tensor(43866.4023, grad_fn=<MeanBackward0>) data_loss tensor(40444.2102, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1952.0859, grad_fn=<MeanBackward0>) bc_loss tensor(4.6281, grad_fn=<AddBackward0>) ic_loss tensor(43318.3867, grad_fn=<MeanBackward0>) data_loss tensor(40350.3582, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1910.5690, grad_fn=<MeanBackward0>) bc_loss tensor(5.8853, grad_fn=<AddBackward0>) ic_loss tensor(42775.8359, grad_fn=<MeanBackward0>) data_loss tensor(40257.7849, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 785, Loss: 84950.07392491725\n",
      "pde_loss tensor(1869.6395, grad_fn=<MeanBackward0>) bc_loss tensor(12.8601, grad_fn=<AddBackward0>) ic_loss tensor(42237.1367, grad_fn=<MeanBackward0>) data_loss tensor(40167.0401, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1829.2661, grad_fn=<MeanBackward0>) bc_loss tensor(13.5725, grad_fn=<AddBackward0>) ic_loss tensor(41706.4297, grad_fn=<MeanBackward0>) data_loss tensor(40076.6058, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1789.4368, grad_fn=<MeanBackward0>) bc_loss tensor(6.6820, grad_fn=<AddBackward0>) ic_loss tensor(41188.9766, grad_fn=<MeanBackward0>) data_loss tensor(39984.5899, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1750.1935, grad_fn=<MeanBackward0>) bc_loss tensor(8.3041, grad_fn=<AddBackward0>) ic_loss tensor(40672.9766, grad_fn=<MeanBackward0>) data_loss tensor(39895.1484, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1711.4866, grad_fn=<MeanBackward0>) bc_loss tensor(5.0736, grad_fn=<AddBackward0>) ic_loss tensor(40167.9375, grad_fn=<MeanBackward0>) data_loss tensor(39804.7558, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 790, Loss: 81689.25190307983\n",
      "pde_loss tensor(1673.3870, grad_fn=<MeanBackward0>) bc_loss tensor(12.3448, grad_fn=<AddBackward0>) ic_loss tensor(39656.8555, grad_fn=<MeanBackward0>) data_loss tensor(39719.6479, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1635.8167, grad_fn=<MeanBackward0>) bc_loss tensor(5.9571, grad_fn=<AddBackward0>) ic_loss tensor(39161.8828, grad_fn=<MeanBackward0>) data_loss tensor(39631.8036, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1598.8444, grad_fn=<MeanBackward0>) bc_loss tensor(3.6235, grad_fn=<AddBackward0>) ic_loss tensor(38661.4297, grad_fn=<MeanBackward0>) data_loss tensor(39549.0440, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1562.4426, grad_fn=<MeanBackward0>) bc_loss tensor(11.0219, grad_fn=<AddBackward0>) ic_loss tensor(38165.2695, grad_fn=<MeanBackward0>) data_loss tensor(39467.8795, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1526.5745, grad_fn=<MeanBackward0>) bc_loss tensor(12.3530, grad_fn=<AddBackward0>) ic_loss tensor(37684.7500, grad_fn=<MeanBackward0>) data_loss tensor(39384.1161, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 795, Loss: 78607.79184897797\n",
      "pde_loss tensor(1491.2969, grad_fn=<MeanBackward0>) bc_loss tensor(8.0835, grad_fn=<AddBackward0>) ic_loss tensor(37197.0469, grad_fn=<MeanBackward0>) data_loss tensor(39306.0450, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1456.5485, grad_fn=<MeanBackward0>) bc_loss tensor(10.6460, grad_fn=<AddBackward0>) ic_loss tensor(36719.2852, grad_fn=<MeanBackward0>) data_loss tensor(39227.4104, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1422.3041, grad_fn=<MeanBackward0>) bc_loss tensor(9.2093, grad_fn=<AddBackward0>) ic_loss tensor(36254.3086, grad_fn=<MeanBackward0>) data_loss tensor(39147.0791, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1388.6139, grad_fn=<MeanBackward0>) bc_loss tensor(8.0321, grad_fn=<AddBackward0>) ic_loss tensor(35781.6055, grad_fn=<MeanBackward0>) data_loss tensor(39072.6007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1355.3818, grad_fn=<MeanBackward0>) bc_loss tensor(3.9772, grad_fn=<AddBackward0>) ic_loss tensor(35323.4375, grad_fn=<MeanBackward0>) data_loss tensor(38995.4409, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 800, Loss: 75678.2378011985\n",
      "pde_loss tensor(1322.7035, grad_fn=<MeanBackward0>) bc_loss tensor(3.7521, grad_fn=<AddBackward0>) ic_loss tensor(34860.7852, grad_fn=<MeanBackward0>) data_loss tensor(38922.7906, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1290.5410, grad_fn=<MeanBackward0>) bc_loss tensor(2.7853, grad_fn=<AddBackward0>) ic_loss tensor(34405.7578, grad_fn=<MeanBackward0>) data_loss tensor(38850.0091, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1258.8964, grad_fn=<MeanBackward0>) bc_loss tensor(4.1170, grad_fn=<AddBackward0>) ic_loss tensor(33954.3789, grad_fn=<MeanBackward0>) data_loss tensor(38778.5528, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1227.7528, grad_fn=<MeanBackward0>) bc_loss tensor(6.6543, grad_fn=<AddBackward0>) ic_loss tensor(33507.5547, grad_fn=<MeanBackward0>) data_loss tensor(38708.0124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1197.0859, grad_fn=<MeanBackward0>) bc_loss tensor(6.5724, grad_fn=<AddBackward0>) ic_loss tensor(33071.0430, grad_fn=<MeanBackward0>) data_loss tensor(38636.0638, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 805, Loss: 72910.76692892346\n",
      "pde_loss tensor(1166.9154, grad_fn=<MeanBackward0>) bc_loss tensor(11.2076, grad_fn=<AddBackward0>) ic_loss tensor(32632.3281, grad_fn=<MeanBackward0>) data_loss tensor(38567.5590, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1137.1664, grad_fn=<MeanBackward0>) bc_loss tensor(38.6245, grad_fn=<AddBackward0>) ic_loss tensor(32212.9473, grad_fn=<MeanBackward0>) data_loss tensor(38493.9459, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1108.2614, grad_fn=<MeanBackward0>) bc_loss tensor(147.9884, grad_fn=<AddBackward0>) ic_loss tensor(31774.1152, grad_fn=<MeanBackward0>) data_loss tensor(38432.8042, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1079.2699, grad_fn=<MeanBackward0>) bc_loss tensor(350.2144, grad_fn=<AddBackward0>) ic_loss tensor(31418.0215, grad_fn=<MeanBackward0>) data_loss tensor(38340.1255, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(1051.3048, grad_fn=<MeanBackward0>) bc_loss tensor(191.8752, grad_fn=<AddBackward0>) ic_loss tensor(31033.3574, grad_fn=<MeanBackward0>) data_loss tensor(38262.8422, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 810, Loss: 70539.37933840723\n",
      "pde_loss tensor(1024.1598, grad_fn=<MeanBackward0>) bc_loss tensor(6.8871, grad_fn=<AddBackward0>) ic_loss tensor(30631.5020, grad_fn=<MeanBackward0>) data_loss tensor(38196.0932, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(997.5292, grad_fn=<MeanBackward0>) bc_loss tensor(220.4303, grad_fn=<AddBackward0>) ic_loss tensor(30234.6289, grad_fn=<MeanBackward0>) data_loss tensor(38130.6915, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(971.2516, grad_fn=<MeanBackward0>) bc_loss tensor(160.3728, grad_fn=<AddBackward0>) ic_loss tensor(29859.0703, grad_fn=<MeanBackward0>) data_loss tensor(38059.3876, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(945.3563, grad_fn=<MeanBackward0>) bc_loss tensor(7.3899, grad_fn=<AddBackward0>) ic_loss tensor(29495.6641, grad_fn=<MeanBackward0>) data_loss tensor(37985.7377, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(919.9729, grad_fn=<MeanBackward0>) bc_loss tensor(61.5237, grad_fn=<AddBackward0>) ic_loss tensor(29131.4395, grad_fn=<MeanBackward0>) data_loss tensor(37915.1148, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 815, Loss: 68028.0503667855\n",
      "pde_loss tensor(895.2371, grad_fn=<MeanBackward0>) bc_loss tensor(99.4699, grad_fn=<AddBackward0>) ic_loss tensor(28757.7441, grad_fn=<MeanBackward0>) data_loss tensor(37851.2956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(871.1766, grad_fn=<MeanBackward0>) bc_loss tensor(11.9120, grad_fn=<AddBackward0>) ic_loss tensor(28371.6738, grad_fn=<MeanBackward0>) data_loss tensor(37795.5715, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(847.6292, grad_fn=<MeanBackward0>) bc_loss tensor(115.7835, grad_fn=<AddBackward0>) ic_loss tensor(27985.3301, grad_fn=<MeanBackward0>) data_loss tensor(37742.7782, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(824.3705, grad_fn=<MeanBackward0>) bc_loss tensor(15.6833, grad_fn=<AddBackward0>) ic_loss tensor(27625.8691, grad_fn=<MeanBackward0>) data_loss tensor(37681.4095, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(801.5471, grad_fn=<MeanBackward0>) bc_loss tensor(30.8738, grad_fn=<AddBackward0>) ic_loss tensor(27271.9785, grad_fn=<MeanBackward0>) data_loss tensor(37620.3512, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 820, Loss: 65724.74958950107\n",
      "pde_loss tensor(779.3206, grad_fn=<MeanBackward0>) bc_loss tensor(31.7570, grad_fn=<AddBackward0>) ic_loss tensor(26910.0684, grad_fn=<MeanBackward0>) data_loss tensor(37565.6077, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(757.6845, grad_fn=<MeanBackward0>) bc_loss tensor(15.5826, grad_fn=<AddBackward0>) ic_loss tensor(26540.0078, grad_fn=<MeanBackward0>) data_loss tensor(37517.3345, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(736.4726, grad_fn=<MeanBackward0>) bc_loss tensor(37.5809, grad_fn=<AddBackward0>) ic_loss tensor(26179.5684, grad_fn=<MeanBackward0>) data_loss tensor(37467.8278, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(715.6451, grad_fn=<MeanBackward0>) bc_loss tensor(5.4088, grad_fn=<AddBackward0>) ic_loss tensor(25832.3750, grad_fn=<MeanBackward0>) data_loss tensor(37415.3480, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(695.3176, grad_fn=<MeanBackward0>) bc_loss tensor(13.7759, grad_fn=<AddBackward0>) ic_loss tensor(25484.0312, grad_fn=<MeanBackward0>) data_loss tensor(37366.1007, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 825, Loss: 63559.22570482476\n",
      "pde_loss tensor(675.5151, grad_fn=<MeanBackward0>) bc_loss tensor(7.7358, grad_fn=<AddBackward0>) ic_loss tensor(25132.6484, grad_fn=<MeanBackward0>) data_loss tensor(37320.9941, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(656.2006, grad_fn=<MeanBackward0>) bc_loss tensor(8.5378, grad_fn=<AddBackward0>) ic_loss tensor(24780.9590, grad_fn=<MeanBackward0>) data_loss tensor(37278.8663, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(637.3092, grad_fn=<MeanBackward0>) bc_loss tensor(16.2689, grad_fn=<AddBackward0>) ic_loss tensor(24434.1582, grad_fn=<MeanBackward0>) data_loss tensor(37237.3334, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(618.8061, grad_fn=<MeanBackward0>) bc_loss tensor(6.8301, grad_fn=<AddBackward0>) ic_loss tensor(24094.7422, grad_fn=<MeanBackward0>) data_loss tensor(37195.1688, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(600.7177, grad_fn=<MeanBackward0>) bc_loss tensor(7.7026, grad_fn=<AddBackward0>) ic_loss tensor(23759.4160, grad_fn=<MeanBackward0>) data_loss tensor(37153.7574, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 830, Loss: 61521.59331115956\n",
      "pde_loss tensor(583.0781, grad_fn=<MeanBackward0>) bc_loss tensor(9.0035, grad_fn=<AddBackward0>) ic_loss tensor(23424.1035, grad_fn=<MeanBackward0>) data_loss tensor(37114.9053, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(565.8853, grad_fn=<MeanBackward0>) bc_loss tensor(5.1147, grad_fn=<AddBackward0>) ic_loss tensor(23088.6797, grad_fn=<MeanBackward0>) data_loss tensor(37078.7046, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(549.1018, grad_fn=<MeanBackward0>) bc_loss tensor(11.8409, grad_fn=<AddBackward0>) ic_loss tensor(22757.1133, grad_fn=<MeanBackward0>) data_loss tensor(37043.3755, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(532.6971, grad_fn=<MeanBackward0>) bc_loss tensor(4.6634, grad_fn=<AddBackward0>) ic_loss tensor(22433.7266, grad_fn=<MeanBackward0>) data_loss tensor(37006.9611, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(516.7013, grad_fn=<MeanBackward0>) bc_loss tensor(3.0982, grad_fn=<AddBackward0>) ic_loss tensor(22114.7344, grad_fn=<MeanBackward0>) data_loss tensor(36971.1886, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 835, Loss: 59605.72184514074\n",
      "pde_loss tensor(501.1348, grad_fn=<MeanBackward0>) bc_loss tensor(3.0515, grad_fn=<AddBackward0>) ic_loss tensor(21797.7324, grad_fn=<MeanBackward0>) data_loss tensor(36937.1517, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(485.9987, grad_fn=<MeanBackward0>) bc_loss tensor(2.0748, grad_fn=<AddBackward0>) ic_loss tensor(21482.3164, grad_fn=<MeanBackward0>) data_loss tensor(36905.0217, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(471.2789, grad_fn=<MeanBackward0>) bc_loss tensor(3.9164, grad_fn=<AddBackward0>) ic_loss tensor(21169.9434, grad_fn=<MeanBackward0>) data_loss tensor(36874.1011, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(456.9554, grad_fn=<MeanBackward0>) bc_loss tensor(4.8578, grad_fn=<AddBackward0>) ic_loss tensor(20862.5332, grad_fn=<MeanBackward0>) data_loss tensor(36843.4672, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(443.0229, grad_fn=<MeanBackward0>) bc_loss tensor(3.4688, grad_fn=<AddBackward0>) ic_loss tensor(20560.8262, grad_fn=<MeanBackward0>) data_loss tensor(36812.7483, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 840, Loss: 57820.066676821385\n",
      "pde_loss tensor(429.4862, grad_fn=<MeanBackward0>) bc_loss tensor(3.9294, grad_fn=<AddBackward0>) ic_loss tensor(20262.7910, grad_fn=<MeanBackward0>) data_loss tensor(36782.8440, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(416.3444, grad_fn=<MeanBackward0>) bc_loss tensor(4.8070, grad_fn=<AddBackward0>) ic_loss tensor(19967.1699, grad_fn=<MeanBackward0>) data_loss tensor(36754.2889, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(403.5811, grad_fn=<MeanBackward0>) bc_loss tensor(6.0868, grad_fn=<AddBackward0>) ic_loss tensor(19675.9082, grad_fn=<MeanBackward0>) data_loss tensor(36726.1299, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(391.1920, grad_fn=<MeanBackward0>) bc_loss tensor(5.0900, grad_fn=<AddBackward0>) ic_loss tensor(19390.0059, grad_fn=<MeanBackward0>) data_loss tensor(36697.8506, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(379.1827, grad_fn=<MeanBackward0>) bc_loss tensor(4.7148, grad_fn=<AddBackward0>) ic_loss tensor(19107.4414, grad_fn=<MeanBackward0>) data_loss tensor(36670.4134, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 845, Loss: 56161.75325098554\n",
      "pde_loss tensor(367.9634, grad_fn=<MeanBackward0>) bc_loss tensor(4.3021, grad_fn=<AddBackward0>) ic_loss tensor(18827.4004, grad_fn=<MeanBackward0>) data_loss tensor(36644.2403, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(357.3410, grad_fn=<MeanBackward0>) bc_loss tensor(3.7632, grad_fn=<AddBackward0>) ic_loss tensor(18551.8281, grad_fn=<MeanBackward0>) data_loss tensor(36618.4124, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(347.0788, grad_fn=<MeanBackward0>) bc_loss tensor(2.9596, grad_fn=<AddBackward0>) ic_loss tensor(18280.5137, grad_fn=<MeanBackward0>) data_loss tensor(36593.0041, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(337.1758, grad_fn=<MeanBackward0>) bc_loss tensor(2.7269, grad_fn=<AddBackward0>) ic_loss tensor(18011.8105, grad_fn=<MeanBackward0>) data_loss tensor(36568.8201, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(327.6230, grad_fn=<MeanBackward0>) bc_loss tensor(3.1629, grad_fn=<AddBackward0>) ic_loss tensor(17746.5703, grad_fn=<MeanBackward0>) data_loss tensor(36545.4095, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 850, Loss: 54622.76497349213\n",
      "pde_loss tensor(318.4098, grad_fn=<MeanBackward0>) bc_loss tensor(2.2231, grad_fn=<AddBackward0>) ic_loss tensor(17486.5332, grad_fn=<MeanBackward0>) data_loss tensor(36521.8667, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(309.5318, grad_fn=<MeanBackward0>) bc_loss tensor(2.1129, grad_fn=<AddBackward0>) ic_loss tensor(17229.8027, grad_fn=<MeanBackward0>) data_loss tensor(36499.0896, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(300.9851, grad_fn=<MeanBackward0>) bc_loss tensor(2.1542, grad_fn=<AddBackward0>) ic_loss tensor(16975.8477, grad_fn=<MeanBackward0>) data_loss tensor(36477.3116, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(292.7594, grad_fn=<MeanBackward0>) bc_loss tensor(2.5609, grad_fn=<AddBackward0>) ic_loss tensor(16726.3633, grad_fn=<MeanBackward0>) data_loss tensor(36455.6322, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(284.8460, grad_fn=<MeanBackward0>) bc_loss tensor(1.8010, grad_fn=<AddBackward0>) ic_loss tensor(16481.7812, grad_fn=<MeanBackward0>) data_loss tensor(36433.7666, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 855, Loss: 53202.19437043798\n",
      "pde_loss tensor(277.2487, grad_fn=<MeanBackward0>) bc_loss tensor(1.8695, grad_fn=<AddBackward0>) ic_loss tensor(16240.6260, grad_fn=<MeanBackward0>) data_loss tensor(36412.4480, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(269.9684, grad_fn=<MeanBackward0>) bc_loss tensor(2.0387, grad_fn=<AddBackward0>) ic_loss tensor(16001.8467, grad_fn=<MeanBackward0>) data_loss tensor(36392.2289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(262.9964, grad_fn=<MeanBackward0>) bc_loss tensor(1.7317, grad_fn=<AddBackward0>) ic_loss tensor(15766.8594, grad_fn=<MeanBackward0>) data_loss tensor(36372.3649, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(256.3350, grad_fn=<MeanBackward0>) bc_loss tensor(1.5416, grad_fn=<AddBackward0>) ic_loss tensor(15535.3936, grad_fn=<MeanBackward0>) data_loss tensor(36352.9879, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(249.9821, grad_fn=<MeanBackward0>) bc_loss tensor(1.8136, grad_fn=<AddBackward0>) ic_loss tensor(15306.4453, grad_fn=<MeanBackward0>) data_loss tensor(36334.6217, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 860, Loss: 51892.86290407014\n",
      "pde_loss tensor(243.9331, grad_fn=<MeanBackward0>) bc_loss tensor(1.9715, grad_fn=<AddBackward0>) ic_loss tensor(15081.1748, grad_fn=<MeanBackward0>) data_loss tensor(36316.6283, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(238.1844, grad_fn=<MeanBackward0>) bc_loss tensor(1.7647, grad_fn=<AddBackward0>) ic_loss tensor(14859.8877, grad_fn=<MeanBackward0>) data_loss tensor(36298.8225, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(232.7341, grad_fn=<MeanBackward0>) bc_loss tensor(2.1523, grad_fn=<AddBackward0>) ic_loss tensor(14640.9004, grad_fn=<MeanBackward0>) data_loss tensor(36282.0879, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(227.5767, grad_fn=<MeanBackward0>) bc_loss tensor(1.9945, grad_fn=<AddBackward0>) ic_loss tensor(14426.3906, grad_fn=<MeanBackward0>) data_loss tensor(36265.2103, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(222.7078, grad_fn=<MeanBackward0>) bc_loss tensor(2.5760, grad_fn=<AddBackward0>) ic_loss tensor(14213.6016, grad_fn=<MeanBackward0>) data_loss tensor(36249.6446, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 865, Loss: 50688.53031324749\n",
      "pde_loss tensor(218.1216, grad_fn=<MeanBackward0>) bc_loss tensor(2.5563, grad_fn=<AddBackward0>) ic_loss tensor(14005.8789, grad_fn=<MeanBackward0>) data_loss tensor(36233.5351, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(213.8177, grad_fn=<MeanBackward0>) bc_loss tensor(4.3637, grad_fn=<AddBackward0>) ic_loss tensor(13798.5107, grad_fn=<MeanBackward0>) data_loss tensor(36219.4229, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(209.7855, grad_fn=<MeanBackward0>) bc_loss tensor(6.1909, grad_fn=<AddBackward0>) ic_loss tensor(13598.2227, grad_fn=<MeanBackward0>) data_loss tensor(36203.5704, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(206.0363, grad_fn=<MeanBackward0>) bc_loss tensor(16.1681, grad_fn=<AddBackward0>) ic_loss tensor(13394.5361, grad_fn=<MeanBackward0>) data_loss tensor(36191.8088, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(202.5471, grad_fn=<MeanBackward0>) bc_loss tensor(35.5653, grad_fn=<AddBackward0>) ic_loss tensor(13205.1113, grad_fn=<MeanBackward0>) data_loss tensor(36174.2205, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 870, Loss: 49617.444164651504\n",
      "pde_loss tensor(199.3458, grad_fn=<MeanBackward0>) bc_loss tensor(6.3398, grad_fn=<AddBackward0>) ic_loss tensor(13006.1602, grad_fn=<MeanBackward0>) data_loss tensor(36164.1707, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(196.3914, grad_fn=<MeanBackward0>) bc_loss tensor(17.4885, grad_fn=<AddBackward0>) ic_loss tensor(12814.9971, grad_fn=<MeanBackward0>) data_loss tensor(36151.8761, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(193.6736, grad_fn=<MeanBackward0>) bc_loss tensor(14.6401, grad_fn=<AddBackward0>) ic_loss tensor(12634.1113, grad_fn=<MeanBackward0>) data_loss tensor(36135.8048, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(191.2186, grad_fn=<MeanBackward0>) bc_loss tensor(6.6421, grad_fn=<AddBackward0>) ic_loss tensor(12449.1689, grad_fn=<MeanBackward0>) data_loss tensor(36124.1529, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(189.0152, grad_fn=<MeanBackward0>) bc_loss tensor(11.6227, grad_fn=<AddBackward0>) ic_loss tensor(12263.1475, grad_fn=<MeanBackward0>) data_loss tensor(36115.2302, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 875, Loss: 48579.01530706978\n",
      "pde_loss tensor(187.0343, grad_fn=<MeanBackward0>) bc_loss tensor(10.0029, grad_fn=<AddBackward0>) ic_loss tensor(12084.3906, grad_fn=<MeanBackward0>) data_loss tensor(36104.1391, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(185.2775, grad_fn=<MeanBackward0>) bc_loss tensor(4.1303, grad_fn=<AddBackward0>) ic_loss tensor(11911.7549, grad_fn=<MeanBackward0>) data_loss tensor(36091.4956, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(183.7592, grad_fn=<MeanBackward0>) bc_loss tensor(9.3925, grad_fn=<AddBackward0>) ic_loss tensor(11738.3750, grad_fn=<MeanBackward0>) data_loss tensor(36081.2893, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(182.4770, grad_fn=<MeanBackward0>) bc_loss tensor(1.2068, grad_fn=<AddBackward0>) ic_loss tensor(11563.3525, grad_fn=<MeanBackward0>) data_loss tensor(36074.0769, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(181.4126, grad_fn=<MeanBackward0>) bc_loss tensor(11.4417, grad_fn=<AddBackward0>) ic_loss tensor(11391.6104, grad_fn=<MeanBackward0>) data_loss tensor(36066.9308, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 880, Loss: 47651.39567710824\n",
      "pde_loss tensor(180.5607, grad_fn=<MeanBackward0>) bc_loss tensor(0.8900, grad_fn=<AddBackward0>) ic_loss tensor(11226.2754, grad_fn=<MeanBackward0>) data_loss tensor(36057.9551, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(179.9239, grad_fn=<MeanBackward0>) bc_loss tensor(6.7739, grad_fn=<AddBackward0>) ic_loss tensor(11063.3066, grad_fn=<MeanBackward0>) data_loss tensor(36049.5276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(179.5024, grad_fn=<MeanBackward0>) bc_loss tensor(2.3246, grad_fn=<AddBackward0>) ic_loss tensor(10899.2217, grad_fn=<MeanBackward0>) data_loss tensor(36043.7495, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(179.2885, grad_fn=<MeanBackward0>) bc_loss tensor(5.4625, grad_fn=<AddBackward0>) ic_loss tensor(10735.8252, grad_fn=<MeanBackward0>) data_loss tensor(36039.5344, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(179.2804, grad_fn=<MeanBackward0>) bc_loss tensor(4.1136, grad_fn=<AddBackward0>) ic_loss tensor(10578.0918, grad_fn=<MeanBackward0>) data_loss tensor(36033.7917, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 885, Loss: 46795.2770098799\n",
      "pde_loss tensor(179.4737, grad_fn=<MeanBackward0>) bc_loss tensor(2.3361, grad_fn=<AddBackward0>) ic_loss tensor(10425.0947, grad_fn=<MeanBackward0>) data_loss tensor(36027.0310, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(179.8605, grad_fn=<MeanBackward0>) bc_loss tensor(4.0159, grad_fn=<AddBackward0>) ic_loss tensor(10272.4443, grad_fn=<MeanBackward0>) data_loss tensor(36021.9184, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(180.4368, grad_fn=<MeanBackward0>) bc_loss tensor(0.7910, grad_fn=<AddBackward0>) ic_loss tensor(10119.4404, grad_fn=<MeanBackward0>) data_loss tensor(36019.0299, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(181.1969, grad_fn=<MeanBackward0>) bc_loss tensor(4.5829, grad_fn=<AddBackward0>) ic_loss tensor(9969.5781, grad_fn=<MeanBackward0>) data_loss tensor(36015.9622, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(182.1396, grad_fn=<MeanBackward0>) bc_loss tensor(2.9728, grad_fn=<AddBackward0>) ic_loss tensor(9824.8877, grad_fn=<MeanBackward0>) data_loss tensor(36011.2318, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 890, Loss: 46021.23176935137\n",
      "pde_loss tensor(183.2626, grad_fn=<MeanBackward0>) bc_loss tensor(1.4441, grad_fn=<AddBackward0>) ic_loss tensor(9679.9463, grad_fn=<MeanBackward0>) data_loss tensor(36008.9325, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(184.5548, grad_fn=<MeanBackward0>) bc_loss tensor(4.8371, grad_fn=<AddBackward0>) ic_loss tensor(9536.0596, grad_fn=<MeanBackward0>) data_loss tensor(36007.9928, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(186.0174, grad_fn=<MeanBackward0>) bc_loss tensor(1.5661, grad_fn=<AddBackward0>) ic_loss tensor(9397.4434, grad_fn=<MeanBackward0>) data_loss tensor(36004.9332, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(187.6405, grad_fn=<MeanBackward0>) bc_loss tensor(2.0930, grad_fn=<AddBackward0>) ic_loss tensor(9259.5879, grad_fn=<MeanBackward0>) data_loss tensor(36003.3935, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(189.4178, grad_fn=<MeanBackward0>) bc_loss tensor(4.0510, grad_fn=<AddBackward0>) ic_loss tensor(9122.2373, grad_fn=<MeanBackward0>) data_loss tensor(36003.5736, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 895, Loss: 45319.27968698532\n",
      "pde_loss tensor(191.3564, grad_fn=<MeanBackward0>) bc_loss tensor(1.2093, grad_fn=<AddBackward0>) ic_loss tensor(8989.9336, grad_fn=<MeanBackward0>) data_loss tensor(36001.7171, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(193.4407, grad_fn=<MeanBackward0>) bc_loss tensor(1.6665, grad_fn=<AddBackward0>) ic_loss tensor(8858.5918, grad_fn=<MeanBackward0>) data_loss tensor(36001.1339, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(195.6662, grad_fn=<MeanBackward0>) bc_loss tensor(4.2390, grad_fn=<AddBackward0>) ic_loss tensor(8727.7842, grad_fn=<MeanBackward0>) data_loss tensor(36002.1564, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(198.0466, grad_fn=<MeanBackward0>) bc_loss tensor(2.7573, grad_fn=<AddBackward0>) ic_loss tensor(8602.5586, grad_fn=<MeanBackward0>) data_loss tensor(36000.7346, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(200.5524, grad_fn=<MeanBackward0>) bc_loss tensor(1.2112, grad_fn=<AddBackward0>) ic_loss tensor(8476.8955, grad_fn=<MeanBackward0>) data_loss tensor(36001.6502, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 900, Loss: 44680.30935361441\n",
      "pde_loss tensor(203.1914, grad_fn=<MeanBackward0>) bc_loss tensor(2.6547, grad_fn=<AddBackward0>) ic_loss tensor(8353.6943, grad_fn=<MeanBackward0>) data_loss tensor(36002.5138, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(205.9702, grad_fn=<MeanBackward0>) bc_loss tensor(7.8729, grad_fn=<AddBackward0>) ic_loss tensor(8235.2012, grad_fn=<MeanBackward0>) data_loss tensor(36001.4818, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(208.8450, grad_fn=<MeanBackward0>) bc_loss tensor(9.7236, grad_fn=<AddBackward0>) ic_loss tensor(8113.7725, grad_fn=<MeanBackward0>) data_loss tensor(36004.8022, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(211.8725, grad_fn=<MeanBackward0>) bc_loss tensor(6.0341, grad_fn=<AddBackward0>) ic_loss tensor(7999.6304, grad_fn=<MeanBackward0>) data_loss tensor(36003.9634, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(214.9890, grad_fn=<MeanBackward0>) bc_loss tensor(0.5687, grad_fn=<AddBackward0>) ic_loss tensor(7883.0513, grad_fn=<MeanBackward0>) data_loss tensor(36006.9655, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 905, Loss: 44105.57443166128\n",
      "pde_loss tensor(218.2247, grad_fn=<MeanBackward0>) bc_loss tensor(3.2875, grad_fn=<AddBackward0>) ic_loss tensor(7768.9111, grad_fn=<MeanBackward0>) data_loss tensor(36009.7036, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(221.5935, grad_fn=<MeanBackward0>) bc_loss tensor(7.0007, grad_fn=<AddBackward0>) ic_loss tensor(7659.2139, grad_fn=<MeanBackward0>) data_loss tensor(36010.5120, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(225.0256, grad_fn=<MeanBackward0>) bc_loss tensor(3.3807, grad_fn=<AddBackward0>) ic_loss tensor(7546.8081, grad_fn=<MeanBackward0>) data_loss tensor(36015.3839, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(228.5898, grad_fn=<MeanBackward0>) bc_loss tensor(0.4903, grad_fn=<AddBackward0>) ic_loss tensor(7439.1719, grad_fn=<MeanBackward0>) data_loss tensor(36018.0284, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(232.2547, grad_fn=<MeanBackward0>) bc_loss tensor(2.7758, grad_fn=<AddBackward0>) ic_loss tensor(7333.2642, grad_fn=<MeanBackward0>) data_loss tensor(36020.9217, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 910, Loss: 43589.216166842016\n",
      "pde_loss tensor(235.9878, grad_fn=<MeanBackward0>) bc_loss tensor(4.4049, grad_fn=<AddBackward0>) ic_loss tensor(7226.6221, grad_fn=<MeanBackward0>) data_loss tensor(36026.1562, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(239.8542, grad_fn=<MeanBackward0>) bc_loss tensor(1.9450, grad_fn=<AddBackward0>) ic_loss tensor(7125.2930, grad_fn=<MeanBackward0>) data_loss tensor(36028.6269, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(243.7814, grad_fn=<MeanBackward0>) bc_loss tensor(0.3671, grad_fn=<AddBackward0>) ic_loss tensor(7023.4482, grad_fn=<MeanBackward0>) data_loss tensor(36033.1791, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(247.7932, grad_fn=<MeanBackward0>) bc_loss tensor(2.5438, grad_fn=<AddBackward0>) ic_loss tensor(6923.1558, grad_fn=<MeanBackward0>) data_loss tensor(36038.0162, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(251.9169, grad_fn=<MeanBackward0>) bc_loss tensor(6.1114, grad_fn=<AddBackward0>) ic_loss tensor(6826.9204, grad_fn=<MeanBackward0>) data_loss tensor(36040.9814, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 915, Loss: 43125.93012317354\n",
      "pde_loss tensor(256.0578, grad_fn=<MeanBackward0>) bc_loss tensor(4.7331, grad_fn=<AddBackward0>) ic_loss tensor(6728.2715, grad_fn=<MeanBackward0>) data_loss tensor(36047.6284, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(260.3267, grad_fn=<MeanBackward0>) bc_loss tensor(0.6653, grad_fn=<AddBackward0>) ic_loss tensor(6634.6167, grad_fn=<MeanBackward0>) data_loss tensor(36051.5212, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(264.6477, grad_fn=<MeanBackward0>) bc_loss tensor(0.8574, grad_fn=<AddBackward0>) ic_loss tensor(6541.1167, grad_fn=<MeanBackward0>) data_loss tensor(36056.7979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(269.0132, grad_fn=<MeanBackward0>) bc_loss tensor(5.1242, grad_fn=<AddBackward0>) ic_loss tensor(6447.7188, grad_fn=<MeanBackward0>) data_loss tensor(36063.5134, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(273.5133, grad_fn=<MeanBackward0>) bc_loss tensor(12.1386, grad_fn=<AddBackward0>) ic_loss tensor(6359.9150, grad_fn=<MeanBackward0>) data_loss tensor(36066.8706, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 920, Loss: 42712.437458249326\n",
      "pde_loss tensor(277.9769, grad_fn=<MeanBackward0>) bc_loss tensor(10.3149, grad_fn=<AddBackward0>) ic_loss tensor(6268.1509, grad_fn=<MeanBackward0>) data_loss tensor(36075.2419, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(282.5905, grad_fn=<MeanBackward0>) bc_loss tensor(3.8952, grad_fn=<AddBackward0>) ic_loss tensor(6183.9521, grad_fn=<MeanBackward0>) data_loss tensor(36078.4997, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(287.2024, grad_fn=<MeanBackward0>) bc_loss tensor(0.3236, grad_fn=<AddBackward0>) ic_loss tensor(6098.0151, grad_fn=<MeanBackward0>) data_loss tensor(36084.7303, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(291.8571, grad_fn=<MeanBackward0>) bc_loss tensor(4.8455, grad_fn=<AddBackward0>) ic_loss tensor(6012.9531, grad_fn=<MeanBackward0>) data_loss tensor(36091.5867, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(296.6220, grad_fn=<MeanBackward0>) bc_loss tensor(4.4562, grad_fn=<AddBackward0>) ic_loss tensor(5932.0884, grad_fn=<MeanBackward0>) data_loss tensor(36096.0108, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 925, Loss: 42329.17734535825\n",
      "pde_loss tensor(301.3685, grad_fn=<MeanBackward0>) bc_loss tensor(0.6420, grad_fn=<AddBackward0>) ic_loss tensor(5849.4131, grad_fn=<MeanBackward0>) data_loss tensor(36103.4373, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(306.1654, grad_fn=<MeanBackward0>) bc_loss tensor(3.3210, grad_fn=<AddBackward0>) ic_loss tensor(5768.3589, grad_fn=<MeanBackward0>) data_loss tensor(36110.7487, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(311.0548, grad_fn=<MeanBackward0>) bc_loss tensor(4.5207, grad_fn=<AddBackward0>) ic_loss tensor(5690.9702, grad_fn=<MeanBackward0>) data_loss tensor(36116.0610, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(315.9128, grad_fn=<MeanBackward0>) bc_loss tensor(1.0780, grad_fn=<AddBackward0>) ic_loss tensor(5611.7051, grad_fn=<MeanBackward0>) data_loss tensor(36124.3824, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(320.8329, grad_fn=<MeanBackward0>) bc_loss tensor(1.2252, grad_fn=<AddBackward0>) ic_loss tensor(5534.8916, grad_fn=<MeanBackward0>) data_loss tensor(36131.7786, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 930, Loss: 41988.72831790996\n",
      "pde_loss tensor(325.8211, grad_fn=<MeanBackward0>) bc_loss tensor(4.0746, grad_fn=<AddBackward0>) ic_loss tensor(5460.6709, grad_fn=<MeanBackward0>) data_loss tensor(36138.0468, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(330.7701, grad_fn=<MeanBackward0>) bc_loss tensor(4.3288, grad_fn=<AddBackward0>) ic_loss tensor(5384.6953, grad_fn=<MeanBackward0>) data_loss tensor(36147.2028, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(335.8238, grad_fn=<MeanBackward0>) bc_loss tensor(3.0958, grad_fn=<AddBackward0>) ic_loss tensor(5313.2212, grad_fn=<MeanBackward0>) data_loss tensor(36153.4520, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(340.8405, grad_fn=<MeanBackward0>) bc_loss tensor(1.1015, grad_fn=<AddBackward0>) ic_loss tensor(5240.2515, grad_fn=<MeanBackward0>) data_loss tensor(36162.3118, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(345.9211, grad_fn=<MeanBackward0>) bc_loss tensor(0.2215, grad_fn=<AddBackward0>) ic_loss tensor(5170.0483, grad_fn=<MeanBackward0>) data_loss tensor(36169.8069, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 935, Loss: 41685.99777600813\n",
      "pde_loss tensor(351.0198, grad_fn=<MeanBackward0>) bc_loss tensor(0.5051, grad_fn=<AddBackward0>) ic_loss tensor(5100.7129, grad_fn=<MeanBackward0>) data_loss tensor(36177.6814, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(356.1173, grad_fn=<MeanBackward0>) bc_loss tensor(2.0079, grad_fn=<AddBackward0>) ic_loss tensor(5031.6665, grad_fn=<MeanBackward0>) data_loss tensor(36186.4583, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(361.2904, grad_fn=<MeanBackward0>) bc_loss tensor(6.2096, grad_fn=<AddBackward0>) ic_loss tensor(4966.0781, grad_fn=<MeanBackward0>) data_loss tensor(36193.1895, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(366.3779, grad_fn=<MeanBackward0>) bc_loss tensor(13.5892, grad_fn=<AddBackward0>) ic_loss tensor(4897.8984, grad_fn=<MeanBackward0>) data_loss tensor(36203.5239, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(371.6444, grad_fn=<MeanBackward0>) bc_loss tensor(35.4558, grad_fn=<AddBackward0>) ic_loss tensor(4837.2681, grad_fn=<MeanBackward0>) data_loss tensor(36207.9136, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 940, Loss: 41452.281786108615\n",
      "pde_loss tensor(376.6809, grad_fn=<MeanBackward0>) bc_loss tensor(21.5491, grad_fn=<AddBackward0>) ic_loss tensor(4769.8569, grad_fn=<MeanBackward0>) data_loss tensor(36219.9696, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(381.9198, grad_fn=<MeanBackward0>) bc_loss tensor(1.4590, grad_fn=<AddBackward0>) ic_loss tensor(4710.1553, grad_fn=<MeanBackward0>) data_loss tensor(36225.6775, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(387.1401, grad_fn=<MeanBackward0>) bc_loss tensor(10.3849, grad_fn=<AddBackward0>) ic_loss tensor(4650.5508, grad_fn=<MeanBackward0>) data_loss tensor(36232.3683, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(392.2480, grad_fn=<MeanBackward0>) bc_loss tensor(9.1828, grad_fn=<AddBackward0>) ic_loss tensor(4588.3364, grad_fn=<MeanBackward0>) data_loss tensor(36242.6062, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(397.4604, grad_fn=<MeanBackward0>) bc_loss tensor(2.6781, grad_fn=<AddBackward0>) ic_loss tensor(4530.0869, grad_fn=<MeanBackward0>) data_loss tensor(36250.0272, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 945, Loss: 41180.25281515898\n",
      "pde_loss tensor(402.6967, grad_fn=<MeanBackward0>) bc_loss tensor(10.5237, grad_fn=<AddBackward0>) ic_loss tensor(4473.3271, grad_fn=<MeanBackward0>) data_loss tensor(36256.9979, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(407.8109, grad_fn=<MeanBackward0>) bc_loss tensor(3.5476, grad_fn=<AddBackward0>) ic_loss tensor(4413.9033, grad_fn=<MeanBackward0>) data_loss tensor(36267.5289, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(412.9841, grad_fn=<MeanBackward0>) bc_loss tensor(4.1838, grad_fn=<AddBackward0>) ic_loss tensor(4357.1982, grad_fn=<MeanBackward0>) data_loss tensor(36276.4175, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(418.2308, grad_fn=<MeanBackward0>) bc_loss tensor(8.1851, grad_fn=<AddBackward0>) ic_loss tensor(4303.4663, grad_fn=<MeanBackward0>) data_loss tensor(36283.2948, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(423.3624, grad_fn=<MeanBackward0>) bc_loss tensor(1.8822, grad_fn=<AddBackward0>) ic_loss tensor(4247.3916, grad_fn=<MeanBackward0>) data_loss tensor(36293.4485, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 950, Loss: 40966.08472880351\n",
      "pde_loss tensor(428.5476, grad_fn=<MeanBackward0>) bc_loss tensor(1.5657, grad_fn=<AddBackward0>) ic_loss tensor(4193.9790, grad_fn=<MeanBackward0>) data_loss tensor(36301.9068, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(433.7701, grad_fn=<MeanBackward0>) bc_loss tensor(7.0812, grad_fn=<AddBackward0>) ic_loss tensor(4142.8320, grad_fn=<MeanBackward0>) data_loss tensor(36309.0477, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(438.8663, grad_fn=<MeanBackward0>) bc_loss tensor(7.5076, grad_fn=<AddBackward0>) ic_loss tensor(4088.9785, grad_fn=<MeanBackward0>) data_loss tensor(36319.7294, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(444.0690, grad_fn=<MeanBackward0>) bc_loss tensor(3.3060, grad_fn=<AddBackward0>) ic_loss tensor(4039.5232, grad_fn=<MeanBackward0>) data_loss tensor(36326.9403, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(449.1812, grad_fn=<MeanBackward0>) bc_loss tensor(0.1384, grad_fn=<AddBackward0>) ic_loss tensor(3988.5103, grad_fn=<MeanBackward0>) data_loss tensor(36336.5721, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 955, Loss: 40774.402131068215\n",
      "pde_loss tensor(454.3041, grad_fn=<MeanBackward0>) bc_loss tensor(1.3586, grad_fn=<AddBackward0>) ic_loss tensor(3938.8755, grad_fn=<MeanBackward0>) data_loss tensor(36345.6805, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(459.4716, grad_fn=<MeanBackward0>) bc_loss tensor(4.3447, grad_fn=<AddBackward0>) ic_loss tensor(3891.5295, grad_fn=<MeanBackward0>) data_loss tensor(36353.3098, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(464.5306, grad_fn=<MeanBackward0>) bc_loss tensor(4.8947, grad_fn=<AddBackward0>) ic_loss tensor(3842.3789, grad_fn=<MeanBackward0>) data_loss tensor(36363.6130, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(469.6790, grad_fn=<MeanBackward0>) bc_loss tensor(2.3839, grad_fn=<AddBackward0>) ic_loss tensor(3796.8096, grad_fn=<MeanBackward0>) data_loss tensor(36371.0846, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(474.7373, grad_fn=<MeanBackward0>) bc_loss tensor(0.2129, grad_fn=<AddBackward0>) ic_loss tensor(3750.0420, grad_fn=<MeanBackward0>) data_loss tensor(36380.5822, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 960, Loss: 40605.57433938364\n",
      "pde_loss tensor(479.7984, grad_fn=<MeanBackward0>) bc_loss tensor(0.7742, grad_fn=<AddBackward0>) ic_loss tensor(3704.5261, grad_fn=<MeanBackward0>) data_loss tensor(36389.5908, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(484.8781, grad_fn=<MeanBackward0>) bc_loss tensor(2.6479, grad_fn=<AddBackward0>) ic_loss tensor(3660.6836, grad_fn=<MeanBackward0>) data_loss tensor(36397.6473, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(489.8626, grad_fn=<MeanBackward0>) bc_loss tensor(3.0816, grad_fn=<AddBackward0>) ic_loss tensor(3615.6741, grad_fn=<MeanBackward0>) data_loss tensor(36407.6939, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(494.9143, grad_fn=<MeanBackward0>) bc_loss tensor(1.6431, grad_fn=<AddBackward0>) ic_loss tensor(3573.5522, grad_fn=<MeanBackward0>) data_loss tensor(36415.4949, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(499.8861, grad_fn=<MeanBackward0>) bc_loss tensor(0.2989, grad_fn=<AddBackward0>) ic_loss tensor(3530.6714, grad_fn=<MeanBackward0>) data_loss tensor(36424.8385, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 965, Loss: 40455.69496496409\n",
      "pde_loss tensor(504.8572, grad_fn=<MeanBackward0>) bc_loss tensor(0.5792, grad_fn=<AddBackward0>) ic_loss tensor(3489.0251, grad_fn=<MeanBackward0>) data_loss tensor(36433.6350, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(509.8375, grad_fn=<MeanBackward0>) bc_loss tensor(1.7461, grad_fn=<AddBackward0>) ic_loss tensor(3448.8054, grad_fn=<MeanBackward0>) data_loss tensor(36441.6472, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(514.7319, grad_fn=<MeanBackward0>) bc_loss tensor(2.1401, grad_fn=<AddBackward0>) ic_loss tensor(3407.8298, grad_fn=<MeanBackward0>) data_loss tensor(36451.1798, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(519.6763, grad_fn=<MeanBackward0>) bc_loss tensor(1.4916, grad_fn=<AddBackward0>) ic_loss tensor(3369.2329, grad_fn=<MeanBackward0>) data_loss tensor(36458.8839, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(524.5388, grad_fn=<MeanBackward0>) bc_loss tensor(0.5661, grad_fn=<AddBackward0>) ic_loss tensor(3330.0186, grad_fn=<MeanBackward0>) data_loss tensor(36467.9424, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 970, Loss: 40323.065886356555\n",
      "pde_loss tensor(529.4011, grad_fn=<MeanBackward0>) bc_loss tensor(0.4353, grad_fn=<AddBackward0>) ic_loss tensor(3292.0278, grad_fn=<MeanBackward0>) data_loss tensor(36476.3671, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(534.2504, grad_fn=<MeanBackward0>) bc_loss tensor(1.0488, grad_fn=<AddBackward0>) ic_loss tensor(3254.9941, grad_fn=<MeanBackward0>) data_loss tensor(36484.4347, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(539.0341, grad_fn=<MeanBackward0>) bc_loss tensor(1.5501, grad_fn=<AddBackward0>) ic_loss tensor(3217.7324, grad_fn=<MeanBackward0>) data_loss tensor(36493.4081, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(543.8517, grad_fn=<MeanBackward0>) bc_loss tensor(1.5850, grad_fn=<AddBackward0>) ic_loss tensor(3182.4236, grad_fn=<MeanBackward0>) data_loss tensor(36500.9122, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(548.5806, grad_fn=<MeanBackward0>) bc_loss tensor(1.0135, grad_fn=<AddBackward0>) ic_loss tensor(3146.4573, grad_fn=<MeanBackward0>) data_loss tensor(36509.7629, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 975, Loss: 40205.81413764497\n",
      "pde_loss tensor(553.3278, grad_fn=<MeanBackward0>) bc_loss tensor(0.5685, grad_fn=<AddBackward0>) ic_loss tensor(3112.0657, grad_fn=<MeanBackward0>) data_loss tensor(36517.5186, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(558.0312, grad_fn=<MeanBackward0>) bc_loss tensor(0.5575, grad_fn=<AddBackward0>) ic_loss tensor(3077.9446, grad_fn=<MeanBackward0>) data_loss tensor(36525.5868, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(562.6919, grad_fn=<MeanBackward0>) bc_loss tensor(0.8843, grad_fn=<AddBackward0>) ic_loss tensor(3044.1599, grad_fn=<MeanBackward0>) data_loss tensor(36533.8890, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(567.3594, grad_fn=<MeanBackward0>) bc_loss tensor(1.1897, grad_fn=<AddBackward0>) ic_loss tensor(3011.6707, grad_fn=<MeanBackward0>) data_loss tensor(36541.3489, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(571.9479, grad_fn=<MeanBackward0>) bc_loss tensor(1.0972, grad_fn=<AddBackward0>) ic_loss tensor(2978.7964, grad_fn=<MeanBackward0>) data_loss tensor(36549.8255, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 980, Loss: 40101.66706968088\n",
      "pde_loss tensor(576.5520, grad_fn=<MeanBackward0>) bc_loss tensor(0.8425, grad_fn=<AddBackward0>) ic_loss tensor(2947.3896, grad_fn=<MeanBackward0>) data_loss tensor(36557.2373, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(581.0950, grad_fn=<MeanBackward0>) bc_loss tensor(0.5902, grad_fn=<AddBackward0>) ic_loss tensor(2915.9375, grad_fn=<MeanBackward0>) data_loss tensor(36565.2665, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(585.6222, grad_fn=<MeanBackward0>) bc_loss tensor(0.5702, grad_fn=<AddBackward0>) ic_loss tensor(2885.3083, grad_fn=<MeanBackward0>) data_loss tensor(36572.9229, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(590.1285, grad_fn=<MeanBackward0>) bc_loss tensor(0.7264, grad_fn=<AddBackward0>) ic_loss tensor(2855.3953, grad_fn=<MeanBackward0>) data_loss tensor(36580.3053, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(594.5778, grad_fn=<MeanBackward0>) bc_loss tensor(0.8778, grad_fn=<AddBackward0>) ic_loss tensor(2825.5378, grad_fn=<MeanBackward0>) data_loss tensor(36588.1695, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 985, Loss: 40009.16292825908\n",
      "pde_loss tensor(599.0368, grad_fn=<MeanBackward0>) bc_loss tensor(1.0637, grad_fn=<AddBackward0>) ic_loss tensor(2797.0012, grad_fn=<MeanBackward0>) data_loss tensor(36595.0627, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(603.4138, grad_fn=<MeanBackward0>) bc_loss tensor(1.1118, grad_fn=<AddBackward0>) ic_loss tensor(2767.9995, grad_fn=<MeanBackward0>) data_loss tensor(36602.9893, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(607.8146, grad_fn=<MeanBackward0>) bc_loss tensor(1.3015, grad_fn=<AddBackward0>) ic_loss tensor(2740.5591, grad_fn=<MeanBackward0>) data_loss tensor(36609.6276, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(612.1222, grad_fn=<MeanBackward0>) bc_loss tensor(1.4126, grad_fn=<AddBackward0>) ic_loss tensor(2712.4851, grad_fn=<MeanBackward0>) data_loss tensor(36617.4934, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(616.4668, grad_fn=<MeanBackward0>) bc_loss tensor(1.8821, grad_fn=<AddBackward0>) ic_loss tensor(2686.2161, grad_fn=<MeanBackward0>) data_loss tensor(36623.7592, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 990, Loss: 39928.32414025989\n",
      "pde_loss tensor(620.6997, grad_fn=<MeanBackward0>) bc_loss tensor(2.5010, grad_fn=<AddBackward0>) ic_loss tensor(2658.9543, grad_fn=<MeanBackward0>) data_loss tensor(36631.6564, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(625.0008, grad_fn=<MeanBackward0>) bc_loss tensor(4.4187, grad_fn=<AddBackward0>) ic_loss tensor(2634.1013, grad_fn=<MeanBackward0>) data_loss tensor(36637.2313, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(629.1370, grad_fn=<MeanBackward0>) bc_loss tensor(7.6378, grad_fn=<AddBackward0>) ic_loss tensor(2607.2729, grad_fn=<MeanBackward0>) data_loss tensor(36645.5742, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(633.4210, grad_fn=<MeanBackward0>) bc_loss tensor(17.3276, grad_fn=<AddBackward0>) ic_loss tensor(2584.4099, grad_fn=<MeanBackward0>) data_loss tensor(36649.7609, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(637.3987, grad_fn=<MeanBackward0>) bc_loss tensor(30.0196, grad_fn=<AddBackward0>) ic_loss tensor(2557.1382, grad_fn=<MeanBackward0>) data_loss tensor(36659.5520, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 995, Loss: 39884.10859603284\n",
      "pde_loss tensor(641.7129, grad_fn=<MeanBackward0>) bc_loss tensor(66.0050, grad_fn=<AddBackward0>) ic_loss tensor(2537.4370, grad_fn=<MeanBackward0>) data_loss tensor(36660.9617, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(645.4866, grad_fn=<MeanBackward0>) bc_loss tensor(46.1553, grad_fn=<AddBackward0>) ic_loss tensor(2509.6965, grad_fn=<MeanBackward0>) data_loss tensor(36672.2775, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(649.6429, grad_fn=<MeanBackward0>) bc_loss tensor(4.9768, grad_fn=<AddBackward0>) ic_loss tensor(2489.6206, grad_fn=<MeanBackward0>) data_loss tensor(36674.9687, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(653.6359, grad_fn=<MeanBackward0>) bc_loss tensor(14.1036, grad_fn=<AddBackward0>) ic_loss tensor(2467.8550, grad_fn=<MeanBackward0>) data_loss tensor(36680.0877, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "pde_loss tensor(657.3932, grad_fn=<MeanBackward0>) bc_loss tensor(31.1190, grad_fn=<AddBackward0>) ic_loss tensor(2443.3506, grad_fn=<MeanBackward0>) data_loss tensor(36688.8639, dtype=torch.float64, grad_fn=<MeanBackward0>)\n",
      "Epoch 1000, Loss: 39820.726729944196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Record losses for plotting\n",
    "epoch_losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Predict temperature\n",
    "    T_pred = model(X, T)   # The model outputs the temperature at each (x, t) coordinate\n",
    "\n",
    "    # Compute PDE residual\n",
    "    T_pred = T_pred.reshape(X.shape) # Reshape the prediction to match the shape of X\n",
    "    T_pred_t = torch.autograd.grad(T_pred, T, torch.ones(T.shape, device=T_pred.device), create_graph=True)[0] # Compute the gradient of T_pred w.r.t. t\n",
    "    T_pred_x = torch.autograd.grad(T_pred, X, torch.ones(X.shape, device=T_pred.device), create_graph=True)[0] # Compute the gradient of T_pred w.r.t. x\n",
    "    T_pred_xx = torch.autograd.grad(T_pred_x, X, torch.ones(X.shape, device=T_pred.device), create_graph=True)[0] # Compute the second gradient of T_pred w.r.t. x\n",
    "    \n",
    "    # PDE Loss\n",
    "    pde_residual = T_pred_t - alpha * T_pred_xx # Compute the residual of the PDE\n",
    "    pde_loss = torch.mean(pde_residual ** 2)  # Compute the mean squared error of the PDE residual\n",
    "    \n",
    "    # Boundary condition loss\n",
    "    # Adjust boundary tensors to have the same size in the time dimension as T\n",
    "    left_boundary = torch.ones(T.shape, device=X.device, requires_grad=True)*500.0\n",
    "    right_boundary = torch.ones(T.shape, device=X.device, requires_grad=True)*500.0\n",
    "\n",
    "    bc_loss = torch.mean((model(left_boundary, T) - 500.0) ** 2) \\\n",
    "            + torch.mean((model(right_boundary, T) - 500.0) ** 2)\n",
    "    \n",
    "    # Initial condition loss\n",
    "    ic_loss = torch.mean((model(X, torch.zeros_like(T)) - init_temp) ** 2)\n",
    "    \n",
    "\n",
    "    # Data loss\n",
    "\n",
    "    data_loss = torch.mean((model(X, T) - temp_inp) ** 2)\n",
    "\n",
    "    # Total loss\n",
    "    loss = pde_loss + bc_loss + ic_loss + data_loss\n",
    "\n",
    "    print(\"pde_loss\",pde_loss,\"bc_loss\",bc_loss,\"ic_loss\",ic_loss,\"data_loss\",data_loss)\n",
    "    epoch_losses.append(loss.item())\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Plotting the error for a fixed time\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m t_fixed \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Choose a time to observe error\u001b[39;00m\n\u001b[1;32m     13\u001b[0m x_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m exact_solution \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msin(np\u001b[38;5;241m.\u001b[39mpi\u001b[38;5;241m*\u001b[39mx_values\u001b[38;5;241m.\u001b[39mnumpy())  \u001b[38;5;66;03m# If you have an exact solution function, apply it here\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epoch_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plotting the error for a fixed time\n",
    "t_fixed = torch.tensor([[5]] * 100, requires_grad=True)  # Choose a time to observe error\n",
    "x_values = torch.linspace(0, 1, 100).view(-1, 1).requires_grad_(True)\n",
    "exact_solution = np.exp(-np.pi**2*0.1)*np.sin(np.pi*x_values.numpy())  # If you have an exact solution function, apply it here\n",
    "\n",
    "T_pred_fixed_time = model(x_values, t_fixed).detach().numpy().flatten()\n",
    "error_at_fixed_time = np.abs(T_pred_fixed_time - exact_solution)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x_values.detach().numpy(), error_at_fixed_time, label='Error at t = 0.5')\n",
    "plt.xlabel('Position (x)')\n",
    "plt.ylabel('Error |T_pred - T_exact|')\n",
    "plt.title('Prediction Error at Fixed Time')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Plot the predicted temperature field\n",
    "    X_plot, T_plot = torch.meshgrid(torch.linspace(0, 1, 100), torch.linspace(0, 20, 100)) # Create a meshgrid of X and T\n",
    "    X_plot = X_plot.reshape(-1, 1)  # Flatten the meshgrid\n",
    "    T_plot = T_plot.reshape(-1, 1)  # Flatten the meshgrid\n",
    "    T_pred_plot = model(X_plot, T_plot).reshape(100, 100).numpy() # Predict the temperature at each (x, t) coordinate\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.contourf(X_plot[:, 0].numpy().reshape(100, 100), T_plot[:, 0].numpy().reshape(100, 100), T_pred_plot, cmap='coolwarm')\n",
    "    plt.xlabel('Position (x)')\n",
    "    plt.ylabel('Time (t)')\n",
    "    plt.title('Predicted Temperature Field')\n",
    "    plt.colorbar()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
