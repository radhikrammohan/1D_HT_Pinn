{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Three Phase Simulation of Alloys and PINN model development \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the simulation of 1D Phase change of aluminium alloy. There will be three phases (solid,liquid and mushy).   \n",
    "\n",
    "The approach used is finite difference method and the physics involved in heat conduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import csv\n",
    "from sklearn import svm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the constants and initial conditions\n",
    "\n",
    "Material :- AL 380\n",
    "\n",
    "| Sr.No | Properties  | Symbol | Value  | Unit |Range(source) |\n",
    "|:---:|:---:|:---:|:---:|:---:|:--:|\n",
    "| 1  | Liquidus Density | $\\rho_{l}$  | 2300 | $kg/m^3$  |  2200-2400 (ASM handbook) |\n",
    "|  2 |  Solidus Density  |  $\\rho_{s}$  | 2500  |  $kg/m^3$  | 2400-2600 (ASM handbook) |\n",
    "|  3 |  Mushy Desnity |  $\\rho_{m}$  |  2400 | $kg/m^3$   |Increase linearly from liquid to solid (ASM handbook) |\n",
    "|  4 |  Liquidus Thermal Conductivity| $k_l$  |  70 | $W/m-K$  |60-80 (ASM handbook) |\n",
    "|  5 |  Solidus Thermal Conductivity | $k_s$  | 180  |  $W/m-K$ | 150-210(ASM handbook) |\n",
    "|  6 | Mushy Zone Thermal Conductivity | $k_m$  | 125  |  $W/m-K$ |Decrease linearly from liquid to solid (ASM handbook) |\n",
    "|  7 | Liquidus Specific Heat | $c_{pl}$  | 1190  | $J/kg-K$  | 1100 -1200 (ASM handbook)|\n",
    "|  8 | Solidus Specific Heat | $c_{ps}$  |  1100 |  $J/kg-K$  | 1100-1200 (ASM handbook)|\n",
    "|  9 | Mushy Zone Specific Heat |  $c_{pm}$ | 1175 | $J/kg-K$   |decrease lineraly from liquid to solid (ASM handbook)|\n",
    "|  10 | Latent Heat of Fusion | $L_{fusion}$  | 450e3  | $J/kg$ | (400-500)e3 (ASM handbook) |\n",
    "| 11 | Left Boundary Temperature |$BCT_{l}$|623 |$K$| (623-723) Nissan Data |\n",
    "|12 | Right Boundary Temperature | $BCT_{r}$|623 |$K$| (623-723) Nissan Data |\n",
    "|13| Freezing time | |60 |sec|||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_l = 3.394878564540885e-05, alpha_s = 3.686205086349929e-05, m_eff = 6.296953764744878e-06\n",
      "dx is 0.0003061224489795918\n",
      "dt is  0.0012711033647622566\n",
      "num_steps is 47203\n",
      "cfl is 0.0012711033647622566\n",
      "stability criteria satisfied\n"
     ]
    }
   ],
   "source": [
    "# Geometry\n",
    "length = 15.0e-3             # Length of the rod\n",
    "\n",
    "# Material properties\n",
    "rho = 2300.0                     # Density of AL380 (kg/m^3)\n",
    "rho_l = 2460.0                   # Density of AL380 (kg/m^3)\n",
    "rho_s = 2710.0                    # Density of AL380 (kg/m^3)\n",
    "rho_m = (rho_l + rho_s )/2       # Desnity in mushy zone is taken as average of liquid and solid density\n",
    "\n",
    "k = 104.0                       # W/m-K\n",
    "k_l = k                       # W/m-K\n",
    "k_s = 96.2                    # W/m-K\n",
    "k_m =  (k_l+k_s)/2                     # W/m-K\n",
    "k_mo = 41.5\n",
    "\n",
    "\n",
    "cp = 1245.3                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_l = cp                      # Specific heat of aluminum (J/kg-K)\n",
    "cp_s = 963.0                 # Specific heat of aluminum (J/kg-K)\n",
    "cp_m =  (cp_l+cp_s)/2                 # Specific heat of mushy zone is taken as average of liquid and solid specific heat\n",
    "# cp_m = cp\n",
    "           # Thermal diffusivity\n",
    "alpha_l = k_l / (rho_l * cp_l) \n",
    "alpha_s = k_s / (rho_s*cp_s)\n",
    "alpha_m = k_m / (rho_m * cp_m)          #`Thermal diffusivity in mushy zone is taken as average of liquid and solid thermal diffusivity`\n",
    "\n",
    "\n",
    "#L_fusion = 3.9e3                 # J/kg\n",
    "L_fusion = 389.0e3               # J/kg  # Latent heat of fusion of aluminum\n",
    "         # Thermal diffusivity\n",
    "\n",
    "\n",
    "T_L = 574.4 +273.0                       #  K -Liquidus Temperature (615 c) AL 380\n",
    "T_S = 497.3 +273.0                     # K- Solidus Temperature (550 C)\n",
    "m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "print (f\"alpha_l = {alpha_l}, alpha_s = {alpha_s}, m_eff = {m_eff}\")\n",
    "\n",
    "# htc = 10.0                   # W/m^2-K\n",
    "# q = htc*(919.0-723.0)\n",
    "# q = 10000.0\n",
    "\n",
    "\n",
    "num_points = 50                        # Number of spatial points\n",
    "dx = length / (num_points - 1)         # Distance between two spatial points\n",
    "print('dx is',dx)\n",
    "\n",
    "                                                              \n",
    "# Time Discretization  \n",
    "# \n",
    "time_end = 60        # seconds                         \n",
    "\n",
    "maxi = max(alpha_s,alpha_l,alpha_m)\n",
    "dt = abs(0.5*((dx**2) /maxi)) \n",
    "\n",
    "print('dt is ',dt)\n",
    "num_steps = round(time_end/dt)\n",
    "print('num_steps is',num_steps)\n",
    "cfl = 0.5 *(dx**2/max(alpha_l,alpha_s,alpha_m))\n",
    "print('cfl is',cfl)\n",
    "\n",
    "time_steps = np.linspace(0, time_end, num_steps + 1)\n",
    "step_coeff = dt / (dx ** 2)\n",
    "\n",
    "if dt <= cfl:\n",
    "    print('stability criteria satisfied')\n",
    "else:\n",
    "    print('stability criteria not satisfied')\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial and Boundary Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initial temperature and phase fields\n",
    "temperature = np.full(num_points+2, 919.0)            # Initial temperature of the rod with ghost points at both ends\n",
    "phase = np.zeros(num_points+2)*0.0                    # Initial phase of the rod with ghost points at both ends\n",
    "\n",
    "# Set boundary conditions\n",
    "# temperature[-1] = 919.0 \n",
    "phase[-1] = 1.0\n",
    "\n",
    "# temperature[0] = 919.0 #(40 C)\n",
    "phase[0] = 1.0\n",
    "\n",
    "# Store initial state in history\n",
    "temperature_history = [temperature.copy()]    # List to store temperature at each time step\n",
    "phi_history = [phase.copy()]                    # List to store phase at each time step\n",
    "temp_init = temperature.copy()                 # Initial temperature of the rod\n",
    "# print(temperature_history,phi_history)\n",
    "# Array to store temperature at midpoint over time\n",
    "midpoint_index = num_points // 2                          # Index of the midpoint\n",
    "\n",
    "midpoint_temperature_history = [temperature[midpoint_index]]            # List to store temperature at midpoint over time\n",
    "dm = 60.0e-3                                                            # die thickness in m\n",
    "\n",
    "# r_m =  (k_mo / dm) + (1/htc)\n",
    "\n",
    "t_surr = 500.0                                        # Surrounding temperature in K\n",
    "# t_surr = h()\n",
    "\n",
    "def kramp(temp,v1,v2,T_L,T_s):                                      # Function to calculate thermal conductivity in Mushy Zone\n",
    "        slope = (v1-v2)/(T_L-T_S)\n",
    "        if temp > T_L:\n",
    "            k_m = k_l\n",
    "        elif temp < T_S:\n",
    "            k_m = k_s\n",
    "        else:\n",
    "            k_m = k_s + slope*(temp-T_S)\n",
    "        return k_m\n",
    "\n",
    "def cp_ramp(temp,v1,v2,T_L,T_s):                                    # Function to calculate specific heat capacity in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        cp_m = cp_l\n",
    "    elif temp < T_S:\n",
    "        cp_m = cp_s\n",
    "    else:\n",
    "        cp_m = cp_s + slope*(temp-T_S)\n",
    "    return cp_m\n",
    "\n",
    "def rho_ramp(temp,v1,v2,T_L,T_s):                                       # Function to calculate density in Mushy Zone\n",
    "    slope = (v1-v2)/(T_L-T_S)\n",
    "    if temp > T_L:\n",
    "        rho_m = rho_l\n",
    "    elif temp < T_S:\n",
    "        rho_m = rho_s\n",
    "    else:\n",
    "        rho_m = rho_s + slope*(temp-T_S)\n",
    "    return rho_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the heat transfer equation and phase change using numerical methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for m in range(1, num_steps+1):                                                                            # time loop\n",
    "    htc = 10.0                   # htc of Still air in W/m^2-K\n",
    "    q1 = htc*(temp_init[0]-t_surr)   # Heat flux at the left boundary\n",
    "    \n",
    "    # print(f\"q1 is {q1}\")\n",
    "    temperature[0] = temp_init[0] + alpha_l * step_coeff * ((2.0*temp_init[1]) - (2.0 * temp_init[0])-(2.0*dx*(q1)))  # Update boundary condition temperature\n",
    "    \n",
    "    q2 = htc*(temp_init[-1]-t_surr)                   # Heat flux at the right boundary\n",
    "    temperature[-1] = temp_init[-1] + alpha_l * step_coeff * ((2.0*temp_init[-2]) - (2.0 * temp_init[-1])-(2.0*dx*(q2)))  # Update boundary condition temperature\n",
    "    \n",
    "    for n in range(1,num_points+1):              # space loop, adjusted range\n",
    "       \n",
    "        if temperature[n] >= T_L:\n",
    "            temperature[n] += ((alpha_l * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            phase[n] = 0\n",
    "            \n",
    "            # print(f\" Time-Step{m},Spatial point{n},Temperature{temperature[n]}\")\n",
    "        elif T_S < temperature[n] < T_L:\n",
    "            \n",
    "            k_m = kramp(temperature[n],k_l,k_s,T_L,T_S)\n",
    "            cp_m = cp_ramp(temperature[n],cp_l,cp_s,T_L,T_S)\n",
    "            rho_m = rho_ramp(temperature[n],rho_l,rho_s,T_L,T_S)\n",
    "            m_eff =(k_m/(rho_m*(cp_m + (L_fusion/(T_L-T_S)))))\n",
    "            \n",
    "            temperature[n] += ((m_eff * step_coeff)* (temp_init[n+1] - (2.0 * temp_init[n]) + temp_init[n-1]))\n",
    "            \n",
    "            phase[n] = (T_L - temperature[n]) / (T_L - T_S)\n",
    "            # print(m,n,temperature[n],phase[n])\n",
    "         \n",
    "        elif temperature[n]<T_S:\n",
    "            temperature[n] += ((alpha_s * step_coeff) * (temp_init[n+1] - (2.0 * temp_init[n])+ temp_init[n-1]))\n",
    "            phase[n] = 1\n",
    "                     \n",
    "        else:\n",
    "            print(\"ERROR: should not be here\")\n",
    "\n",
    "     \n",
    "          \n",
    "    temperature = temperature.copy()                                                                # Update temperature\n",
    "    phase = phase.copy()                                                                            # Update phase\n",
    "    temp_init = temperature.copy()                                                                  # Update last time step temperature\n",
    "    temperature_history.append(temperature.copy())                                                  # Append the temperature history to add ghost points\n",
    "    phi_history.append(phase.copy())                                                                # Append the phase history to add ghost points\n",
    "    midpoint_temperature_history.append(temperature[midpoint_index])                                # Store midpoint temperature\n",
    "    \n",
    "    \n",
    "    # print(f\"Step {m}, Temperature: {temperature}\")\n",
    "    \n",
    "\n",
    "\n",
    "# print(midpoint_temperature_history)\n",
    "#print(phi_history)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot temperature history for debugging\n",
    "# temperature_history_1 = np.array(temperature_history)\n",
    "# print(temperature_history_1.shape)\n",
    "# time_ss= np.linspace(0, time_end, num_steps+1)\n",
    "# # print(time_ss.shape)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(time_ss, midpoint_temperature_history, label='Midpoint Temperature')\n",
    "# plt.axhline(y=T_L, color='r', linestyle='--', label='Liquidus Temperature')\n",
    "# plt.axhline(y=T_S, color='g', linestyle='--', label='Solidus Temperature')\n",
    "# plt.xlabel('Time(s)')\n",
    "# plt.ylabel('Temperature (K)')\n",
    "# plt.title('Temperature Distribution Over Time at x = 7.5mm') \n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot temperature history for debugging\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for i in range(0, num_steps, num_steps // 50):\n",
    "#     plt.plot(np.linspace(0, length, num_points+2), temperature_history[i], label=f't={i * dt:.2f} s')\n",
    "\n",
    "# plt.xlabel('Position (m)')\n",
    "# plt.ylabel('Temperature (K)')\n",
    "# plt.title('Temperature Distribution Over Time')\n",
    "# # plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_history = np.array(temperature_history)\n",
    "phi_history = np.array(phi_history)\n",
    "# print(temperature_history.shape)\n",
    "# print(phi_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47204, 52)\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have temperature_history and phi_history as lists of arrays\n",
    "temperature_history = np.array(temperature_history)\n",
    "phi_history = np.array(phi_history)\n",
    "\n",
    "print(temperature_history.shape)\n",
    "\n",
    "# # Check the new shape after transposing\n",
    "# print(\"Transposed Temperature History Shape:\", temperature_history.shape)\n",
    "# print(\"Transposed Phi History Shape:\", phi_history.shape)\n",
    "\n",
    "# # Create a meshgrid for space and time coordinates\n",
    "# space_coord, time_coord = np.meshgrid(np.arange(temperature_history.shape[1]), np.arange(temperature_history.shape[0]))\n",
    "\n",
    "# time_coord = time_coord * dt \n",
    "# # Create a figure with two subplots\n",
    "# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# # Plot the temperature history on the left subplot\n",
    "# im1 = ax1.pcolormesh(space_coord, time_coord, temperature_history, cmap='viridis')\n",
    "# ax1.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax1.set_title('Temperature Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im1, ax=ax1, label='Temperature')\n",
    "\n",
    "# # Plot the phase history on the right subplot\n",
    "# im2 = ax2.pcolormesh(space_coord, time_coord, phi_history, cmap='viridis')\n",
    "# ax2.set_xlabel('Space Coordinate', fontname='Times New Roman', fontsize=18)\n",
    "# ax2.set_ylabel('Time',fontname='Times New Roman', fontsize=16)\n",
    "# ax2.set_title('Phase Variation Over Time',fontname='Times New Roman', fontsize=20)\n",
    "# fig.colorbar(im2, ax=ax2, label='Phase')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# #plot the main\n",
    "# fig, ax = plt.subplots(figsize=(14, 6))\n",
    "# im = ax.pcolormesh(space_coord, time_coord, Dim_ny, cmap='viridis')\n",
    "# ax.set_xlabel('Space Coordinate')\n",
    "# ax.set_ylabel('Time')\n",
    "# ax.set_title('Niyama Variation Over Time')\n",
    "# fig.colorbar(im, ax=ax, label='Main')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check for gpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 1)\n",
      "(47204, 1)\n",
      "torch.Size([2454608, 2])\n",
      "torch.Size([2454608, 1])\n"
     ]
    }
   ],
   "source": [
    "space = np.linspace(0, length, num_points+2)\n",
    "time = np.linspace(0, time_end, num_steps+1)\n",
    "\n",
    "scaler_space = StandardScaler()\n",
    "scaler_time = StandardScaler()\n",
    "\n",
    "space_tr = scaler_space.fit_transform(space.reshape(-1,1))\n",
    "time_tr = scaler_time.fit_transform(time.reshape(-1,1))\n",
    "\n",
    "print(space_tr.shape)\n",
    "print(time_tr.shape)\n",
    "\n",
    "\n",
    "# create mesh grid of space and time\n",
    "\n",
    "space_tr, time_tr = np.meshgrid(space_tr, time_tr)\n",
    "space_tr = space_tr.flatten().reshape(-1,1)\n",
    "time_tr = time_tr.flatten().reshape(-1,1)\n",
    "inputs = np.hstack([space_tr,time_tr]) # Concatenate the spatial and temporal inputs\n",
    "inputs = torch.tensor(inputs).float().to(device) # Convert the inputs to a tensor\n",
    "print(inputs.shape)\n",
    "\n",
    "# label/temp data\n",
    "temp_tr = torch.tensor(temperature_history).float().to(device) # Convert the temperature history to a tensor\n",
    "temp_inp = temp_tr.reshape(-1,1) # Reshape the temperature tensor to a column vector\n",
    "print(temp_inp.shape)\n",
    "\n",
    "\n",
    "\n",
    "#Data Splitting\n",
    "\n",
    "train_inputs, val_test_inputs, train_temp_inp, val_test_temp_inp = train_test_split(inputs, temp_inp, test_size=0.2, random_state=42)\n",
    "val_inputs, test_inputs, val_temp_inp, test_temp_inp = train_test_split(val_test_inputs, val_test_temp_inp, test_size=0.8, random_state=42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "\n",
    "train_dataset = TensorDataset(train_inputs, train_temp_inp) # Create the training dataset\n",
    "val_dataset = TensorDataset(val_inputs, val_temp_inp) # Create the validation dataset\n",
    "test_dataset = TensorDataset(test_inputs, test_temp_inp) # Create the test dataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Create the training dataloader\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True) # Create the validation dataloader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True) # Create the test dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_data(u_pred, u_true):\n",
    "    return nn.MSELoss()(u_pred, u_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the neural netowrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the neural network architecture\n",
    "class Mushydata(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size): # This is the constructor\n",
    "        super(Mushydata, self).__init__()\n",
    "        self.base = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):                               # This is the forward pass\n",
    "        input_features = torch.cat([x, t], dim=1)          # Concatenate the input features\n",
    "        m = self.base(input_features)                                 # Pass through the third layer\n",
    "        return m                    # Return the output of the network\n",
    "\n",
    "\n",
    "# features = torch.rand(1, 2)\n",
    "# model = HeatPINN(2, 20, 1)\n",
    "# output = model(features[:, 0:1], features[:, 1:2])\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# Loss function for data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparamters Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hidden_size = 128\n",
    "learning_rate = 0.08\n",
    "epochs = 30000\n",
    "# alpha = 0.01  # Adjust this value based on your problem\n",
    "# boundary_value = 313.0\n",
    "# initial_value = init_temp\n",
    "# Initialize the model\n",
    "model = Mushydata(input_size=2, hidden_size=hidden_size,output_size=1).to(device)\n",
    "lambda_l1 = 0.2\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of train_loader is <class 'torch.utils.data.dataloader.DataLoader'>\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(f\"Datatype of train_loader is {type(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(epochs, model, loss_fn_data, optimizer, train_dataloader, val_dataloader):\n",
    "    train_losses = []  # Initialize the list to store the training losses\n",
    "    val_losses = []    # Initialize the list to store the validation losses\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()                 # Set the model to training mode\n",
    "        train_loss = 0                # Initialize the training loss\n",
    "\n",
    "        for batch in train_dataloader:              # Loop through the training dataloader\n",
    "            inputs, temp_inp= batch                 # Get the inputs and the true values\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device) # Move the inputs and true values to the GPU\n",
    "            optimizer.zero_grad()                               # Zero the gradients\n",
    "            \n",
    "            # Forward pass\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1)).float().to(device) # Get the predictions\n",
    "            \n",
    "            # Loss calculation\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp.float())                                  # Calculate the data loss\n",
    "            # l1_regularization_loss = l1_regularization(model, lambda_l1)                  # Calculate the L1 regularization loss\n",
    "            loss = data_loss                                     # Calculate the total loss\n",
    "            \n",
    "            # Backpropagation\n",
    "            loss.backward(retain_graph=True)                             # Backpropagate the gradients\n",
    "            \n",
    "            optimizer.step()                                                   # Update the weights\n",
    "            \n",
    "            train_loss += loss.item()     \n",
    "                                        # Add the loss to the training set loss\n",
    "        train_losses.append(train_loss)                               # Append the training loss to the list of training losses\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Training-Loss {train_loss}\")\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0                                                                     # Initialize the validation loss\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:                                                            # Loop through the validation dataloader\n",
    "                inputs, temp_inp= batch                                                        # Get the inputs and the true values\n",
    "                inputs, temp_inp= inputs.to(device), temp_inp.to(device)                                     # Move the inputs and true values to the GPU\n",
    "                u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))                        # Get the predictions\n",
    "                data_loss = loss_fn_data(u_pred, temp_inp.float())                                          # Calculate the data loss\n",
    "                loss = data_loss                                                                          # Calculate the total loss\n",
    "                val_loss += loss.item()                                                                     # Add the loss to the validation set loss\n",
    "            val_losses.append(val_loss)                                                                 # Append the validation loss to the list of validation losses\n",
    "                                                                        # Append the validation loss to the list of validation losses\n",
    "    return train_losses, val_losses                                                                    # Return the training and validation losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valdiation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(epochs, model, loss_fn_data, optimizer, train_dataloader, test_dataloader):\n",
    "      \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():   \n",
    "        for batch in test_dataloader:\n",
    "            inputs, temp_inp= batch\n",
    "            inputs, temp_inp= inputs.to(device), temp_inp.to(device)\n",
    "            u_pred = model(inputs[:,0].unsqueeze(1), inputs[:,1].unsqueeze(1))\n",
    "            data_loss = loss_fn_data(u_pred, temp_inp.float())\n",
    "            loss = data_loss \n",
    "            test_loss += loss.item()\n",
    "        test_losses.append(test_loss)\n",
    "    if epochs % 10 == 0:\n",
    "        print(f\"Epoch {epochs}, Test-Loss {test_loss}\")    \n",
    "    return test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Training-Loss 4530035.365522742\n",
      "Epoch 10, Training-Loss 73580266.01592016\n",
      "Epoch 20, Training-Loss 477944257.35377985\n",
      "Epoch 30, Training-Loss 945265904.6379569\n",
      "Epoch 40, Training-Loss 4314255143.702787\n",
      "Epoch 50, Training-Loss 221249.60863900185\n",
      "Epoch 60, Training-Loss 5881279293.226515\n",
      "Epoch 70, Training-Loss 4261505969.555608\n",
      "Epoch 80, Training-Loss 4245259692.4010315\n",
      "Epoch 90, Training-Loss 565255892.190474\n",
      "Epoch 100, Training-Loss 2529880.985947609\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m test_loop(epochs, model, loss_fn_data, optimizer, train_loader, test_loader)  \u001b[38;5;66;03m# Test the model\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[16], line 23\u001b[0m, in \u001b[0;36mtraining_loop\u001b[0;34m(epochs, model, loss_fn_data, optimizer, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m data_loss                                     \u001b[38;5;66;03m# Calculate the total loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m                             \u001b[38;5;66;03m# Backpropagate the gradients\u001b[39;00m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()                                                   \u001b[38;5;66;03m# Update the weights\u001b[39;00m\n\u001b[1;32m     27\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()     \n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch-env/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses, val_losses = training_loop(epochs, model, loss_fn_data, optimizer,train_loader, val_loader)  # Train the model\n",
    " \n",
    "test_losses = test_loop(epochs, model, loss_fn_data, optimizer, train_loader, test_loader)  # Test the model\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-env)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
